{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "import random\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#then import my own modules\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight\n",
    "from timeit import default_timer as timer\n",
    "from AttentiveFP import Fingerprint, Fingerprint_viz, graph_dict, graph_dataset, null_collate, Graph, Logger, time_to_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_aviable = torch.cuda.is_available()\n",
    "device = torch.device(0)\n",
    "\n",
    "SEED = 8\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1128\n",
      "number of successfully processed smiles:  1128\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'solubility'\n",
    "tasks = ['measured log solubility in mols per litre']\n",
    "\n",
    "raw_filename = \"../data/delaney-processed.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \", len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)].reset_index()\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "epochs = 80\n",
    "batch_size =100\n",
    "\n",
    "p_dropout= 0.2\n",
    "fingerprint_dim = 200\n",
    "\n",
    "weight_decay = 6 # also known as l2_regularization_lambda\n",
    "learning_rate = 3\n",
    "K = 3\n",
    "T = 2\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph dicts loaded from ../data/delaney-processed.pkl\n"
     ]
    }
   ],
   "source": [
    "smiles_list = smiles_tasks_df['smiles'].values\n",
    "label_list = smiles_tasks_df[tasks[0]].values\n",
    "graph_dict = graph_dict(smiles_list, label_list, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  101\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "train_fold = []\n",
    "valid_fold = []\n",
    "for k, (train_idx, valid_idx) in enumerate(kfold.split(smiles_list)):\n",
    "    train_fold.append(train_idx)\n",
    "    valid_fold.append(valid_idx)\n",
    "    \n",
    "# avoiding the last batch has too few samples by slightly tune the batch_size\n",
    "while (len(train_fold[0]) % batch_size) / batch_size <0.8:\n",
    "    batch_size +=1\n",
    "print(\"batch size: \", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604858\n",
      "atom_preprocess.preprocess.0.linear.weight torch.Size([200, 39])\n",
      "atom_preprocess.preprocess.0.linear.bias torch.Size([200])\n",
      "atom_preprocess.preprocess.0.bn.weight torch.Size([200])\n",
      "atom_preprocess.preprocess.0.bn.bias torch.Size([200])\n",
      "atom_preprocess.preprocess.3.linear.weight torch.Size([200, 200])\n",
      "atom_preprocess.preprocess.3.linear.bias torch.Size([200])\n",
      "atom_preprocess.preprocess.3.bn.weight torch.Size([200])\n",
      "atom_preprocess.preprocess.3.bn.bias torch.Size([200])\n",
      "bond_preprocess.preprocess.0.linear.weight torch.Size([200, 10])\n",
      "bond_preprocess.preprocess.0.linear.bias torch.Size([200])\n",
      "bond_preprocess.preprocess.0.bn.weight torch.Size([200])\n",
      "bond_preprocess.preprocess.0.bn.bias torch.Size([200])\n",
      "bond_preprocess.preprocess.3.linear.weight torch.Size([200, 200])\n",
      "bond_preprocess.preprocess.3.linear.bias torch.Size([200])\n",
      "bond_preprocess.preprocess.3.bn.weight torch.Size([200])\n",
      "bond_preprocess.preprocess.3.bn.bias torch.Size([200])\n",
      "align.linear.weight torch.Size([1, 800])\n",
      "align.linear.bias torch.Size([1])\n",
      "align.bn.weight torch.Size([1])\n",
      "align.bn.bias torch.Size([1])\n",
      "attend.linear.weight torch.Size([600, 600])\n",
      "attend.linear.bias torch.Size([600])\n",
      "attend.bn.weight torch.Size([600])\n",
      "attend.bn.bias torch.Size([600])\n",
      "gru.weight_ih torch.Size([600, 600])\n",
      "gru.weight_hh torch.Size([600, 200])\n",
      "gru.bias_ih torch.Size([600])\n",
      "gru.bias_hh torch.Size([600])\n",
      "propagate.align.linear.weight torch.Size([1, 400])\n",
      "propagate.align.linear.bias torch.Size([1])\n",
      "propagate.align.bn.weight torch.Size([1])\n",
      "propagate.align.bn.bias torch.Size([1])\n",
      "propagate.attend.linear.weight torch.Size([200, 200])\n",
      "propagate.attend.linear.bias torch.Size([200])\n",
      "propagate.attend.bn.weight torch.Size([200])\n",
      "propagate.attend.bn.bias torch.Size([200])\n",
      "propagate.gru.weight_ih torch.Size([600, 200])\n",
      "propagate.gru.weight_hh torch.Size([600, 200])\n",
      "propagate.gru.bias_ih torch.Size([600])\n",
      "propagate.gru.bias_hh torch.Size([600])\n",
      "superGather.align.linear.weight torch.Size([1, 400])\n",
      "superGather.align.linear.bias torch.Size([1])\n",
      "superGather.align.bn.weight torch.Size([1])\n",
      "superGather.align.bn.bias torch.Size([1])\n",
      "superGather.attend.linear.weight torch.Size([200, 200])\n",
      "superGather.attend.linear.bias torch.Size([200])\n",
      "superGather.attend.bn.weight torch.Size([200])\n",
      "superGather.attend.bn.bias torch.Size([200])\n",
      "superGather.gru.weight_ih torch.Size([600, 200])\n",
      "superGather.gru.weight_hh torch.Size([600, 200])\n",
      "superGather.gru.bias_ih torch.Size([600])\n",
      "superGather.gru.bias_hh torch.Size([600])\n",
      "predict.0.linear.weight torch.Size([512, 200])\n",
      "predict.0.linear.bias torch.Size([512])\n",
      "predict.0.bn.weight torch.Size([512])\n",
      "predict.0.bn.bias torch.Size([512])\n",
      "predict.3.weight torch.Size([1, 512])\n",
      "predict.3.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "# optimizer = optim.SGD(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in  model.parameters()])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(smiles_list):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=True, worker_init_fn=np.random.seed(SEED))\n",
    "    losses = []\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(train_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        \n",
    "        loss = loss_function(mol_prediction, label.view(-1,1))     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "        \n",
    "def eval(smiles_list):\n",
    "    model.eval()\n",
    "    eval_MAE_list = []\n",
    "    eval_MSE_list = []\n",
    "    eval_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(eval_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        MAE = F.l1_loss(mol_prediction, label.view(-1,1), reduction='none')        \n",
    "        MSE = F.mse_loss(mol_prediction, label.view(-1,1), reduction='none')\n",
    "        \n",
    "        eval_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        eval_MSE_list.extend(MSE.data.squeeze().cpu().numpy())\n",
    "    return np.array(eval_MAE_list).mean(), np.array(eval_MSE_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = Logger()\n",
    "# log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "# f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "# log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "# start = timer()\n",
    "\n",
    "# best_param ={}\n",
    "# best_param[\"train_epoch\"] = 0\n",
    "# best_param[\"valid_epoch\"] = 0\n",
    "# best_param[\"train_MSE\"] = 9e8\n",
    "# best_param[\"valid_MSE\"] = 9e8\n",
    "\n",
    "# fold_index = 3\n",
    "# for epoch in range(800):\n",
    "#     losses = train(smiles_list[train_fold[fold_index]])\n",
    "#     traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "#     valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "\n",
    "#     timing = time_to_str((timer() - start), 'min')  \n",
    "#     log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "\n",
    "#     if train_MSE < best_param[\"train_MSE\"]:\n",
    "#         best_param[\"train_epoch\"] = epoch\n",
    "#         best_param[\"train_MSE\"] = train_MSE\n",
    "#     if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#         best_param[\"valid_epoch\"] = epoch\n",
    "#         best_param[\"valid_MSE\"] = valid_MSE\n",
    "# #         if valid_MSE < 0.35:\n",
    "# #              torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#     if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "#         break\n",
    "# print(best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | loss | train MSE |  valid MSE |  time \n",
      "  0   | 8.3497  | 8.1133  | 8.4669  |  0 hr 00 min \n",
      "  1   | 3.3139  | 4.2084  | 5.3518  |  0 hr 00 min \n",
      "  2   | 1.6392  | 5.1288  | 6.3670  |  0 hr 00 min \n",
      "  3   | 1.1428  | 2.0913  | 2.3153  |  0 hr 00 min \n",
      "  4   | 0.8297  | 0.8265  | 0.8951  |  0 hr 00 min \n",
      "  5   | 0.7894  | 0.6475  | 0.6111  |  0 hr 00 min \n",
      "  6   | 0.7783  | 1.1765  | 1.1926  |  0 hr 00 min \n",
      "  7   | 0.8091  | 0.4043  | 0.5436  |  0 hr 00 min \n",
      "  8   | 0.5737  | 0.5056  | 0.6108  |  0 hr 00 min \n",
      "  9   | 0.5417  | 0.4019  | 0.5705  |  0 hr 00 min \n",
      " 10   | 0.5075  | 0.3567  | 0.5144  |  0 hr 00 min \n",
      " 11   | 0.4792  | 0.3182  | 0.4460  |  0 hr 00 min \n",
      " 12   | 0.4963  | 0.3155  | 0.4574  |  0 hr 00 min \n",
      " 13   | 0.4454  | 0.3177  | 0.5461  |  0 hr 00 min \n",
      " 14   | 0.3872  | 0.2508  | 0.3869  |  0 hr 00 min \n",
      " 15   | 0.4004  | 0.3186  | 0.5091  |  0 hr 00 min \n",
      " 16   | 0.3506  | 0.2758  | 0.4616  |  0 hr 00 min \n",
      " 17   | 0.3503  | 0.2397  | 0.3865  |  0 hr 00 min \n",
      " 18   | 0.3927  | 0.2818  | 0.5075  |  0 hr 00 min \n",
      " 19   | 0.3200  | 0.2171  | 0.4031  |  0 hr 00 min \n",
      " 20   | 0.3783  | 0.2704  | 0.4758  |  0 hr 00 min \n",
      " 21   | 0.3678  | 0.2268  | 0.4855  |  0 hr 00 min \n",
      " 22   | 0.3635  | 0.2513  | 0.4777  |  0 hr 00 min \n",
      " 23   | 0.3185  | 0.2248  | 0.4134  |  0 hr 01 min \n",
      " 24   | 0.3388  | 0.2933  | 0.5472  |  0 hr 01 min \n",
      " 25   | 0.4284  | 0.1969  | 0.4568  |  0 hr 01 min \n",
      " 26   | 0.3050  | 0.1816  | 0.4324  |  0 hr 01 min \n",
      " 27   | 0.2907  | 0.2389  | 0.5172  |  0 hr 01 min \n",
      " 28   | 0.3007  | 0.2308  | 0.5214  |  0 hr 01 min \n",
      " 29   | 0.3207  | 0.1727  | 0.4226  |  0 hr 01 min \n",
      " 30   | 0.2764  | 0.2633  | 0.5557  |  0 hr 01 min \n",
      " 31   | 0.2347  | 0.1352  | 0.4045  |  0 hr 01 min \n",
      " 32   | 0.2229  | 0.1399  | 0.3827  |  0 hr 01 min \n",
      " 33   | 0.3239  | 0.1699  | 0.4214  |  0 hr 01 min \n",
      " 34   | 0.2901  | 0.2002  | 0.4825  |  0 hr 01 min \n",
      " 35   | 0.2518  | 0.1189  | 0.3670  |  0 hr 01 min \n",
      " 36   | 0.2731  | 0.1675  | 0.4570  |  0 hr 01 min \n",
      " 37   | 0.2248  | 0.1664  | 0.4395  |  0 hr 01 min \n",
      " 38   | 0.2907  | 0.1856  | 0.4472  |  0 hr 01 min \n",
      " 39   | 0.2416  | 0.1300  | 0.4381  |  0 hr 01 min \n",
      " 40   | 0.2244  | 0.1592  | 0.5042  |  0 hr 01 min \n",
      " 41   | 0.2577  | 0.1318  | 0.4283  |  0 hr 01 min \n",
      " 42   | 0.2084  | 0.1362  | 0.4196  |  0 hr 01 min \n",
      " 43   | 0.2564  | 0.1574  | 0.4359  |  0 hr 01 min \n",
      " 44   | 0.2182  | 0.1357  | 0.4022  |  0 hr 01 min \n",
      " 45   | 0.2660  | 0.1112  | 0.4267  |  0 hr 01 min \n",
      " 46   | 0.2870  | 0.1577  | 0.4228  |  0 hr 02 min \n",
      " 47   | 0.2630  | 0.1484  | 0.4547  |  0 hr 02 min \n",
      " 48   | 0.1963  | 0.1197  | 0.3871  |  0 hr 02 min \n",
      " 49   | 0.2450  | 0.1664  | 0.4400  |  0 hr 02 min \n",
      " 50   | 0.2489  | 0.1703  | 0.4535  |  0 hr 02 min \n",
      " 51   | 0.2338  | 0.1509  | 0.4393  |  0 hr 02 min \n",
      " 52   | 0.1891  | 0.1149  | 0.3646  |  0 hr 02 min \n",
      " 53   | 0.2151  | 0.1607  | 0.4635  |  0 hr 02 min \n",
      " 54   | 0.1965  | 0.1002  | 0.3856  |  0 hr 02 min \n",
      " 55   | 0.2137  | 0.0882  | 0.3733  |  0 hr 02 min \n",
      " 56   | 0.2021  | 0.1441  | 0.4403  |  0 hr 02 min \n",
      " 57   | 0.1740  | 0.1175  | 0.3832  |  0 hr 02 min \n",
      " 58   | 0.2490  | 0.1074  | 0.4287  |  0 hr 02 min \n",
      " 59   | 0.1728  | 0.1048  | 0.4007  |  0 hr 02 min \n",
      " 60   | 0.2020  | 0.1078  | 0.4218  |  0 hr 02 min \n",
      " 61   | 0.1928  | 0.0983  | 0.3753  |  0 hr 02 min \n",
      " 62   | 0.1785  | 0.0909  | 0.4280  |  0 hr 02 min \n",
      " 63   | 0.1904  | 0.1029  | 0.3770  |  0 hr 02 min \n",
      " 64   | 0.1553  | 0.0876  | 0.4014  |  0 hr 02 min \n",
      " 65   | 0.1639  | 0.0827  | 0.3945  |  0 hr 02 min \n",
      " 66   | 0.1755  | 0.1289  | 0.4501  |  0 hr 02 min \n",
      " 67   | 0.2100  | 0.1355  | 0.4652  |  0 hr 02 min \n",
      " 68   | 0.2000  | 0.1018  | 0.3866  |  0 hr 02 min \n",
      " 69   | 0.2111  | 0.1121  | 0.4250  |  0 hr 03 min \n",
      " 70   | 0.1908  | 0.1120  | 0.4059  |  0 hr 03 min \n",
      " 71   | 0.2278  | 0.1508  | 0.4457  |  0 hr 03 min \n",
      " 72   | 0.2227  | 0.0732  | 0.3906  |  0 hr 03 min \n",
      " 73   | 0.2704  | 0.1865  | 0.5336  |  0 hr 03 min \n",
      " 74   | 0.2138  | 0.1197  | 0.4335  |  0 hr 03 min \n",
      " 75   | 0.2307  | 0.1009  | 0.3906  |  0 hr 03 min \n",
      " 76   | 0.2151  | 0.1529  | 0.4799  |  0 hr 03 min \n",
      " 77   | 0.1747  | 0.0855  | 0.3747  |  0 hr 03 min \n",
      " 78   | 0.2151  | 0.0746  | 0.3990  |  0 hr 03 min \n",
      " 79   | 0.1587  | 0.0984  | 0.4436  |  0 hr 03 min \n",
      " 80   | 0.1759  | 0.0867  | 0.4248  |  0 hr 03 min \n",
      " 81   | 0.1407  | 0.0617  | 0.4136  |  0 hr 03 min \n",
      " 82   | 0.1415  | 0.0638  | 0.4034  |  0 hr 03 min \n",
      " 83   | 0.1410  | 0.0683  | 0.3828  |  0 hr 03 min \n",
      " 84   | 0.1459  | 0.0660  | 0.4005  |  0 hr 03 min \n",
      " 85   | 0.1444  | 0.0669  | 0.3854  |  0 hr 03 min \n",
      " 86   | 0.1510  | 0.0691  | 0.3743  |  0 hr 03 min \n",
      " 87   | 0.1500  | 0.0552  | 0.3591  |  0 hr 03 min \n",
      " 88   | 0.1692  | 0.0557  | 0.3907  |  0 hr 03 min \n",
      " 89   | 0.1385  | 0.0662  | 0.4237  |  0 hr 03 min \n",
      " 90   | 0.1474  | 0.0621  | 0.4298  |  0 hr 03 min \n",
      " 91   | 0.1218  | 0.0705  | 0.4036  |  0 hr 03 min \n",
      " 92   | 0.1412  | 0.0572  | 0.3909  |  0 hr 03 min \n",
      " 93   | 0.1636  | 0.0522  | 0.3846  |  0 hr 04 min \n",
      " 94   | 0.1564  | 0.0813  | 0.4187  |  0 hr 04 min \n",
      " 95   | 0.1561  | 0.1144  | 0.4556  |  0 hr 04 min \n",
      " 96   | 0.1850  | 0.1101  | 0.4511  |  0 hr 04 min \n",
      " 97   | 0.1588  | 0.1067  | 0.4273  |  0 hr 04 min \n",
      " 98   | 0.2024  | 0.0982  | 0.3886  |  0 hr 04 min \n",
      " 99   | 0.1480  | 0.0760  | 0.4244  |  0 hr 04 min \n",
      " 100  | 0.1471  | 0.1018  | 0.4565  |  0 hr 04 min \n",
      " 101  | 0.1355  | 0.0517  | 0.3823  |  0 hr 04 min \n",
      " 102  | 0.1434  | 0.0617  | 0.4072  |  0 hr 04 min \n",
      " 103  | 0.1544  | 0.0729  | 0.4031  |  0 hr 04 min \n",
      " 104  | 0.1830  | 0.0883  | 0.4018  |  0 hr 04 min \n",
      " 105  | 0.1294  | 0.0619  | 0.4107  |  0 hr 04 min \n",
      " 106  | 0.1439  | 0.0529  | 0.3792  |  0 hr 04 min \n",
      " 107  | 0.1408  | 0.0696  | 0.4024  |  0 hr 04 min \n",
      " 108  | 0.1209  | 0.0468  | 0.3662  |  0 hr 04 min \n",
      " 109  | 0.1290  | 0.0579  | 0.3975  |  0 hr 04 min \n",
      " 110  | 0.1755  | 0.0592  | 0.3817  |  0 hr 04 min \n",
      " 111  | 0.1158  | 0.0546  | 0.4304  |  0 hr 04 min \n",
      " 112  | 0.2210  | 0.0602  | 0.4201  |  0 hr 04 min \n",
      " 113  | 0.1490  | 0.0502  | 0.3848  |  0 hr 04 min \n",
      " 114  | 0.1812  | 0.0804  | 0.4547  |  0 hr 04 min \n",
      " 115  | 0.1506  | 0.0493  | 0.4055  |  0 hr 04 min \n",
      " 116  | 0.1003  | 0.0417  | 0.3748  |  0 hr 05 min \n",
      " 117  | 0.1423  | 0.0533  | 0.4171  |  0 hr 05 min \n",
      " 118  | 0.1757  | 0.0623  | 0.4028  |  0 hr 05 min \n",
      " 119  | 0.2005  | 0.0470  | 0.3894  |  0 hr 05 min \n",
      " 120  | 0.1065  | 0.0465  | 0.3725  |  0 hr 05 min \n",
      " 121  | 0.1207  | 0.0620  | 0.3986  |  0 hr 05 min \n",
      " 122  | 0.1474  | 0.0479  | 0.3944  |  0 hr 05 min \n",
      " 123  | 0.1360  | 0.0526  | 0.4181  |  0 hr 05 min \n",
      " 124  | 0.0938  | 0.0409  | 0.3826  |  0 hr 05 min \n",
      " 125  | 0.1696  | 0.0453  | 0.3882  |  0 hr 05 min \n",
      " 126  | 0.1712  | 0.0685  | 0.3837  |  0 hr 05 min \n",
      " 127  | 0.1198  | 0.0462  | 0.3859  |  0 hr 05 min \n",
      " 128  | 0.1030  | 0.0382  | 0.3456  |  0 hr 05 min \n",
      " 129  | 0.1213  | 0.0355  | 0.3771  |  0 hr 05 min \n",
      " 130  | 0.1108  | 0.0418  | 0.3936  |  0 hr 05 min \n",
      " 131  | 0.1164  | 0.0537  | 0.3805  |  0 hr 05 min \n",
      " 132  | 0.1042  | 0.0420  | 0.3527  |  0 hr 05 min \n",
      " 133  | 0.1251  | 0.0484  | 0.3934  |  0 hr 05 min \n",
      " 134  | 0.1218  | 0.0478  | 0.3544  |  0 hr 05 min \n",
      " 135  | 0.1376  | 0.0593  | 0.4080  |  0 hr 05 min \n",
      " 136  | 0.1107  | 0.0403  | 0.3576  |  0 hr 05 min \n",
      " 137  | 0.1206  | 0.0390  | 0.3734  |  0 hr 05 min \n",
      " 138  | 0.1597  | 0.0430  | 0.3537  |  0 hr 05 min \n",
      " 139  | 0.1138  | 0.0413  | 0.3832  |  0 hr 05 min \n",
      " 140  | 0.0866  | 0.0352  | 0.3465  |  0 hr 06 min \n",
      " 141  | 0.1159  | 0.0405  | 0.3668  |  0 hr 06 min \n",
      " 142  | 0.0968  | 0.0421  | 0.3853  |  0 hr 06 min \n",
      " 143  | 0.1236  | 0.0372  | 0.3501  |  0 hr 06 min \n",
      " 144  | 0.1657  | 0.0511  | 0.4007  |  0 hr 06 min \n",
      " 145  | 0.1291  | 0.0510  | 0.4039  |  0 hr 06 min \n",
      " 146  | 0.1098  | 0.0340  | 0.3808  |  0 hr 06 min \n",
      " 147  | 0.1538  | 0.0361  | 0.3564  |  0 hr 06 min \n",
      " 148  | 0.0992  | 0.0544  | 0.4082  |  0 hr 06 min \n",
      " 149  | 0.1530  | 0.0461  | 0.4175  |  0 hr 06 min \n",
      " 150  | 0.1140  | 0.0640  | 0.3972  |  0 hr 06 min \n",
      " 151  | 0.1054  | 0.0488  | 0.4162  |  0 hr 06 min \n",
      " 152  | 0.1285  | 0.0491  | 0.4002  |  0 hr 06 min \n",
      " 153  | 0.0828  | 0.0382  | 0.3889  |  0 hr 06 min \n",
      " 154  | 0.1120  | 0.0311  | 0.3641  |  0 hr 06 min \n",
      " 155  | 0.1334  | 0.0477  | 0.3785  |  0 hr 06 min \n",
      " 156  | 0.1107  | 0.0427  | 0.3981  |  0 hr 06 min \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 157  | 0.1251  | 0.0474  | 0.4061  |  0 hr 06 min \n",
      " 158  | 0.1232  | 0.0382  | 0.3829  |  0 hr 06 min \n",
      " 159  | 0.1371  | 0.0452  | 0.3878  |  0 hr 06 min \n",
      " 160  | 0.1317  | 0.0615  | 0.4364  |  0 hr 06 min \n",
      " 161  | 0.0980  | 0.0448  | 0.3593  |  0 hr 06 min \n",
      " 162  | 0.1187  | 0.0447  | 0.3904  |  0 hr 06 min \n",
      " 163  | 0.1385  | 0.0445  | 0.3890  |  0 hr 06 min \n",
      " 164  | 0.1263  | 0.0376  | 0.3693  |  0 hr 07 min \n",
      " 165  | 0.1417  | 0.0376  | 0.3765  |  0 hr 07 min \n",
      "fold | epoch | train_MSE | valid MSE \n",
      "  0   |  128  | 0.0311  | 0.3456  \n",
      "  0   | 8.2000  | 8.3639  | 7.6634  |  0 hr 07 min \n",
      "  1   | 3.1394  | 3.5846  | 2.9984  |  0 hr 07 min \n",
      "  2   | 1.1530  | 5.1014  | 4.7351  |  0 hr 07 min \n",
      "  3   | 0.7967  | 1.6694  | 1.6866  |  0 hr 07 min \n",
      "  4   | 0.8080  | 0.7121  | 0.6699  |  0 hr 07 min \n",
      "  5   | 0.6816  | 0.5594  | 0.5995  |  0 hr 07 min \n",
      "  6   | 0.5602  | 0.4078  | 0.4741  |  0 hr 07 min \n",
      "  7   | 0.5257  | 0.3024  | 0.4107  |  0 hr 07 min \n",
      "  8   | 0.4900  | 0.3481  | 0.4378  |  0 hr 07 min \n",
      "  9   | 0.4265  | 0.4524  | 0.4699  |  0 hr 07 min \n",
      " 10   | 0.4436  | 0.2952  | 0.3903  |  0 hr 07 min \n",
      " 11   | 0.4905  | 0.3246  | 0.5343  |  0 hr 07 min \n",
      " 12   | 0.4217  | 0.2593  | 0.3269  |  0 hr 07 min \n",
      " 13   | 0.3374  | 0.2559  | 0.3468  |  0 hr 07 min \n",
      " 14   | 0.3619  | 0.3115  | 0.3580  |  0 hr 07 min \n",
      " 15   | 0.3875  | 0.2228  | 0.3194  |  0 hr 07 min \n",
      " 16   | 0.3409  | 0.2876  | 0.3897  |  0 hr 07 min \n",
      " 17   | 0.3595  | 0.2511  | 0.3513  |  0 hr 07 min \n",
      " 18   | 0.3812  | 0.2831  | 0.4132  |  0 hr 07 min \n",
      " 19   | 0.3593  | 0.2892  | 0.4795  |  0 hr 07 min \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f00ec54b2f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data2/erikxiong/miniconda3/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 30884, 30888, 30890) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6272827898b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtraine_MAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mvalid_MAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4a8b8c0f49f0>\u001b[0m in \u001b[0;36meval\u001b[0;34m(smiles_list)\u001b[0m\n\u001b[1;32m     28\u001b[0m     eval_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n\u001b[1;32m     29\u001b[0m                               num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0matom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mbond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 30884, 30888, 30890) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "\n",
    "log = Logger()\n",
    "log.open(f'log/{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "start = timer()\n",
    "\n",
    "log2 = Logger()\n",
    "log2.open(f'{prefix_filename}_best_{start_time}.txt')\n",
    "f2 = '{:^5} | {:^5} | {:^7.4f} | {:^7.4f} \\n'\n",
    "\n",
    "for fold_index in range(5):\n",
    "    \n",
    "    model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "    \n",
    "    best_param ={}\n",
    "    best_param[\"train_epoch\"] = 0\n",
    "    best_param[\"valid_epoch\"] = 0\n",
    "    best_param[\"train_MSE\"] = 800\n",
    "    best_param[\"valid_MSE\"] = 800\n",
    "    for epoch in range(800):\n",
    "        losses = train(smiles_list[train_fold[fold_index]])\n",
    "        traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "        valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "        \n",
    "        timing = time_to_str((timer() - start), 'min')  \n",
    "        log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "        \n",
    "        if train_MSE < best_param[\"train_MSE\"]:\n",
    "            best_param[\"train_epoch\"] = epoch\n",
    "            best_param[\"train_MSE\"] = train_MSE\n",
    "        if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "            best_param[\"valid_epoch\"] = epoch\n",
    "            best_param[\"valid_MSE\"] = valid_MSE\n",
    "#             if valid_MSE < 0.35:\n",
    "#                  torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "        if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "            break\n",
    "\n",
    "    log2.write('fold | epoch | train_MSE | valid MSE \\n')\n",
    "    log2.write(f2.format(fold_index, best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model\n",
    "# best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"valid_epoch\"])+'.pt')     \n",
    "\n",
    "# best_model_dict = best_model.state_dict()\n",
    "# best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "# (best_model.align[0].weight == model.align[0].weight).all()\n",
    "# test_MAE, test_MSE = eval(model, test_df)\n",
    "# print(\"best epoch:\",best_param[\"test_epoch\"],\"\\n\",\"test MSE:\",test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range(20):\n",
    "#     losses = train(smiles_list[valid_fold[fold_index]])\n",
    "#     print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
