{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "import random\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#then import my own modules\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight\n",
    "from timeit import default_timer as timer\n",
    "from AttentiveFP.featurizing import graph_dict\n",
    "from AttentiveFP.AttentiveLayers_new_resnet import Fingerprint, graph_dataset, null_collate, Graph, Logger, time_to_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_aviable = torch.cuda.is_available()\n",
    "device = torch.device(0)\n",
    "\n",
    "SEED = 8\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1128\n",
      "number of successfully processed smiles:  1128\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'solubility'\n",
    "tasks = ['measured log solubility in mols per litre']\n",
    "\n",
    "raw_filename = \"../data/delaney-processed.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \", len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)].reset_index()\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 200\n",
    "\n",
    "p_dropout= 0.2\n",
    "fingerprint_dim = 32\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 2.5\n",
    "K = 5\n",
    "T = 3\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph dicts loaded from ../data/delaney-processed.pkl\n"
     ]
    }
   ],
   "source": [
    "smiles_list = smiles_tasks_df['smiles'].values\n",
    "label_list = smiles_tasks_df[tasks[0]].values\n",
    "graph_dict = graph_dict(smiles_list, label_list, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "train_fold = []\n",
    "valid_fold = []\n",
    "for k, (train_idx, valid_idx) in enumerate(kfold.split(smiles_list)):\n",
    "    train_fold.append(train_idx)\n",
    "    valid_fold.append(valid_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    3,    5,    8,   13,   15,   25,   27,   33,   47,   49,\n",
       "         59,   63,   71,   84,   95,   96,  108,  119,  122,  135,  138,\n",
       "        149,  158,  160,  161,  162,  164,  182,  186,  188,  190,  191,\n",
       "        203,  211,  213,  219,  221,  223,  227,  228,  234,  240,  260,\n",
       "        272,  273,  279,  287,  294,  298,  303,  310,  311,  313,  314,\n",
       "        315,  320,  329,  335,  338,  342,  348,  350,  352,  358,  360,\n",
       "        370,  371,  373,  375,  395,  401,  402,  407,  409,  412,  420,\n",
       "        436,  443,  455,  459,  465,  473,  477,  493,  496,  499,  500,\n",
       "        501,  521,  522,  524,  534,  545,  549,  550,  558,  567,  583,\n",
       "        590,  592,  601,  604,  606,  608,  615,  621,  622,  623,  629,\n",
       "        631,  633,  639,  644,  649,  655,  658,  666,  668,  669,  670,\n",
       "        675,  680,  686,  687,  689,  693,  695,  696,  697,  707,  718,\n",
       "        719,  725,  726,  731,  741,  745,  751,  753,  757,  760,  764,\n",
       "        768,  780,  781,  786,  790,  791,  793,  794,  802,  804,  814,\n",
       "        818,  821,  824,  826,  831,  832,  836,  837,  844,  845,  855,\n",
       "        859,  864,  868,  871,  879,  881,  882,  883,  888,  889,  890,\n",
       "        903,  905,  907,  908,  910,  919,  925,  926,  931,  936,  939,\n",
       "        940,  944,  953,  956,  959,  968,  969,  976,  977,  979,  982,\n",
       "       1000, 1001, 1014, 1016, 1023, 1025, 1031, 1039, 1046, 1059, 1060,\n",
       "       1064, 1069, 1072, 1075, 1076, 1080, 1083, 1084, 1095, 1101, 1102,\n",
       "       1108, 1116, 1117, 1123, 1126, 1127])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_fold[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152105\n",
      "preprocess.0.linear.weight torch.Size([32, 39])\n",
      "preprocess.0.linear.bias torch.Size([32])\n",
      "preprocess.0.bn.weight torch.Size([32])\n",
      "preprocess.0.bn.bias torch.Size([32])\n",
      "resnetBn.0.0.linear.weight torch.Size([32, 32])\n",
      "resnetBn.0.0.linear.bias torch.Size([32])\n",
      "resnetBn.0.0.bn.weight torch.Size([32])\n",
      "resnetBn.0.0.bn.bias torch.Size([32])\n",
      "resnetBn.1.0.linear.weight torch.Size([32, 32])\n",
      "resnetBn.1.0.linear.bias torch.Size([32])\n",
      "resnetBn.1.0.bn.weight torch.Size([32])\n",
      "resnetBn.1.0.bn.bias torch.Size([32])\n",
      "resnetBn.2.0.linear.weight torch.Size([32, 32])\n",
      "resnetBn.2.0.linear.bias torch.Size([32])\n",
      "resnetBn.2.0.bn.weight torch.Size([32])\n",
      "resnetBn.2.0.bn.bias torch.Size([32])\n",
      "resnetBn.3.0.linear.weight torch.Size([32, 32])\n",
      "resnetBn.3.0.linear.bias torch.Size([32])\n",
      "resnetBn.3.0.bn.weight torch.Size([32])\n",
      "resnetBn.3.0.bn.bias torch.Size([32])\n",
      "resnetBn.4.0.linear.weight torch.Size([32, 32])\n",
      "resnetBn.4.0.linear.bias torch.Size([32])\n",
      "resnetBn.4.0.bn.weight torch.Size([32])\n",
      "resnetBn.4.0.bn.bias torch.Size([32])\n",
      "propagate.0.encoder.0.linear.weight torch.Size([1024, 10])\n",
      "propagate.0.encoder.0.linear.bias torch.Size([1024])\n",
      "propagate.0.encoder.0.bn.weight torch.Size([1024])\n",
      "propagate.0.encoder.0.bn.bias torch.Size([1024])\n",
      "propagate.0.align.weight torch.Size([1, 64])\n",
      "propagate.0.align.bias torch.Size([1])\n",
      "propagate.0.attend.linear.weight torch.Size([32, 32])\n",
      "propagate.0.attend.linear.bias torch.Size([32])\n",
      "propagate.0.attend.bn.weight torch.Size([32])\n",
      "propagate.0.attend.bn.bias torch.Size([32])\n",
      "propagate.0.gru.weight_ih torch.Size([96, 32])\n",
      "propagate.0.gru.weight_hh torch.Size([96, 32])\n",
      "propagate.0.gru.bias_ih torch.Size([96])\n",
      "propagate.0.gru.bias_hh torch.Size([96])\n",
      "propagate.1.encoder.0.linear.weight torch.Size([1024, 10])\n",
      "propagate.1.encoder.0.linear.bias torch.Size([1024])\n",
      "propagate.1.encoder.0.bn.weight torch.Size([1024])\n",
      "propagate.1.encoder.0.bn.bias torch.Size([1024])\n",
      "propagate.1.align.weight torch.Size([1, 64])\n",
      "propagate.1.align.bias torch.Size([1])\n",
      "propagate.1.attend.linear.weight torch.Size([32, 32])\n",
      "propagate.1.attend.linear.bias torch.Size([32])\n",
      "propagate.1.attend.bn.weight torch.Size([32])\n",
      "propagate.1.attend.bn.bias torch.Size([32])\n",
      "propagate.1.gru.weight_ih torch.Size([96, 32])\n",
      "propagate.1.gru.weight_hh torch.Size([96, 32])\n",
      "propagate.1.gru.bias_ih torch.Size([96])\n",
      "propagate.1.gru.bias_hh torch.Size([96])\n",
      "propagate.2.encoder.0.linear.weight torch.Size([1024, 10])\n",
      "propagate.2.encoder.0.linear.bias torch.Size([1024])\n",
      "propagate.2.encoder.0.bn.weight torch.Size([1024])\n",
      "propagate.2.encoder.0.bn.bias torch.Size([1024])\n",
      "propagate.2.align.weight torch.Size([1, 64])\n",
      "propagate.2.align.bias torch.Size([1])\n",
      "propagate.2.attend.linear.weight torch.Size([32, 32])\n",
      "propagate.2.attend.linear.bias torch.Size([32])\n",
      "propagate.2.attend.bn.weight torch.Size([32])\n",
      "propagate.2.attend.bn.bias torch.Size([32])\n",
      "propagate.2.gru.weight_ih torch.Size([96, 32])\n",
      "propagate.2.gru.weight_hh torch.Size([96, 32])\n",
      "propagate.2.gru.bias_ih torch.Size([96])\n",
      "propagate.2.gru.bias_hh torch.Size([96])\n",
      "propagate.3.encoder.0.linear.weight torch.Size([1024, 10])\n",
      "propagate.3.encoder.0.linear.bias torch.Size([1024])\n",
      "propagate.3.encoder.0.bn.weight torch.Size([1024])\n",
      "propagate.3.encoder.0.bn.bias torch.Size([1024])\n",
      "propagate.3.align.weight torch.Size([1, 64])\n",
      "propagate.3.align.bias torch.Size([1])\n",
      "propagate.3.attend.linear.weight torch.Size([32, 32])\n",
      "propagate.3.attend.linear.bias torch.Size([32])\n",
      "propagate.3.attend.bn.weight torch.Size([32])\n",
      "propagate.3.attend.bn.bias torch.Size([32])\n",
      "propagate.3.gru.weight_ih torch.Size([96, 32])\n",
      "propagate.3.gru.weight_hh torch.Size([96, 32])\n",
      "propagate.3.gru.bias_ih torch.Size([96])\n",
      "propagate.3.gru.bias_hh torch.Size([96])\n",
      "propagate.4.encoder.0.linear.weight torch.Size([1024, 10])\n",
      "propagate.4.encoder.0.linear.bias torch.Size([1024])\n",
      "propagate.4.encoder.0.bn.weight torch.Size([1024])\n",
      "propagate.4.encoder.0.bn.bias torch.Size([1024])\n",
      "propagate.4.align.weight torch.Size([1, 64])\n",
      "propagate.4.align.bias torch.Size([1])\n",
      "propagate.4.attend.linear.weight torch.Size([32, 32])\n",
      "propagate.4.attend.linear.bias torch.Size([32])\n",
      "propagate.4.attend.bn.weight torch.Size([32])\n",
      "propagate.4.attend.bn.bias torch.Size([32])\n",
      "propagate.4.gru.weight_ih torch.Size([96, 32])\n",
      "propagate.4.gru.weight_hh torch.Size([96, 32])\n",
      "propagate.4.gru.bias_ih torch.Size([96])\n",
      "propagate.4.gru.bias_hh torch.Size([96])\n",
      "superGather.0.align.weight torch.Size([1, 64])\n",
      "superGather.0.align.bias torch.Size([1])\n",
      "superGather.0.attend.linear.weight torch.Size([32, 32])\n",
      "superGather.0.attend.linear.bias torch.Size([32])\n",
      "superGather.0.attend.bn.weight torch.Size([32])\n",
      "superGather.0.attend.bn.bias torch.Size([32])\n",
      "superGather.0.gru.weight_ih torch.Size([96, 32])\n",
      "superGather.0.gru.weight_hh torch.Size([96, 32])\n",
      "superGather.0.gru.bias_ih torch.Size([96])\n",
      "superGather.0.gru.bias_hh torch.Size([96])\n",
      "superGather.1.align.weight torch.Size([1, 64])\n",
      "superGather.1.align.bias torch.Size([1])\n",
      "superGather.1.attend.linear.weight torch.Size([32, 32])\n",
      "superGather.1.attend.linear.bias torch.Size([32])\n",
      "superGather.1.attend.bn.weight torch.Size([32])\n",
      "superGather.1.attend.bn.bias torch.Size([32])\n",
      "superGather.1.gru.weight_ih torch.Size([96, 32])\n",
      "superGather.1.gru.weight_hh torch.Size([96, 32])\n",
      "superGather.1.gru.bias_ih torch.Size([96])\n",
      "superGather.1.gru.bias_hh torch.Size([96])\n",
      "superGather.2.align.weight torch.Size([1, 64])\n",
      "superGather.2.align.bias torch.Size([1])\n",
      "superGather.2.attend.linear.weight torch.Size([32, 32])\n",
      "superGather.2.attend.linear.bias torch.Size([32])\n",
      "superGather.2.attend.bn.weight torch.Size([32])\n",
      "superGather.2.attend.bn.bias torch.Size([32])\n",
      "superGather.2.gru.weight_ih torch.Size([96, 32])\n",
      "superGather.2.gru.weight_hh torch.Size([96, 32])\n",
      "superGather.2.gru.bias_ih torch.Size([96])\n",
      "superGather.2.gru.bias_hh torch.Size([96])\n",
      "predict.0.linear.weight torch.Size([512, 32])\n",
      "predict.0.linear.bias torch.Size([512])\n",
      "predict.0.bn.weight torch.Size([512])\n",
      "predict.0.bn.bias torch.Size([512])\n",
      "predict.3.weight torch.Size([1, 512])\n",
      "predict.3.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "# optimizer = optim.SGD(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in  model.parameters()])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(smiles_list):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=True, worker_init_fn=np.random.seed(SEED))\n",
    "    losses = []\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(train_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        \n",
    "        loss = loss_function(mol_prediction, label.view(-1,1))     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "        \n",
    "def eval(smiles_list):\n",
    "    model.eval()\n",
    "    eval_MAE_list = []\n",
    "    eval_MSE_list = []\n",
    "    eval_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(eval_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        MAE = F.l1_loss(mol_prediction, label.view(-1,1), reduction='none')        \n",
    "        MSE = F.mse_loss(mol_prediction, label.view(-1,1), reduction='none')\n",
    "        \n",
    "        eval_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        eval_MSE_list.extend(MSE.data.squeeze().cpu().numpy())\n",
    "    return np.array(eval_MAE_list).mean(), np.array(eval_MSE_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = Logger()\n",
    "# log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "# f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "# log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "# start = timer()\n",
    "\n",
    "# best_param ={}\n",
    "# best_param[\"train_epoch\"] = 0\n",
    "# best_param[\"valid_epoch\"] = 0\n",
    "# best_param[\"train_MSE\"] = 9e8\n",
    "# best_param[\"valid_MSE\"] = 9e8\n",
    "\n",
    "# fold_index = 0\n",
    "# for epoch in range(800):\n",
    "#     losses = train(smiles_list[train_fold[fold_index]])\n",
    "#     traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "#     valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "\n",
    "#     timing = time_to_str((timer() - start), 'min')  \n",
    "#     log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "\n",
    "#     if train_MSE < best_param[\"train_MSE\"]:\n",
    "#         best_param[\"train_epoch\"] = epoch\n",
    "#         best_param[\"train_MSE\"] = train_MSE\n",
    "#     if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#         best_param[\"valid_epoch\"] = epoch\n",
    "#         best_param[\"valid_MSE\"] = valid_MSE\n",
    "# #         if valid_MSE < 0.35:\n",
    "# #              torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#     if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "#         break\n",
    "# print(best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | loss | train MSE |  valid MSE |  time \n",
      "  0   | 12.6553 | 11.5771 | 11.5978 |  0 hr 00 min \n",
      "  1   | 4.2940  | 6.8260  | 6.9524  |  0 hr 00 min \n",
      "  2   | 2.1169  | 2.9474  | 3.7891  |  0 hr 00 min \n",
      "  3   | 2.5129  | 3.5664  | 4.4849  |  0 hr 00 min \n",
      "  4   | 1.9650  | 3.8425  | 4.2641  |  0 hr 00 min \n",
      "  5   | 1.3013  | 2.1679  | 2.7987  |  0 hr 00 min \n",
      "  6   | 1.1227  | 1.7325  | 2.2356  |  0 hr 00 min \n",
      "  7   | 1.1166  | 1.3496  | 1.6748  |  0 hr 00 min \n",
      "  8   | 0.9252  | 1.0690  | 1.3680  |  0 hr 00 min \n",
      "  9   | 0.9163  | 1.1103  | 1.4896  |  0 hr 00 min \n",
      " 10   | 0.8691  | 0.7510  | 0.9832  |  0 hr 00 min \n",
      " 11   | 0.7727  | 0.6582  | 0.8540  |  0 hr 00 min \n",
      " 12   | 0.7035  | 0.5703  | 0.8128  |  0 hr 00 min \n",
      " 13   | 0.6015  | 0.5194  | 0.7059  |  0 hr 00 min \n",
      " 14   | 0.5492  | 0.4447  | 0.7162  |  0 hr 00 min \n",
      " 15   | 0.5076  | 0.4117  | 0.6563  |  0 hr 00 min \n",
      " 16   | 0.4866  | 0.3775  | 0.5997  |  0 hr 00 min \n",
      " 17   | 0.4821  | 0.3642  | 0.6176  |  0 hr 00 min \n",
      " 18   | 0.4500  | 0.3301  | 0.5804  |  0 hr 00 min \n",
      " 19   | 0.4132  | 0.3576  | 0.6137  |  0 hr 00 min \n",
      " 20   | 0.4411  | 0.3297  | 0.5920  |  0 hr 00 min \n",
      " 21   | 0.4707  | 0.3181  | 0.5829  |  0 hr 00 min \n",
      " 22   | 0.4214  | 0.2842  | 0.5465  |  0 hr 00 min \n",
      " 23   | 0.4014  | 0.2983  | 0.5834  |  0 hr 00 min \n",
      " 24   | 0.4414  | 0.2628  | 0.4994  |  0 hr 00 min \n",
      " 25   | 0.4349  | 0.3517  | 0.5912  |  0 hr 00 min \n",
      " 26   | 0.4413  | 0.2787  | 0.5196  |  0 hr 00 min \n",
      " 27   | 0.3979  | 0.2855  | 0.5092  |  0 hr 00 min \n",
      " 28   | 0.4151  | 0.2734  | 0.5245  |  0 hr 00 min \n",
      " 29   | 0.3524  | 0.2484  | 0.4918  |  0 hr 00 min \n",
      " 30   | 0.3187  | 0.2356  | 0.5212  |  0 hr 00 min \n",
      " 31   | 0.3184  | 0.2218  | 0.5197  |  0 hr 00 min \n",
      " 32   | 0.2977  | 0.2450  | 0.4879  |  0 hr 00 min \n",
      " 33   | 0.3794  | 0.2468  | 0.4908  |  0 hr 00 min \n",
      " 34   | 0.3540  | 0.2655  | 0.5018  |  0 hr 01 min \n",
      " 35   | 0.3258  | 0.2243  | 0.4685  |  0 hr 01 min \n",
      " 36   | 0.3114  | 0.2350  | 0.5029  |  0 hr 01 min \n",
      " 37   | 0.2909  | 0.2730  | 0.4997  |  0 hr 01 min \n",
      " 38   | 0.3737  | 0.2205  | 0.5083  |  0 hr 01 min \n",
      " 39   | 0.3833  | 0.3691  | 0.6155  |  0 hr 01 min \n",
      " 40   | 0.4022  | 0.3726  | 0.5342  |  0 hr 01 min \n",
      " 41   | 0.4473  | 0.4342  | 0.6869  |  0 hr 01 min \n",
      " 42   | 0.4081  | 0.4114  | 0.6215  |  0 hr 01 min \n",
      " 43   | 0.4084  | 0.3076  | 0.5757  |  0 hr 01 min \n",
      " 44   | 0.3395  | 0.3029  | 0.5673  |  0 hr 01 min \n",
      " 45   | 0.3542  | 0.2716  | 0.5360  |  0 hr 01 min \n",
      " 46   | 0.3493  | 0.3196  | 0.5088  |  0 hr 01 min \n",
      " 47   | 0.3571  | 0.3023  | 0.5593  |  0 hr 01 min \n",
      " 48   | 0.3386  | 0.2498  | 0.4883  |  0 hr 01 min \n",
      " 49   | 0.3692  | 0.2522  | 0.4126  |  0 hr 01 min \n",
      " 50   | 0.3481  | 0.2443  | 0.4337  |  0 hr 01 min \n",
      " 51   | 0.3355  | 0.2883  | 0.4966  |  0 hr 01 min \n",
      " 52   | 0.3527  | 0.2581  | 0.4695  |  0 hr 01 min \n",
      " 53   | 0.3890  | 0.2167  | 0.4658  |  0 hr 01 min \n",
      " 54   | 0.3046  | 0.2201  | 0.4782  |  0 hr 01 min \n",
      " 55   | 0.2777  | 0.2382  | 0.4527  |  0 hr 01 min \n",
      " 56   | 0.2838  | 0.2106  | 0.4611  |  0 hr 01 min \n",
      " 57   | 0.2920  | 0.2064  | 0.5163  |  0 hr 01 min \n",
      " 58   | 0.3372  | 0.2060  | 0.4848  |  0 hr 01 min \n",
      " 59   | 0.3079  | 0.2029  | 0.4525  |  0 hr 01 min \n",
      " 60   | 0.3075  | 0.1868  | 0.3972  |  0 hr 01 min \n",
      " 61   | 0.2664  | 0.1822  | 0.3716  |  0 hr 01 min \n",
      " 62   | 0.2528  | 0.1896  | 0.4198  |  0 hr 01 min \n",
      " 63   | 0.2590  | 0.1956  | 0.4409  |  0 hr 01 min \n",
      " 64   | 0.2171  | 0.1899  | 0.4334  |  0 hr 01 min \n",
      " 65   | 0.2610  | 0.1620  | 0.4380  |  0 hr 01 min \n",
      " 66   | 0.2543  | 0.1817  | 0.4756  |  0 hr 01 min \n",
      " 67   | 0.2758  | 0.1889  | 0.4917  |  0 hr 01 min \n",
      " 68   | 0.2544  | 0.1513  | 0.4650  |  0 hr 02 min \n",
      " 69   | 0.2711  | 0.1545  | 0.4258  |  0 hr 02 min \n",
      " 70   | 0.2435  | 0.1965  | 0.4415  |  0 hr 02 min \n",
      " 71   | 0.3088  | 0.1565  | 0.4089  |  0 hr 02 min \n",
      " 72   | 0.2511  | 0.1803  | 0.4094  |  0 hr 02 min \n",
      " 73   | 0.3088  | 0.1717  | 0.4905  |  0 hr 02 min \n",
      " 74   | 0.2963  | 0.1818  | 0.4585  |  0 hr 02 min \n",
      " 75   | 0.2459  | 0.1654  | 0.4541  |  0 hr 02 min \n",
      " 76   | 0.2356  | 0.1725  | 0.4222  |  0 hr 02 min \n",
      " 77   | 0.2293  | 0.1459  | 0.3611  |  0 hr 02 min \n",
      " 78   | 0.1853  | 0.1371  | 0.4213  |  0 hr 02 min \n",
      " 79   | 0.2312  | 0.1369  | 0.4323  |  0 hr 02 min \n",
      " 80   | 0.2370  | 0.1339  | 0.3705  |  0 hr 02 min \n",
      " 81   | 0.2154  | 0.1408  | 0.4402  |  0 hr 02 min \n",
      " 82   | 0.1743  | 0.1264  | 0.3992  |  0 hr 02 min \n",
      " 83   | 0.1825  | 0.1213  | 0.4102  |  0 hr 02 min \n",
      " 84   | 0.2093  | 0.1249  | 0.4291  |  0 hr 02 min \n",
      " 85   | 0.1896  | 0.1163  | 0.4015  |  0 hr 02 min \n",
      " 86   | 0.1762  | 0.1171  | 0.4061  |  0 hr 02 min \n",
      " 87   | 0.1900  | 0.1167  | 0.4299  |  0 hr 02 min \n",
      " 88   | 0.2039  | 0.1220  | 0.3797  |  0 hr 02 min \n",
      " 89   | 0.2016  | 0.1277  | 0.4100  |  0 hr 02 min \n",
      " 90   | 0.2073  | 0.1243  | 0.4422  |  0 hr 02 min \n",
      " 91   | 0.1917  | 0.1181  | 0.4315  |  0 hr 02 min \n",
      " 92   | 0.1878  | 0.1192  | 0.4769  |  0 hr 02 min \n",
      " 93   | 0.2165  | 0.1234  | 0.4037  |  0 hr 02 min \n",
      " 94   | 0.2251  | 0.1304  | 0.4408  |  0 hr 02 min \n",
      " 95   | 0.2106  | 0.1207  | 0.4540  |  0 hr 02 min \n",
      " 96   | 0.2400  | 0.1429  | 0.4260  |  0 hr 02 min \n",
      "fold | epoch | train_MSE | valid MSE \n",
      "  0   |  77   | 0.1163  | 0.3611  \n",
      "  0   | 9.7721  | 10.1289 | 9.5586  |  0 hr 02 min \n",
      "  1   | 2.9975  | 5.1953  | 4.5701  |  0 hr 02 min \n",
      "  2   | 2.1691  | 3.7872  | 3.0678  |  0 hr 02 min \n",
      "  3   | 1.9089  | 3.3671  | 2.7132  |  0 hr 02 min \n",
      "  4   | 1.3589  | 3.6686  | 3.1982  |  0 hr 02 min \n",
      "  5   | 1.1961  | 3.0699  | 2.3677  |  0 hr 02 min \n",
      "  6   | 0.9630  | 1.7505  | 1.3196  |  0 hr 03 min \n",
      "  7   | 0.8015  | 1.2699  | 1.1036  |  0 hr 03 min \n",
      "  8   | 0.7171  | 0.7323  | 0.5952  |  0 hr 03 min \n",
      "  9   | 0.6040  | 0.5729  | 0.5453  |  0 hr 03 min \n",
      " 10   | 0.5951  | 0.6098  | 0.6608  |  0 hr 03 min \n",
      " 11   | 0.5674  | 0.5699  | 0.6515  |  0 hr 03 min \n",
      " 12   | 0.4979  | 0.5248  | 0.6488  |  0 hr 03 min \n",
      " 13   | 0.5308  | 0.5364  | 0.6297  |  0 hr 03 min \n",
      " 14   | 0.4657  | 0.4635  | 0.5724  |  0 hr 03 min \n",
      " 15   | 0.4947  | 0.4243  | 0.5208  |  0 hr 03 min \n",
      " 16   | 0.4313  | 0.3768  | 0.5295  |  0 hr 03 min \n",
      " 17   | 0.4292  | 0.3249  | 0.4597  |  0 hr 03 min \n",
      " 18   | 0.3986  | 0.3108  | 0.4218  |  0 hr 03 min \n",
      " 19   | 0.3999  | 0.2971  | 0.4528  |  0 hr 03 min \n",
      " 20   | 0.3536  | 0.2886  | 0.4355  |  0 hr 03 min \n",
      " 21   | 0.4021  | 0.2811  | 0.4122  |  0 hr 03 min \n",
      " 22   | 0.3852  | 0.3439  | 0.4444  |  0 hr 03 min \n",
      " 23   | 0.3690  | 0.2734  | 0.4465  |  0 hr 03 min \n",
      " 24   | 0.3489  | 0.2602  | 0.4594  |  0 hr 03 min \n",
      " 25   | 0.4122  | 0.2707  | 0.4654  |  0 hr 03 min \n",
      " 26   | 0.3730  | 0.2577  | 0.4073  |  0 hr 03 min \n",
      " 27   | 0.3860  | 0.2661  | 0.4602  |  0 hr 03 min \n",
      " 28   | 0.3259  | 0.2267  | 0.3486  |  0 hr 03 min \n",
      " 29   | 0.3238  | 0.2185  | 0.3615  |  0 hr 03 min \n",
      " 30   | 0.2848  | 0.2042  | 0.3770  |  0 hr 03 min \n",
      " 31   | 0.3866  | 0.2243  | 0.4224  |  0 hr 03 min \n",
      " 32   | 0.3214  | 0.2527  | 0.4350  |  0 hr 03 min \n",
      " 33   | 0.3153  | 0.2602  | 0.4557  |  0 hr 03 min \n",
      " 34   | 0.3357  | 0.2843  | 0.4306  |  0 hr 03 min \n",
      " 35   | 0.3458  | 0.2219  | 0.4453  |  0 hr 03 min \n",
      " 36   | 0.2782  | 0.2074  | 0.4178  |  0 hr 03 min \n",
      " 37   | 0.3190  | 0.2201  | 0.4210  |  0 hr 03 min \n",
      " 38   | 0.3331  | 0.2292  | 0.4463  |  0 hr 03 min \n",
      " 39   | 0.3150  | 0.1813  | 0.3748  |  0 hr 03 min \n",
      " 40   | 0.3040  | 0.1908  | 0.3837  |  0 hr 03 min \n",
      " 41   | 0.2596  | 0.2010  | 0.4073  |  0 hr 04 min \n",
      " 42   | 0.2817  | 0.1962  | 0.3823  |  0 hr 04 min \n",
      " 43   | 0.2926  | 0.1704  | 0.3803  |  0 hr 04 min \n",
      " 44   | 0.2756  | 0.1775  | 0.3827  |  0 hr 04 min \n",
      " 45   | 0.2613  | 0.2075  | 0.4066  |  0 hr 04 min \n",
      " 46   | 0.3148  | 0.1682  | 0.3491  |  0 hr 04 min \n",
      " 47   | 0.2538  | 0.2032  | 0.3941  |  0 hr 04 min \n",
      " 48   | 0.2610  | 0.2517  | 0.4627  |  0 hr 04 min \n",
      " 49   | 0.2928  | 0.1822  | 0.4425  |  0 hr 04 min \n",
      " 50   | 0.2716  | 0.1557  | 0.3753  |  0 hr 04 min \n",
      " 51   | 0.2655  | 0.1815  | 0.4070  |  0 hr 04 min \n",
      " 52   | 0.2378  | 0.1690  | 0.3717  |  0 hr 04 min \n",
      " 53   | 0.2428  | 0.1728  | 0.3719  |  0 hr 04 min \n",
      " 54   | 0.2776  | 0.2044  | 0.3754  |  0 hr 04 min \n",
      " 55   | 0.2700  | 0.1648  | 0.3717  |  0 hr 04 min \n",
      " 56   | 0.2633  | 0.1847  | 0.3797  |  0 hr 04 min \n",
      " 57   | 0.2440  | 0.1478  | 0.3647  |  0 hr 04 min \n",
      " 58   | 0.2229  | 0.1976  | 0.3759  |  0 hr 04 min \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59   | 0.3362  | 0.1404  | 0.3787  |  0 hr 04 min \n",
      " 60   | 0.2625  | 0.1366  | 0.3775  |  0 hr 04 min \n",
      " 61   | 0.2354  | 0.1895  | 0.4057  |  0 hr 04 min \n",
      " 62   | 0.2617  | 0.1856  | 0.3801  |  0 hr 04 min \n",
      " 63   | 0.2520  | 0.1930  | 0.4504  |  0 hr 04 min \n",
      " 64   | 0.2379  | 0.1651  | 0.3868  |  0 hr 04 min \n",
      " 65   | 0.2615  | 0.1388  | 0.3646  |  0 hr 04 min \n",
      " 66   | 0.2385  | 0.1569  | 0.3782  |  0 hr 04 min \n",
      " 67   | 0.2342  | 0.1673  | 0.3841  |  0 hr 04 min \n",
      " 68   | 0.2391  | 0.1577  | 0.3946  |  0 hr 04 min \n",
      " 69   | 0.2077  | 0.1350  | 0.3933  |  0 hr 04 min \n",
      " 70   | 0.2108  | 0.1257  | 0.3685  |  0 hr 04 min \n",
      " 71   | 0.2159  | 0.1231  | 0.3496  |  0 hr 04 min \n",
      " 72   | 0.1892  | 0.1310  | 0.3108  |  0 hr 04 min \n",
      " 73   | 0.2459  | 0.1243  | 0.3502  |  0 hr 04 min \n",
      " 74   | 0.2585  | 0.1761  | 0.4486  |  0 hr 04 min \n",
      " 75   | 0.2325  | 0.1107  | 0.3413  |  0 hr 05 min \n",
      " 76   | 0.2297  | 0.1914  | 0.4540  |  0 hr 05 min \n",
      " 77   | 0.1927  | 0.1495  | 0.4151  |  0 hr 05 min \n",
      " 78   | 0.1958  | 0.1152  | 0.4056  |  0 hr 05 min \n",
      " 79   | 0.2048  | 0.1177  | 0.4203  |  0 hr 05 min \n",
      " 80   | 0.2492  | 0.1420  | 0.4494  |  0 hr 05 min \n",
      " 81   | 0.1647  | 0.1102  | 0.3826  |  0 hr 05 min \n",
      " 82   | 0.1973  | 0.1288  | 0.4363  |  0 hr 05 min \n",
      " 83   | 0.2195  | 0.1126  | 0.3993  |  0 hr 05 min \n",
      " 84   | 0.2062  | 0.1182  | 0.4148  |  0 hr 05 min \n",
      " 85   | 0.2034  | 0.1137  | 0.4136  |  0 hr 05 min \n",
      " 86   | 0.1738  | 0.1137  | 0.4030  |  0 hr 05 min \n",
      " 87   | 0.2321  | 0.1616  | 0.4176  |  0 hr 05 min \n",
      " 88   | 0.2033  | 0.1068  | 0.3923  |  0 hr 05 min \n",
      " 89   | 0.1782  | 0.1000  | 0.4004  |  0 hr 05 min \n",
      " 90   | 0.1986  | 0.1218  | 0.3710  |  0 hr 05 min \n",
      " 91   | 0.1994  | 0.1120  | 0.3871  |  0 hr 05 min \n",
      " 92   | 0.2218  | 0.1091  | 0.3629  |  0 hr 05 min \n",
      " 93   | 0.2880  | 0.1374  | 0.3821  |  0 hr 05 min \n",
      " 94   | 0.1963  | 0.1591  | 0.4296  |  0 hr 05 min \n",
      " 95   | 0.2356  | 0.1700  | 0.4766  |  0 hr 05 min \n",
      " 96   | 0.2135  | 0.1979  | 0.4481  |  0 hr 05 min \n",
      " 97   | 0.1883  | 0.1722  | 0.4312  |  0 hr 05 min \n",
      " 98   | 0.2127  | 0.1302  | 0.3270  |  0 hr 05 min \n",
      " 99   | 0.2059  | 0.1610  | 0.4198  |  0 hr 05 min \n",
      " 100  | 0.1974  | 0.1200  | 0.3725  |  0 hr 05 min \n",
      "fold | epoch | train_MSE | valid MSE \n",
      "  1   |  72   | 0.1000  | 0.3108  \n",
      "  0   | 9.8222  | 10.4052 | 10.8118 |  0 hr 05 min \n",
      "  1   | 2.7354  | 5.9564  | 6.1257  |  0 hr 05 min \n",
      "  2   | 2.1236  | 3.7022  | 4.0144  |  0 hr 05 min \n",
      "  3   | 2.1778  | 3.4037  | 3.9516  |  0 hr 05 min \n",
      "  4   | 1.4437  | 4.5070  | 5.6763  |  0 hr 05 min \n",
      "  5   | 1.2444  | 3.4326  | 4.3601  |  0 hr 05 min \n",
      "  6   | 1.0962  | 3.1094  | 3.9923  |  0 hr 05 min \n",
      "  7   | 1.0239  | 2.2643  | 2.8197  |  0 hr 05 min \n",
      "  8   | 0.8110  | 0.9624  | 1.2812  |  0 hr 06 min \n",
      "  9   | 0.6724  | 0.7060  | 0.9109  |  0 hr 06 min \n",
      " 10   | 0.6496  | 0.5775  | 0.7339  |  0 hr 06 min \n",
      " 11   | 0.5344  | 0.4527  | 0.6270  |  0 hr 06 min \n",
      " 12   | 0.5445  | 0.4588  | 0.6687  |  0 hr 06 min \n",
      " 13   | 0.4682  | 0.3841  | 0.6075  |  0 hr 06 min \n",
      " 14   | 0.4818  | 0.3497  | 0.5734  |  0 hr 06 min \n",
      " 15   | 0.4277  | 0.3442  | 0.5900  |  0 hr 06 min \n",
      " 16   | 0.4626  | 0.3682  | 0.5537  |  0 hr 06 min \n",
      " 17   | 0.4042  | 0.3842  | 0.5824  |  0 hr 06 min \n",
      " 18   | 0.4367  | 0.3402  | 0.6218  |  0 hr 06 min \n",
      " 19   | 0.3969  | 0.2796  | 0.5495  |  0 hr 06 min \n",
      " 20   | 0.3474  | 0.2581  | 0.4912  |  0 hr 06 min \n",
      " 21   | 0.3603  | 0.2606  | 0.4737  |  0 hr 06 min \n",
      " 22   | 0.3085  | 0.2638  | 0.5310  |  0 hr 06 min \n",
      " 23   | 0.3549  | 0.2579  | 0.5416  |  0 hr 06 min \n",
      " 24   | 0.3026  | 0.2469  | 0.4952  |  0 hr 06 min \n",
      " 25   | 0.3179  | 0.2417  | 0.4668  |  0 hr 06 min \n",
      " 26   | 0.3141  | 0.2339  | 0.4525  |  0 hr 06 min \n",
      " 27   | 0.3534  | 0.2372  | 0.5177  |  0 hr 06 min \n",
      " 28   | 0.3303  | 0.2402  | 0.5775  |  0 hr 06 min \n",
      " 29   | 0.3542  | 0.2658  | 0.5386  |  0 hr 06 min \n",
      " 30   | 0.3931  | 0.2437  | 0.5438  |  0 hr 06 min \n",
      " 31   | 0.3698  | 0.2363  | 0.4632  |  0 hr 06 min \n",
      " 32   | 0.3540  | 0.2776  | 0.5023  |  0 hr 06 min \n",
      " 33   | 0.3335  | 0.2587  | 0.4920  |  0 hr 06 min \n",
      " 34   | 0.3086  | 0.2286  | 0.4820  |  0 hr 06 min \n",
      " 35   | 0.3090  | 0.2179  | 0.4343  |  0 hr 06 min \n",
      " 36   | 0.3249  | 0.2237  | 0.4696  |  0 hr 06 min \n",
      " 37   | 0.3131  | 0.2476  | 0.4922  |  0 hr 06 min \n",
      " 38   | 0.3452  | 0.2002  | 0.4504  |  0 hr 06 min \n",
      " 39   | 0.2840  | 0.2050  | 0.4518  |  0 hr 06 min \n",
      " 40   | 0.3049  | 0.3123  | 0.5350  |  0 hr 06 min \n",
      " 41   | 0.3384  | 0.2431  | 0.4501  |  0 hr 06 min \n",
      " 42   | 0.3304  | 0.2516  | 0.4955  |  0 hr 07 min \n",
      " 43   | 0.3687  | 0.2501  | 0.4559  |  0 hr 07 min \n",
      " 44   | 0.2772  | 0.2366  | 0.4875  |  0 hr 07 min \n",
      " 45   | 0.3086  | 0.2023  | 0.4505  |  0 hr 07 min \n",
      " 46   | 0.2721  | 0.1811  | 0.4487  |  0 hr 07 min \n",
      " 47   | 0.2818  | 0.1886  | 0.4178  |  0 hr 07 min \n",
      " 48   | 0.3041  | 0.1646  | 0.4110  |  0 hr 07 min \n",
      " 49   | 0.2659  | 0.1996  | 0.4515  |  0 hr 07 min \n",
      " 50   | 0.2460  | 0.1674  | 0.3793  |  0 hr 07 min \n",
      " 51   | 0.2379  | 0.1847  | 0.4191  |  0 hr 07 min \n",
      " 52   | 0.2478  | 0.1776  | 0.4180  |  0 hr 07 min \n",
      " 53   | 0.2796  | 0.1868  | 0.4413  |  0 hr 07 min \n",
      " 54   | 0.3078  | 0.1901  | 0.4671  |  0 hr 07 min \n",
      " 55   | 0.2931  | 0.1610  | 0.4133  |  0 hr 07 min \n",
      " 56   | 0.2713  | 0.1650  | 0.4145  |  0 hr 07 min \n",
      " 57   | 0.2482  | 0.1523  | 0.4296  |  0 hr 07 min \n",
      " 58   | 0.2320  | 0.1465  | 0.4526  |  0 hr 07 min \n",
      " 59   | 0.2361  | 0.1443  | 0.4046  |  0 hr 07 min \n",
      " 60   | 0.2221  | 0.1380  | 0.4077  |  0 hr 07 min \n",
      " 61   | 0.2364  | 0.1455  | 0.4393  |  0 hr 07 min \n",
      " 62   | 0.2230  | 0.1441  | 0.4238  |  0 hr 07 min \n",
      " 63   | 0.2078  | 0.1274  | 0.4210  |  0 hr 07 min \n",
      " 64   | 0.2548  | 0.1264  | 0.4195  |  0 hr 07 min \n",
      " 65   | 0.2283  | 0.1336  | 0.4103  |  0 hr 07 min \n",
      " 66   | 0.2252  | 0.1254  | 0.3968  |  0 hr 07 min \n",
      " 67   | 0.2234  | 0.1641  | 0.4742  |  0 hr 07 min \n",
      " 68   | 0.2138  | 0.1223  | 0.4542  |  0 hr 07 min \n",
      " 69   | 0.2032  | 0.1198  | 0.4125  |  0 hr 07 min \n",
      " 70   | 0.2021  | 0.1217  | 0.3822  |  0 hr 07 min \n",
      " 71   | 0.1861  | 0.1138  | 0.4112  |  0 hr 07 min \n",
      " 72   | 0.2210  | 0.1263  | 0.4158  |  0 hr 07 min \n",
      " 73   | 0.2767  | 0.1508  | 0.4421  |  0 hr 07 min \n",
      " 74   | 0.2488  | 0.1238  | 0.3882  |  0 hr 07 min \n",
      " 75   | 0.2217  | 0.1210  | 0.4131  |  0 hr 08 min \n",
      " 76   | 0.1979  | 0.1605  | 0.4532  |  0 hr 08 min \n",
      " 77   | 0.2003  | 0.1203  | 0.4079  |  0 hr 08 min \n",
      " 78   | 0.1981  | 0.1247  | 0.4182  |  0 hr 08 min \n",
      " 79   | 0.2219  | 0.1288  | 0.4621  |  0 hr 08 min \n",
      " 80   | 0.2753  | 0.1469  | 0.4201  |  0 hr 08 min \n",
      " 81   | 0.2457  | 0.1449  | 0.4037  |  0 hr 08 min \n",
      " 82   | 0.2027  | 0.1964  | 0.5767  |  0 hr 08 min \n",
      "fold | epoch | train_MSE | valid MSE \n",
      "  2   |  50   | 0.1138  | 0.3793  \n",
      "  0   | 9.9828  | 10.2567 | 7.4537  |  0 hr 08 min \n",
      "  1   | 3.0550  | 6.3051  | 4.6998  |  0 hr 08 min \n",
      "  2   | 2.1305  | 3.4998  | 3.2005  |  0 hr 08 min \n",
      "  3   | 1.8730  | 3.1222  | 2.9113  |  0 hr 08 min \n",
      "  4   | 1.3833  | 3.1501  | 2.9347  |  0 hr 08 min \n",
      "  5   | 1.0377  | 2.5141  | 2.4829  |  0 hr 08 min \n",
      "  6   | 0.8756  | 2.0213  | 2.0451  |  0 hr 08 min \n",
      "  7   | 0.7106  | 1.8634  | 1.8845  |  0 hr 08 min \n",
      "  8   | 0.6400  | 0.9379  | 1.0002  |  0 hr 08 min \n",
      "  9   | 0.6097  | 0.6640  | 0.7212  |  0 hr 08 min \n",
      " 10   | 0.5488  | 0.5205  | 0.5056  |  0 hr 08 min \n",
      " 11   | 0.5285  | 0.4016  | 0.4345  |  0 hr 08 min \n",
      " 12   | 0.5055  | 0.4378  | 0.4506  |  0 hr 08 min \n",
      " 13   | 0.5086  | 0.3578  | 0.3975  |  0 hr 08 min \n",
      " 14   | 0.4043  | 0.3530  | 0.3887  |  0 hr 08 min \n",
      " 15   | 0.4360  | 0.3111  | 0.3664  |  0 hr 08 min \n",
      " 16   | 0.4135  | 0.3018  | 0.3727  |  0 hr 08 min \n",
      " 17   | 0.3705  | 0.2773  | 0.3551  |  0 hr 08 min \n",
      " 18   | 0.3790  | 0.2620  | 0.3452  |  0 hr 08 min \n",
      " 19   | 0.3754  | 0.2647  | 0.3476  |  0 hr 08 min \n",
      " 20   | 0.3703  | 0.2487  | 0.3525  |  0 hr 08 min \n",
      " 21   | 0.3647  | 0.2546  | 0.3721  |  0 hr 08 min \n",
      " 22   | 0.3307  | 0.2561  | 0.3677  |  0 hr 08 min \n",
      " 23   | 0.3634  | 0.2562  | 0.3584  |  0 hr 08 min \n",
      " 24   | 0.3239  | 0.3075  | 0.3857  |  0 hr 09 min \n",
      " 25   | 0.3386  | 0.2488  | 0.3631  |  0 hr 09 min \n",
      " 26   | 0.3580  | 0.2638  | 0.3939  |  0 hr 09 min \n",
      " 27   | 0.4194  | 0.2487  | 0.3976  |  0 hr 09 min \n",
      " 28   | 0.3439  | 0.3011  | 0.4028  |  0 hr 09 min \n",
      " 29   | 0.3066  | 0.2525  | 0.4212  |  0 hr 09 min \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30   | 0.3047  | 0.2238  | 0.3728  |  0 hr 09 min \n",
      " 31   | 0.3258  | 0.1925  | 0.3338  |  0 hr 09 min \n",
      " 32   | 0.2718  | 0.1940  | 0.3250  |  0 hr 09 min \n",
      " 33   | 0.2675  | 0.1811  | 0.3199  |  0 hr 09 min \n",
      " 34   | 0.2913  | 0.1862  | 0.3653  |  0 hr 09 min \n",
      " 35   | 0.3955  | 0.1662  | 0.3461  |  0 hr 09 min \n",
      " 36   | 0.2820  | 0.1943  | 0.3470  |  0 hr 09 min \n",
      " 37   | 0.2728  | 0.1767  | 0.3434  |  0 hr 09 min \n",
      " 38   | 0.2828  | 0.1914  | 0.3670  |  0 hr 09 min \n",
      " 39   | 0.2983  | 0.1854  | 0.3535  |  0 hr 09 min \n",
      " 40   | 0.2928  | 0.1667  | 0.3252  |  0 hr 09 min \n",
      " 41   | 0.2482  | 0.1741  | 0.3034  |  0 hr 09 min \n",
      " 42   | 0.2549  | 0.1572  | 0.3245  |  0 hr 09 min \n",
      " 43   | 0.2574  | 0.1919  | 0.3436  |  0 hr 09 min \n",
      " 44   | 0.2666  | 0.1763  | 0.3644  |  0 hr 09 min \n",
      " 45   | 0.2795  | 0.1693  | 0.3423  |  0 hr 09 min \n",
      " 46   | 0.2524  | 0.1570  | 0.3168  |  0 hr 09 min \n",
      " 47   | 0.2499  | 0.1386  | 0.3107  |  0 hr 09 min \n",
      " 48   | 0.2267  | 0.1576  | 0.3370  |  0 hr 09 min \n",
      " 49   | 0.2263  | 0.1557  | 0.3237  |  0 hr 09 min \n",
      " 50   | 0.3729  | 0.1473  | 0.3230  |  0 hr 09 min \n",
      " 51   | 0.2696  | 0.1825  | 0.3116  |  0 hr 09 min \n",
      " 52   | 0.2978  | 0.1646  | 0.3158  |  0 hr 09 min \n",
      " 53   | 0.2656  | 0.1827  | 0.3430  |  0 hr 09 min \n",
      " 54   | 0.2763  | 0.1725  | 0.3239  |  0 hr 09 min \n",
      " 55   | 0.2445  | 0.1842  | 0.3403  |  0 hr 09 min \n",
      " 56   | 0.2745  | 0.1575  | 0.3161  |  0 hr 10 min \n",
      " 57   | 0.2543  | 0.1531  | 0.3451  |  0 hr 10 min \n",
      " 58   | 0.2809  | 0.1804  | 0.3268  |  0 hr 10 min \n",
      " 59   | 0.2813  | 0.1600  | 0.3016  |  0 hr 10 min \n",
      " 60   | 0.2348  | 0.1434  | 0.3086  |  0 hr 10 min \n",
      " 61   | 0.2656  | 0.1469  | 0.3139  |  0 hr 10 min \n",
      " 62   | 0.2971  | 0.1301  | 0.2882  |  0 hr 10 min \n",
      " 63   | 0.2340  | 0.1905  | 0.3234  |  0 hr 10 min \n",
      " 64   | 0.2766  | 0.1567  | 0.3327  |  0 hr 10 min \n",
      " 65   | 0.2202  | 0.1578  | 0.3311  |  0 hr 10 min \n",
      " 66   | 0.1901  | 0.1431  | 0.3256  |  0 hr 10 min \n",
      " 67   | 0.2641  | 0.1315  | 0.3046  |  0 hr 10 min \n",
      " 68   | 0.2183  | 0.1276  | 0.3095  |  0 hr 10 min \n",
      " 69   | 0.2146  | 0.1523  | 0.3270  |  0 hr 10 min \n",
      " 70   | 0.3028  | 0.1176  | 0.2741  |  0 hr 10 min \n",
      " 71   | 0.2800  | 0.1661  | 0.3486  |  0 hr 10 min \n",
      " 72   | 0.2696  | 0.1316  | 0.3477  |  0 hr 10 min \n",
      " 73   | 0.2890  | 0.2482  | 0.3562  |  0 hr 10 min \n",
      " 74   | 0.2027  | 0.1290  | 0.2972  |  0 hr 10 min \n",
      " 75   | 0.1984  | 0.1351  | 0.3217  |  0 hr 10 min \n",
      " 76   | 0.2138  | 0.1242  | 0.3222  |  0 hr 10 min \n",
      " 77   | 0.1977  | 0.1597  | 0.3425  |  0 hr 10 min \n",
      " 78   | 0.2184  | 0.1383  | 0.2941  |  0 hr 10 min \n",
      " 79   | 0.2130  | 0.1324  | 0.2998  |  0 hr 10 min \n",
      " 80   | 0.2247  | 0.1244  | 0.2975  |  0 hr 10 min \n",
      " 81   | 0.2146  | 0.1383  | 0.3139  |  0 hr 10 min \n",
      " 82   | 0.1984  | 0.1153  | 0.2871  |  0 hr 10 min \n",
      " 83   | 0.1850  | 0.1258  | 0.2871  |  0 hr 10 min \n",
      " 84   | 0.1803  | 0.1145  | 0.2980  |  0 hr 10 min \n",
      " 85   | 0.2044  | 0.1020  | 0.3046  |  0 hr 10 min \n",
      " 86   | 0.2028  | 0.1121  | 0.3166  |  0 hr 10 min \n",
      " 87   | 0.2030  | 0.1217  | 0.3287  |  0 hr 10 min \n",
      " 88   | 0.1934  | 0.1005  | 0.3037  |  0 hr 11 min \n",
      " 89   | 0.1898  | 0.0987  | 0.2914  |  0 hr 11 min \n",
      " 90   | 0.1703  | 0.0943  | 0.3064  |  0 hr 11 min \n",
      " 91   | 0.1769  | 0.1118  | 0.3098  |  0 hr 11 min \n",
      " 92   | 0.1572  | 0.0843  | 0.2913  |  0 hr 11 min \n",
      " 93   | 0.1750  | 0.1312  | 0.3483  |  0 hr 11 min \n",
      " 94   | 0.1613  | 0.0939  | 0.3031  |  0 hr 11 min \n",
      " 95   | 0.1767  | 0.0829  | 0.2833  |  0 hr 11 min \n",
      " 96   | 0.1705  | 0.0857  | 0.2906  |  0 hr 11 min \n",
      " 97   | 0.1533  | 0.0899  | 0.2668  |  0 hr 11 min \n",
      " 98   | 0.1900  | 0.0896  | 0.2988  |  0 hr 11 min \n",
      " 99   | 0.1827  | 0.1120  | 0.3063  |  0 hr 11 min \n",
      " 100  | 0.1816  | 0.0918  | 0.2854  |  0 hr 11 min \n",
      " 101  | 0.2671  | 0.1203  | 0.2986  |  0 hr 11 min \n",
      " 102  | 0.2344  | 0.1748  | 0.3638  |  0 hr 11 min \n",
      " 103  | 0.2143  | 0.1284  | 0.3141  |  0 hr 11 min \n",
      " 104  | 0.2548  | 0.1827  | 0.3643  |  0 hr 11 min \n",
      " 105  | 0.2841  | 0.2056  | 0.3670  |  0 hr 11 min \n",
      " 106  | 0.2381  | 0.2055  | 0.3822  |  0 hr 11 min \n",
      " 107  | 0.2333  | 0.1121  | 0.3010  |  0 hr 11 min \n",
      " 108  | 0.2203  | 0.1155  | 0.2911  |  0 hr 11 min \n",
      " 109  | 0.1578  | 0.1141  | 0.3009  |  0 hr 11 min \n",
      " 110  | 0.2481  | 0.1105  | 0.2928  |  0 hr 11 min \n",
      " 111  | 0.1897  | 0.1858  | 0.3465  |  0 hr 11 min \n",
      " 112  | 0.1909  | 0.1067  | 0.3201  |  0 hr 11 min \n",
      " 113  | 0.2319  | 0.1228  | 0.3274  |  0 hr 11 min \n",
      " 114  | 0.1627  | 0.1274  | 0.2929  |  0 hr 11 min \n",
      " 115  | 0.1947  | 0.0916  | 0.2961  |  0 hr 11 min \n",
      " 116  | 0.1781  | 0.1012  | 0.3144  |  0 hr 11 min \n",
      "fold | epoch | train_MSE | valid MSE \n",
      "  3   |  97   | 0.0829  | 0.2668  \n",
      "  0   | 8.9436  | 10.3710 | 13.4960 |  0 hr 11 min \n",
      "  1   | 2.5467  | 6.5862  | 8.9522  |  0 hr 11 min \n",
      "  2   | 2.0754  | 4.3854  | 5.9254  |  0 hr 12 min \n",
      "  3   | 2.1146  | 3.7422  | 4.9181  |  0 hr 12 min \n",
      "  4   | 1.4104  | 3.0802  | 3.8582  |  0 hr 12 min \n",
      "  5   | 1.1643  | 2.2267  | 2.7151  |  0 hr 12 min \n",
      "  6   | 1.0949  | 1.4610  | 1.5850  |  0 hr 12 min \n",
      "  7   | 0.9287  | 0.9838  | 0.9976  |  0 hr 12 min \n",
      "  8   | 0.7958  | 0.9228  | 0.8759  |  0 hr 12 min \n",
      "  9   | 0.7969  | 0.6718  | 0.7152  |  0 hr 12 min \n",
      " 10   | 0.6433  | 0.5044  | 0.6285  |  0 hr 12 min \n",
      " 11   | 0.6054  | 0.4446  | 0.6262  |  0 hr 12 min \n",
      " 12   | 0.5606  | 0.5171  | 0.6730  |  0 hr 12 min \n",
      " 13   | 0.5687  | 0.5221  | 0.6745  |  0 hr 12 min \n",
      " 14   | 0.5403  | 0.3936  | 0.6251  |  0 hr 12 min \n",
      " 15   | 0.4848  | 0.4098  | 0.6237  |  0 hr 12 min \n",
      " 16   | 0.4451  | 0.4215  | 0.5967  |  0 hr 12 min \n",
      " 17   | 0.4610  | 0.3367  | 0.6043  |  0 hr 12 min \n",
      " 18   | 0.4370  | 0.3432  | 0.5617  |  0 hr 12 min \n",
      " 19   | 0.3896  | 0.2977  | 0.5550  |  0 hr 12 min \n",
      " 20   | 0.5414  | 0.2921  | 0.5186  |  0 hr 12 min \n",
      " 21   | 0.4516  | 0.3911  | 0.5753  |  0 hr 12 min \n",
      " 22   | 0.4318  | 0.3076  | 0.5059  |  0 hr 12 min \n",
      " 23   | 0.3841  | 0.2685  | 0.4869  |  0 hr 12 min \n",
      " 24   | 0.3280  | 0.2485  | 0.4557  |  0 hr 12 min \n",
      " 25   | 0.3623  | 0.2330  | 0.4630  |  0 hr 12 min \n",
      " 26   | 0.3479  | 0.2839  | 0.4905  |  0 hr 12 min \n",
      " 27   | 0.3669  | 0.2364  | 0.4609  |  0 hr 12 min \n",
      " 28   | 0.3183  | 0.2405  | 0.4844  |  0 hr 12 min \n",
      " 29   | 0.3653  | 0.2269  | 0.4196  |  0 hr 12 min \n",
      " 30   | 0.3355  | 0.2181  | 0.4690  |  0 hr 12 min \n",
      " 31   | 0.3170  | 0.2173  | 0.4244  |  0 hr 12 min \n",
      " 32   | 0.3281  | 0.2171  | 0.4642  |  0 hr 12 min \n",
      " 33   | 0.3287  | 0.2228  | 0.3893  |  0 hr 12 min \n",
      " 34   | 0.3055  | 0.2023  | 0.3950  |  0 hr 13 min \n",
      " 35   | 0.3244  | 0.2437  | 0.5269  |  0 hr 13 min \n",
      " 36   | 0.2890  | 0.2250  | 0.4098  |  0 hr 13 min \n",
      " 37   | 0.2780  | 0.2091  | 0.4738  |  0 hr 13 min \n",
      " 38   | 0.2923  | 0.1770  | 0.4312  |  0 hr 13 min \n",
      " 39   | 0.2561  | 0.1607  | 0.4236  |  0 hr 13 min \n",
      " 40   | 0.2530  | 0.1742  | 0.4457  |  0 hr 13 min \n",
      " 41   | 0.2659  | 0.1814  | 0.3711  |  0 hr 13 min \n",
      " 42   | 0.3596  | 0.1567  | 0.4016  |  0 hr 13 min \n",
      " 43   | 0.3007  | 0.2675  | 0.4669  |  0 hr 13 min \n",
      " 44   | 0.2721  | 0.1801  | 0.4410  |  0 hr 13 min \n",
      " 45   | 0.2664  | 0.2257  | 0.4986  |  0 hr 13 min \n",
      " 46   | 0.2905  | 0.1965  | 0.4390  |  0 hr 13 min \n",
      " 47   | 0.3009  | 0.2203  | 0.4586  |  0 hr 13 min \n",
      " 48   | 0.3003  | 0.1932  | 0.5080  |  0 hr 13 min \n",
      " 49   | 0.2803  | 0.1841  | 0.4400  |  0 hr 13 min \n",
      " 50   | 0.2668  | 0.1532  | 0.4167  |  0 hr 13 min \n",
      " 51   | 0.2445  | 0.1700  | 0.4370  |  0 hr 13 min \n",
      " 52   | 0.2933  | 0.1560  | 0.4174  |  0 hr 13 min \n",
      " 53   | 0.2762  | 0.1589  | 0.4588  |  0 hr 13 min \n",
      " 54   | 0.2504  | 0.1915  | 0.4163  |  0 hr 13 min \n",
      " 55   | 0.2412  | 0.1705  | 0.4462  |  0 hr 13 min \n",
      " 56   | 0.2481  | 0.1594  | 0.3753  |  0 hr 13 min \n",
      " 57   | 0.2449  | 0.1322  | 0.4278  |  0 hr 13 min \n",
      " 58   | 0.2208  | 0.1467  | 0.4099  |  0 hr 13 min \n",
      " 59   | 0.2228  | 0.1325  | 0.4358  |  0 hr 13 min \n",
      " 60   | 0.2146  | 0.1457  | 0.4753  |  0 hr 13 min \n",
      " 61   | 0.2419  | 0.1364  | 0.4631  |  0 hr 13 min \n",
      " 62   | 0.2651  | 0.1407  | 0.4570  |  0 hr 13 min \n",
      " 63   | 0.2580  | 0.1643  | 0.4157  |  0 hr 13 min \n",
      " 64   | 0.2467  | 0.1438  | 0.4556  |  0 hr 13 min \n",
      " 65   | 0.2696  | 0.1408  | 0.4659  |  0 hr 13 min \n",
      " 66   | 0.2508  | 0.1264  | 0.4422  |  0 hr 13 min \n",
      " 67   | 0.2127  | 0.1292  | 0.4429  |  0 hr 14 min \n",
      " 68   | 0.1969  | 0.1218  | 0.4445  |  0 hr 14 min \n",
      " 69   | 0.2545  | 0.1402  | 0.4598  |  0 hr 14 min \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70   | 0.2311  | 0.1147  | 0.4205  |  0 hr 14 min \n",
      " 71   | 0.2346  | 0.1196  | 0.4235  |  0 hr 14 min \n",
      " 72   | 0.2518  | 0.1371  | 0.4152  |  0 hr 14 min \n",
      " 73   | 0.2288  | 0.1265  | 0.4235  |  0 hr 14 min \n",
      " 74   | 0.2961  | 0.2058  | 0.4659  |  0 hr 14 min \n",
      " 75   | 0.1925  | 0.1337  | 0.4254  |  0 hr 14 min \n",
      " 76   | 0.2063  | 0.1540  | 0.4241  |  0 hr 14 min \n",
      " 77   | 0.2312  | 0.1283  | 0.4139  |  0 hr 14 min \n",
      " 78   | 0.2124  | 0.1397  | 0.4587  |  0 hr 14 min \n",
      " 79   | 0.1916  | 0.1252  | 0.4194  |  0 hr 14 min \n",
      " 80   | 0.1927  | 0.1055  | 0.4057  |  0 hr 14 min \n",
      " 81   | 0.1756  | 0.1269  | 0.4136  |  0 hr 14 min \n",
      " 82   | 0.1808  | 0.1071  | 0.4659  |  0 hr 14 min \n",
      " 83   | 0.1793  | 0.1051  | 0.4147  |  0 hr 14 min \n",
      " 84   | 0.1870  | 0.1068  | 0.4131  |  0 hr 14 min \n",
      " 85   | 0.1932  | 0.0971  | 0.4198  |  0 hr 14 min \n",
      " 86   | 0.1791  | 0.1087  | 0.4025  |  0 hr 14 min \n",
      " 87   | 0.1629  | 0.1217  | 0.4834  |  0 hr 14 min \n",
      " 88   | 0.1836  | 0.1001  | 0.4313  |  0 hr 14 min \n",
      " 89   | 0.2048  | 0.1078  | 0.4177  |  0 hr 14 min \n",
      " 90   | 0.1642  | 0.0889  | 0.4137  |  0 hr 14 min \n",
      " 91   | 0.1580  | 0.0966  | 0.4267  |  0 hr 14 min \n",
      " 92   | 0.1556  | 0.1026  | 0.3995  |  0 hr 14 min \n",
      " 93   | 0.1846  | 0.0916  | 0.4104  |  0 hr 14 min \n",
      " 94   | 0.1836  | 0.0910  | 0.4072  |  0 hr 14 min \n",
      " 95   | 0.1699  | 0.0840  | 0.4109  |  0 hr 14 min \n",
      " 96   | 0.1656  | 0.0976  | 0.4141  |  0 hr 14 min \n",
      " 97   | 0.2047  | 0.0784  | 0.4246  |  0 hr 14 min \n",
      " 98   | 0.1959  | 0.0856  | 0.4294  |  0 hr 14 min \n",
      " 99   | 0.1673  | 0.0984  | 0.4170  |  0 hr 15 min \n",
      " 100  | 0.1896  | 0.0927  | 0.4214  |  0 hr 15 min \n",
      " 101  | 0.2432  | 0.1096  | 0.4666  |  0 hr 15 min \n",
      " 102  | 0.1546  | 0.1051  | 0.4040  |  0 hr 15 min \n",
      " 103  | 0.1800  | 0.1071  | 0.4163  |  0 hr 15 min \n",
      " 104  | 0.1684  | 0.0963  | 0.4385  |  0 hr 15 min \n",
      " 105  | 0.2017  | 0.0987  | 0.4015  |  0 hr 15 min \n",
      " 106  | 0.2198  | 0.1006  | 0.4144  |  0 hr 15 min \n",
      " 107  | 0.1766  | 0.1171  | 0.4268  |  0 hr 15 min \n",
      " 108  | 0.1867  | 0.1294  | 0.5008  |  0 hr 15 min \n",
      "fold | epoch | train_MSE | valid MSE \n",
      "  4   |  41   | 0.0784  | 0.3711  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "log = Logger()\n",
    "log.open(f'log/{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "start = timer()\n",
    "\n",
    "log2 = Logger()\n",
    "log2.open(f'{prefix_filename}_best_{start_time}.txt')\n",
    "f2 = '{:^5} | {:^5} | {:^7.4f} | {:^7.4f} \\n'\n",
    "\n",
    "for fold_index in range(5):\n",
    "    \n",
    "    model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "    \n",
    "    best_param ={}\n",
    "    best_param[\"train_epoch\"] = 0\n",
    "    best_param[\"valid_epoch\"] = 0\n",
    "    best_param[\"train_MSE\"] = 9e8\n",
    "    best_param[\"valid_MSE\"] = 9e8\n",
    "    for epoch in range(800):\n",
    "        losses = train(smiles_list[train_fold[fold_index]])\n",
    "        traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "        valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "        \n",
    "        timing = time_to_str((timer() - start), 'min')  \n",
    "        log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "        \n",
    "        if train_MSE < best_param[\"train_MSE\"]:\n",
    "            best_param[\"train_epoch\"] = epoch\n",
    "            best_param[\"train_MSE\"] = train_MSE\n",
    "        if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "            best_param[\"valid_epoch\"] = epoch\n",
    "            best_param[\"valid_MSE\"] = valid_MSE\n",
    "            if valid_MSE < 0.35:\n",
    "                 torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "        if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "            break\n",
    "\n",
    "    log2.write('fold | epoch | train_MSE | valid MSE \\n')\n",
    "    log2.write(f2.format(fold_index, best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model\n",
    "# best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"valid_epoch\"])+'.pt')     \n",
    "\n",
    "# best_model_dict = best_model.state_dict()\n",
    "# best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "# (best_model.align[0].weight == model.align[0].weight).all()\n",
    "# test_MAE, test_MSE = eval(model, test_df)\n",
    "# print(\"best epoch:\",best_param[\"test_epoch\"],\"\\n\",\"test MSE:\",test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range(20):\n",
    "#     losses = train(smiles_list[valid_fold[fold_index]])\n",
    "#     print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
