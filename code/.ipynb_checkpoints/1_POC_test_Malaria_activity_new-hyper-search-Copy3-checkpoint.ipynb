{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "import random\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#then import my own modules\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight\n",
    "from timeit import default_timer as timer\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, graph_dict, graph_dataset, null_collate, Graph, Logger, time_to_str\n",
    "from AttentiveFP.featurizing import graph_dict\n",
    "from AttentiveFP.AttentiveLayers_new2 import Fingerprint, graph_dataset, null_collate, Graph, Logger, time_to_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_aviable = torch.cuda.is_available()\n",
    "device = torch.device(0)\n",
    "\n",
    "SEED = 18\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  9999\n",
      "number of successfully processed smiles:  9999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'Malaria Bioactivity'\n",
    "tasks = ['Loge EC50']\n",
    "\n",
    "raw_filename = \"../data/malaria-processed.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename, names = [\"Loge EC50\", \"smiles\"])\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)].reset_index()\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 200\n",
    "\n",
    "p_dropout= 0.5\n",
    "fingerprint_dim = 64\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 3\n",
    "K = 3\n",
    "T = 2\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph dicts loaded from ../data/malaria-processed.pkl\n"
     ]
    }
   ],
   "source": [
    "smiles_list = smiles_tasks_df['smiles'].values\n",
    "label_list = smiles_tasks_df[tasks[0]].values\n",
    "graph_dict = graph_dict(smiles_list, label_list, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  200\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "train_fold = []\n",
    "valid_fold = []\n",
    "for k, (train_idx, valid_idx) in enumerate(kfold.split(smiles_list)):\n",
    "    train_fold.append(train_idx)\n",
    "    valid_fold.append(valid_idx)\n",
    "    \n",
    "while (len(train_fold[0]) % batch_size) / batch_size <0.8:\n",
    "    batch_size +=1\n",
    "print(\"batch size: \", batch_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217095\n",
      "preprocess.0.linear.weight torch.Size([64, 39])\n",
      "preprocess.0.linear.bias torch.Size([64])\n",
      "preprocess.0.bn.weight torch.Size([64])\n",
      "preprocess.0.bn.bias torch.Size([64])\n",
      "encoder.0.linear.weight torch.Size([64, 49])\n",
      "encoder.0.linear.bias torch.Size([64])\n",
      "encoder.0.bn.weight torch.Size([64])\n",
      "encoder.0.bn.bias torch.Size([64])\n",
      "align.weight torch.Size([1, 128])\n",
      "align.bias torch.Size([1])\n",
      "attend.linear.weight torch.Size([64, 64])\n",
      "attend.linear.bias torch.Size([64])\n",
      "attend.bn.weight torch.Size([64])\n",
      "attend.bn.bias torch.Size([64])\n",
      "gru.weight_ih torch.Size([192, 64])\n",
      "gru.weight_hh torch.Size([192, 64])\n",
      "gru.bias_ih torch.Size([192])\n",
      "gru.bias_hh torch.Size([192])\n",
      "propagate.0.align.weight torch.Size([1, 128])\n",
      "propagate.0.align.bias torch.Size([1])\n",
      "propagate.0.attend.linear.weight torch.Size([64, 64])\n",
      "propagate.0.attend.linear.bias torch.Size([64])\n",
      "propagate.0.attend.bn.weight torch.Size([64])\n",
      "propagate.0.attend.bn.bias torch.Size([64])\n",
      "propagate.0.gru.weight_ih torch.Size([192, 64])\n",
      "propagate.0.gru.weight_hh torch.Size([192, 64])\n",
      "propagate.0.gru.bias_ih torch.Size([192])\n",
      "propagate.0.gru.bias_hh torch.Size([192])\n",
      "propagate.1.align.weight torch.Size([1, 128])\n",
      "propagate.1.align.bias torch.Size([1])\n",
      "propagate.1.attend.linear.weight torch.Size([64, 64])\n",
      "propagate.1.attend.linear.bias torch.Size([64])\n",
      "propagate.1.attend.bn.weight torch.Size([64])\n",
      "propagate.1.attend.bn.bias torch.Size([64])\n",
      "propagate.1.gru.weight_ih torch.Size([192, 64])\n",
      "propagate.1.gru.weight_hh torch.Size([192, 64])\n",
      "propagate.1.gru.bias_ih torch.Size([192])\n",
      "propagate.1.gru.bias_hh torch.Size([192])\n",
      "propagate.2.align.weight torch.Size([1, 128])\n",
      "propagate.2.align.bias torch.Size([1])\n",
      "propagate.2.attend.linear.weight torch.Size([64, 64])\n",
      "propagate.2.attend.linear.bias torch.Size([64])\n",
      "propagate.2.attend.bn.weight torch.Size([64])\n",
      "propagate.2.attend.bn.bias torch.Size([64])\n",
      "propagate.2.gru.weight_ih torch.Size([192, 64])\n",
      "propagate.2.gru.weight_hh torch.Size([192, 64])\n",
      "propagate.2.gru.bias_ih torch.Size([192])\n",
      "propagate.2.gru.bias_hh torch.Size([192])\n",
      "superGather.0.align.weight torch.Size([1, 128])\n",
      "superGather.0.align.bias torch.Size([1])\n",
      "superGather.0.attend.linear.weight torch.Size([64, 64])\n",
      "superGather.0.attend.linear.bias torch.Size([64])\n",
      "superGather.0.attend.bn.weight torch.Size([64])\n",
      "superGather.0.attend.bn.bias torch.Size([64])\n",
      "superGather.0.gru.weight_ih torch.Size([192, 64])\n",
      "superGather.0.gru.weight_hh torch.Size([192, 64])\n",
      "superGather.0.gru.bias_ih torch.Size([192])\n",
      "superGather.0.gru.bias_hh torch.Size([192])\n",
      "superGather.1.align.weight torch.Size([1, 128])\n",
      "superGather.1.align.bias torch.Size([1])\n",
      "superGather.1.attend.linear.weight torch.Size([64, 64])\n",
      "superGather.1.attend.linear.bias torch.Size([64])\n",
      "superGather.1.attend.bn.weight torch.Size([64])\n",
      "superGather.1.attend.bn.bias torch.Size([64])\n",
      "superGather.1.gru.weight_ih torch.Size([192, 64])\n",
      "superGather.1.gru.weight_hh torch.Size([192, 64])\n",
      "superGather.1.gru.bias_ih torch.Size([192])\n",
      "superGather.1.gru.bias_hh torch.Size([192])\n",
      "predict.0.linear.weight torch.Size([512, 64])\n",
      "predict.0.linear.bias torch.Size([512])\n",
      "predict.0.bn.weight torch.Size([512])\n",
      "predict.0.bn.bias torch.Size([512])\n",
      "predict.3.weight torch.Size([1, 512])\n",
      "predict.3.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "# optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in  model.parameters()])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(smiles_list):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=True, worker_init_fn=np.random.seed(SEED))\n",
    "    losses = []\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(train_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        \n",
    "        loss = loss_function(mol_prediction, label.view(-1,1))     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "        \n",
    "def eval(smiles_list):\n",
    "    model.eval()\n",
    "    eval_MAE_list = []\n",
    "    eval_MSE_list = []\n",
    "    eval_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(eval_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        MAE = F.l1_loss(mol_prediction, label.view(-1,1), reduction='none')        \n",
    "        MSE = F.mse_loss(mol_prediction, label.view(-1,1), reduction='none')\n",
    "        \n",
    "        eval_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        eval_MSE_list.extend(MSE.data.squeeze().cpu().numpy())\n",
    "    return np.array(eval_MAE_list).mean(), np.array(eval_MSE_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = Logger()\n",
    "# log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "# f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "# log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "# start = timer()\n",
    "\n",
    "# best_param ={}\n",
    "# best_param[\"train_epoch\"] = 0\n",
    "# best_param[\"valid_epoch\"] = 0\n",
    "# best_param[\"train_MSE\"] = 9e8\n",
    "# best_param[\"valid_MSE\"] = 9e8\n",
    "\n",
    "# fold_index = 3\n",
    "# for epoch in range(800):\n",
    "#     losses = train(smiles_list[train_fold[fold_index]])\n",
    "#     traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "#     valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "\n",
    "#     timing = time_to_str((timer() - start), 'min')  \n",
    "#     log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "\n",
    "#     if train_MSE < best_param[\"train_MSE\"]:\n",
    "#         best_param[\"train_epoch\"] = epoch\n",
    "#         best_param[\"train_MSE\"] = train_MSE\n",
    "#     if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#         best_param[\"valid_epoch\"] = epoch\n",
    "#         best_param[\"valid_MSE\"] = valid_MSE\n",
    "# #         if valid_MSE < 0.35:\n",
    "# #              torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#     if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "#         break\n",
    "# print(best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# log = Logger()\n",
    "# log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "# f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "# log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "# start = timer()\n",
    "\n",
    "# log2 = Logger()\n",
    "# log2.open(f'{prefix_filename}_best_{start_time}.txt')\n",
    "# f2 = '{:^5} | {:^5} | {:^7.4f} | {:^7.4f} \\n'\n",
    "\n",
    "# for fold_index in range(5):\n",
    "    \n",
    "#     model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "#     model.to(device)\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "    \n",
    "#     best_param ={}\n",
    "#     best_param[\"train_epoch\"] = 0\n",
    "#     best_param[\"valid_epoch\"] = 0\n",
    "#     best_param[\"train_MSE\"] = 9e8\n",
    "#     best_param[\"valid_MSE\"] = 9e8\n",
    "#     for epoch in range(800):\n",
    "#         losses = train(smiles_list[train_fold[fold_index]])\n",
    "#         traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "#         valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "        \n",
    "#         timing = time_to_str((timer() - start), 'min')  \n",
    "#         log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "        \n",
    "#         if train_MSE < best_param[\"train_MSE\"]:\n",
    "#             best_param[\"train_epoch\"] = epoch\n",
    "#             best_param[\"train_MSE\"] = train_MSE\n",
    "#         if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#             best_param[\"valid_epoch\"] = epoch\n",
    "#             best_param[\"valid_MSE\"] = valid_MSE\n",
    "# #             if valid_MSE < 0.35:\n",
    "# #                  torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#         if (epoch - best_param[\"train_epoch\"] >18) and (epoch - best_param[\"valid_epoch\"] >28):        \n",
    "#             break\n",
    "\n",
    "#     log2.write('fold | epoch | train_MSE | valid MSE \\n')\n",
    "#     log2.write(f2.format(fold_index, best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = prefix_filename+'_'+start_time+'.log'\n",
    "with open(log_file,'a') as f:\n",
    "    f.write(','.join(['K', 'T','fingerprint_dim', 'p_dropout','weight_decay','learning_rate','best_epoch','best_MSE'])+'\\n')\n",
    "    \n",
    "def f(K, T, fingerprint_dim, weight_decay, learning_rate, p_dropout, direction = False):\n",
    "    fold_index=3\n",
    "    model = Fingerprint(output_units_num, int(round(fingerprint_dim)), K=int(round(K)), T=int(round(T)), p_dropout=p_dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "    \n",
    "    best_param ={}\n",
    "    best_param[\"train_epoch\"] = 0\n",
    "    best_param[\"valid_epoch\"] = 0\n",
    "    best_param[\"train_MSE\"] = 8\n",
    "    best_param[\"valid_MSE\"] = 8\n",
    "    for epoch in range(800):\n",
    "        losses = train(smiles_list[train_fold[fold_index]])\n",
    "        traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "        valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "        \n",
    "#         timing = time_to_str((timer() - start), 'min')  \n",
    "#         log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "        \n",
    "        if train_MSE < best_param[\"train_MSE\"]:\n",
    "            best_param[\"train_epoch\"] = epoch\n",
    "            best_param[\"train_MSE\"] = train_MSE\n",
    "        if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "            best_param[\"valid_epoch\"] = epoch\n",
    "            best_param[\"valid_MSE\"] = valid_MSE\n",
    "        if (epoch - best_param[\"train_epoch\"] >8) and (epoch - best_param[\"valid_epoch\"] >28):        \n",
    "            break\n",
    "    with open(log_file,'a') as f:\n",
    "        f.write(','.join([str(int(round(K))), str(int(round(T))), str(int(round(fingerprint_dim))),str(p_dropout), str(weight_decay), str(learning_rate)]))\n",
    "        f.write(','+str(best_param[\"valid_epoch\"])+','+str(best_param[\"valid_MSE\"])+'\\n')\n",
    "\n",
    " \n",
    "    # GPGO maximize performance by default, set performance to its negative value for minimization\n",
    "    if direction:\n",
    "        return best_param[\"valid_MSE\"]\n",
    "    else:\n",
    "        return -best_param[\"valid_MSE\"]\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
      "init   \t [  4.           4.         152.           3.73050198   2.73364633\n",
      "   0.45418766]. \t  -1.2041860818862915 \t -1.126474380493164\n",
      "init   \t [  4.           4.         152.           3.73050198   2.73364633\n",
      "   0.45418766]. \t  -1.126474380493164 \t -1.126474380493164\n",
      "1      \t [  2.           1.         177.           3.70756593   3.70742352\n",
      "   0.42481497]. \t  \u001b[92m-1.1159651279449463\u001b[0m \t -1.1159651279449463\n",
      "2      \t [2.00000000e+00 2.00000000e+00 1.07000000e+02 5.38924955e+00\n",
      " 4.20852388e+00 1.04035560e-02]. \t  \u001b[92m-1.1154321432113647\u001b[0m \t -1.1154321432113647\n",
      "3      \t [3.00000000e+00 1.00000000e+00 2.12000000e+02 4.13281533e+00\n",
      " 3.71758374e+00 6.16626874e-03]. \t  \u001b[92m-1.1118059158325195\u001b[0m \t -1.1118059158325195\n",
      "4      \t [ 4.          2.         45.          3.88080865  4.87910725  0.24984774]. \t  \u001b[92m-1.103712797164917\u001b[0m \t -1.103712797164917\n",
      "5      \t [3.00000000e+00 3.00000000e+00 2.47000000e+02 3.90377398e+00\n",
      " 2.40690115e+00 1.81121431e-01]. \t  \u001b[92m-1.1035503149032593\u001b[0m \t -1.1035503149032593\n"
     ]
    }
   ],
   "source": [
    "from pyGPGO.covfunc import matern32\n",
    "from pyGPGO.acquisition import Acquisition\n",
    "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
    "from pyGPGO.GPGO import GPGO\n",
    "cov = matern32()\n",
    "gp = GaussianProcess(cov)\n",
    "acq = Acquisition(mode='UCB')\n",
    "param = {\n",
    "         'K': ('int', [2, 6]),\n",
    "         'T': ('int', [1, 5]),\n",
    "         'fingerprint_dim': ('int', [32, 256]),\n",
    "         'weight_decay': ('cont', [2, 6]),\n",
    "         'learning_rate': ('cont', [2, 5]),\n",
    "         'p_dropout': ('cont', [0, 0.5])\n",
    "         }\n",
    "np.random.seed(SEED)\n",
    "gpgo = GPGO(gp, acq, f, param)\n",
    "gpgo.run(max_iter=30,init_evals=2)\n",
    "\n",
    "# hp_opt, valid_performance_opt = gpgo.getResult()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range(20):\n",
    "#     losses = train(smiles_list[valid_fold[fold_index]])\n",
    "#     print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
