{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "import random\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#then import my own modules\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight\n",
    "from timeit import default_timer as timer\n",
    "from AttentiveFP.featurizing import graph_dict\n",
    "from AttentiveFP.AttentiveLayers_new3 import Fingerprint, graph_dataset, null_collate, Graph, Logger, time_to_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_aviable = torch.cuda.is_available()\n",
    "device = torch.device(0)\n",
    "\n",
    "SEED = 28\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set_style(\"darkgrid\")\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "import itertools\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  133885\n",
      "number of successfully processed smiles:  133247\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'qm9'\n",
    "tasks = [\n",
    "   \"mu\",\"alpha\",\"homo\",\"lumo\",\"gap\",\"r2\",\"zpve\",\"u0\",\"u298\",\"h298\",\"g298\",\"cv\"\n",
    "]\n",
    "\n",
    "raw_filename = \"../data/gdb9_smiles.csv\"\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \", len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "#         print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)].reset_index()\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 300\n",
    "\n",
    "p_dropout= 0.5\n",
    "fingerprint_dim = 280\n",
    "\n",
    "weight_decay = 4.9 # also known as l2_regularization_lambda\n",
    "learning_rate = 3.4\n",
    "K = 2\n",
    "T = 2\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard deviation</th>\n",
       "      <th>Mean absolute deviation</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mu</td>\n",
       "      <td>2.708713</td>\n",
       "      <td>1.531956</td>\n",
       "      <td>1.191056</td>\n",
       "      <td>1.286217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alpha</td>\n",
       "      <td>75.190920</td>\n",
       "      <td>8.197607</td>\n",
       "      <td>6.308065</td>\n",
       "      <td>1.299544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homo</td>\n",
       "      <td>-0.239974</td>\n",
       "      <td>0.022176</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>1.357178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lumo</td>\n",
       "      <td>0.011026</td>\n",
       "      <td>0.046912</td>\n",
       "      <td>0.038618</td>\n",
       "      <td>1.214779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gap</td>\n",
       "      <td>0.250999</td>\n",
       "      <td>0.047505</td>\n",
       "      <td>0.039648</td>\n",
       "      <td>1.198148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>r2</td>\n",
       "      <td>1190.034973</td>\n",
       "      <td>279.648930</td>\n",
       "      <td>202.109506</td>\n",
       "      <td>1.383651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zpve</td>\n",
       "      <td>0.148522</td>\n",
       "      <td>0.033298</td>\n",
       "      <td>0.026472</td>\n",
       "      <td>1.257836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>u0</td>\n",
       "      <td>-411.587569</td>\n",
       "      <td>40.087880</td>\n",
       "      <td>31.106651</td>\n",
       "      <td>1.288724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>u298</td>\n",
       "      <td>-411.579095</td>\n",
       "      <td>40.087661</td>\n",
       "      <td>31.106518</td>\n",
       "      <td>1.288722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>h298</td>\n",
       "      <td>-411.578151</td>\n",
       "      <td>40.087661</td>\n",
       "      <td>31.106518</td>\n",
       "      <td>1.288722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>g298</td>\n",
       "      <td>-411.620986</td>\n",
       "      <td>40.088392</td>\n",
       "      <td>31.107005</td>\n",
       "      <td>1.288726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cv</td>\n",
       "      <td>31.604483</td>\n",
       "      <td>4.064031</td>\n",
       "      <td>3.204093</td>\n",
       "      <td>1.268387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Task         Mean  Standard deviation  Mean absolute deviation     ratio\n",
       "0      mu     2.708713            1.531956                 1.191056  1.286217\n",
       "1   alpha    75.190920            8.197607                 6.308065  1.299544\n",
       "2    homo    -0.239974            0.022176                 0.016340  1.357178\n",
       "3    lumo     0.011026            0.046912                 0.038618  1.214779\n",
       "4     gap     0.250999            0.047505                 0.039648  1.198148\n",
       "5      r2  1190.034973          279.648930               202.109506  1.383651\n",
       "6    zpve     0.148522            0.033298                 0.026472  1.257836\n",
       "7      u0  -411.587569           40.087880                31.106651  1.288724\n",
       "8    u298  -411.579095           40.087661                31.106518  1.288722\n",
       "9    h298  -411.578151           40.087661                31.106518  1.288722\n",
       "10   g298  -411.620986           40.088392                31.107005  1.288726\n",
       "11     cv    31.604483            4.064031                 3.204093  1.268387"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = smiles_tasks_df.sample(frac=1/10, random_state=SEED) # test set\n",
    "training_data = smiles_tasks_df.drop(test_df.index) # training data\n",
    "\n",
    "# get the stats of the seen dataset (the training data)\n",
    "# which will be used to noramlize the dataset. \n",
    "columns = ['Task','Mean','Standard deviation', 'Mean absolute deviation','ratio']\n",
    "mean_list=[]\n",
    "std_list=[]\n",
    "mad_list=[]\n",
    "ratio_list=[]\n",
    "training_label_list = []\n",
    "test_label_list = []\n",
    "for task in tasks:\n",
    "    mean = training_data[task].mean()\n",
    "    mean_list.append(mean)\n",
    "    std = training_data[task].std()\n",
    "    std_list.append(std)\n",
    "    mad = training_data[task].mad()\n",
    "    mad_list.append(mad)\n",
    "    ratio_list.append(std/mad)\n",
    "    training_data[task+'_normalized'] = (training_data[task]- mean)/std\n",
    "    # the mean and standard deviation of training data is also used in test set\n",
    "    test_df[task+'_normalized'] = (test_df[task]- mean)/std\n",
    "    training_label_list.append(training_data[task+'_normalized'].values)\n",
    "    test_label_list.append(test_df[task+'_normalized'].values)\n",
    "\n",
    "training_label = np.array(training_label_list).astype(np.float).transpose()\n",
    "test_label = np.array(test_label_list).astype(np.float).transpose()\n",
    "\n",
    "    \n",
    "# training data is further divided into validation set and train set\n",
    "valid_df = training_data.sample(frac=1/9, random_state=SEED) # validation set\n",
    "train_df = training_data.drop(valid_df.index) # train set\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "while (len(train_df) % batch_size) / batch_size <0.8:\n",
    "    batch_size +=1\n",
    "print(\"batch size: \", batch_size)\n",
    "\n",
    "list_of_tuples = list(zip(tasks, mean_list, std_list, mad_list, ratio_list))\n",
    "stats  = pd.DataFrame(list_of_tuples, columns = columns)\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the value distribution of different tasks\n",
    "# plt.figure(figsize=(28, 8),dpi=300)\n",
    "# gs = gridspec.GridSpec(2, 6)\n",
    "# sns.set(font_scale=1.8)\n",
    "# for i, task in enumerate(tasks):\n",
    "#     plt.subplot(gs[(i)])\n",
    "#     x = pd.Series(training_data[task].values, name=task)\n",
    "#     ax = sns.distplot(x, bins=28, kde=False)\n",
    "#     plt.tight_layout()\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the distribution after normalization (should be the same)\n",
    "\n",
    "# plt.figure(figsize=(28, 8),dpi=300)\n",
    "# gs = gridspec.GridSpec(2, 6)\n",
    "# sns.set(font_scale=1.8)\n",
    "# for i, task in enumerate(tasks):\n",
    "#     plt.subplot(gs[(i)])\n",
    "#     x = pd.Series(training_data[task+\"_normalized\"].values, name=task)\n",
    "#     ax = sns.distplot(x, bins=28, kde=False)\n",
    "#     plt.tight_layout()\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph dicts loaded from ../data/gdb9_smiles_training_seed28.pkl\n",
      "graph dicts loaded from ../data/gdb9_smiles_test_seed28.pkl\n"
     ]
    }
   ],
   "source": [
    "training_smiles = training_data['smiles'].values\n",
    "test_smiles = test_df['smiles'].values\n",
    "training_graph_dict = graph_dict(training_smiles, list(training_label), filename+'_training_seed'+str(SEED))\n",
    "test_graph_dict = graph_dict(test_smiles, list(test_label), filename+'_test_seed'+str(SEED))\n",
    "whole_graph_dict = {**training_graph_dict, **test_graph_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6544, 1.6888, 1.8419, 1.4757, 1.4356, 1.9145, 1.5822, 1.6608, 1.6608,\n",
       "         1.6608, 1.6608, 1.6088]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_array = torch.from_numpy(np.array(ratio_list).astype(np.float)).float()\n",
    "ratio_array = ratio_array.to(device)\n",
    "ratio_array_square = ratio_array[None,:]**2\n",
    "ratio_array_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4408736\n",
      "preprocess.0.linear.weight torch.Size([280, 39])\n",
      "preprocess.0.linear.bias torch.Size([280])\n",
      "preprocess.0.bn.weight torch.Size([280])\n",
      "preprocess.0.bn.bias torch.Size([280])\n",
      "propagate.0.encoder.0.linear.weight torch.Size([78400, 10])\n",
      "propagate.0.encoder.0.linear.bias torch.Size([78400])\n",
      "propagate.0.encoder.0.bn.weight torch.Size([78400])\n",
      "propagate.0.encoder.0.bn.bias torch.Size([78400])\n",
      "propagate.0.align.weight torch.Size([1, 560])\n",
      "propagate.0.align.bias torch.Size([1])\n",
      "propagate.0.attend.linear.weight torch.Size([280, 280])\n",
      "propagate.0.attend.linear.bias torch.Size([280])\n",
      "propagate.0.attend.bn.weight torch.Size([280])\n",
      "propagate.0.attend.bn.bias torch.Size([280])\n",
      "propagate.0.gru.weight_ih torch.Size([840, 280])\n",
      "propagate.0.gru.weight_hh torch.Size([840, 280])\n",
      "propagate.0.gru.bias_ih torch.Size([840])\n",
      "propagate.0.gru.bias_hh torch.Size([840])\n",
      "propagate.1.encoder.0.linear.weight torch.Size([78400, 10])\n",
      "propagate.1.encoder.0.linear.bias torch.Size([78400])\n",
      "propagate.1.encoder.0.bn.weight torch.Size([78400])\n",
      "propagate.1.encoder.0.bn.bias torch.Size([78400])\n",
      "propagate.1.align.weight torch.Size([1, 560])\n",
      "propagate.1.align.bias torch.Size([1])\n",
      "propagate.1.attend.linear.weight torch.Size([280, 280])\n",
      "propagate.1.attend.linear.bias torch.Size([280])\n",
      "propagate.1.attend.bn.weight torch.Size([280])\n",
      "propagate.1.attend.bn.bias torch.Size([280])\n",
      "propagate.1.gru.weight_ih torch.Size([840, 280])\n",
      "propagate.1.gru.weight_hh torch.Size([840, 280])\n",
      "propagate.1.gru.bias_ih torch.Size([840])\n",
      "propagate.1.gru.bias_hh torch.Size([840])\n",
      "superGather.0.align.weight torch.Size([1, 560])\n",
      "superGather.0.align.bias torch.Size([1])\n",
      "superGather.0.attend.linear.weight torch.Size([280, 280])\n",
      "superGather.0.attend.linear.bias torch.Size([280])\n",
      "superGather.0.attend.bn.weight torch.Size([280])\n",
      "superGather.0.attend.bn.bias torch.Size([280])\n",
      "superGather.0.gru.weight_ih torch.Size([840, 280])\n",
      "superGather.0.gru.weight_hh torch.Size([840, 280])\n",
      "superGather.0.gru.bias_ih torch.Size([840])\n",
      "superGather.0.gru.bias_hh torch.Size([840])\n",
      "superGather.1.align.weight torch.Size([1, 560])\n",
      "superGather.1.align.bias torch.Size([1])\n",
      "superGather.1.attend.linear.weight torch.Size([280, 280])\n",
      "superGather.1.attend.linear.bias torch.Size([280])\n",
      "superGather.1.attend.bn.weight torch.Size([280])\n",
      "superGather.1.attend.bn.bias torch.Size([280])\n",
      "superGather.1.gru.weight_ih torch.Size([840, 280])\n",
      "superGather.1.gru.weight_hh torch.Size([840, 280])\n",
      "superGather.1.gru.bias_ih torch.Size([840])\n",
      "superGather.1.gru.bias_hh torch.Size([840])\n",
      "predict.0.linear.weight torch.Size([512, 280])\n",
      "predict.0.linear.bias torch.Size([512])\n",
      "predict.0.bn.weight torch.Size([512])\n",
      "predict.0.bn.bias torch.Size([512])\n",
      "predict.3.weight torch.Size([12, 512])\n",
      "predict.3.bias torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "ratio_array = torch.from_numpy(np.array(ratio_list).astype(np.float)).float()\n",
    "ratio_array = ratio_array.to(device)\n",
    "std_array = torch.from_numpy(np.array(std_list).astype(np.float)).float()\n",
    "std_array = ratio_array.to(device)\n",
    "\n",
    "ratio_array_square = ratio_array[None,:]**2\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "# optimizer = optim.SGD(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in  model.parameters()])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, smiles_list):\n",
    "#     model.train()\n",
    "#     train_loader = DataLoader(graph_dataset(smiles_list, whole_graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "#                               num_workers=1, pin_memory=True, shuffle=True, worker_init_fn=np.random.seed(SEED))\n",
    "#     losses = []\n",
    "#     for b, (smiles, atom, bond, bond_index, mol_index, label) in tqdm(enumerate(train_loader)):\n",
    "#         atom = atom.to(device)\n",
    "#         bond = bond.to(device)\n",
    "#         bond_index = bond_index.to(device)\n",
    "#         mol_index = mol_index.to(device)\n",
    "#         label = label.to(device)\n",
    "        \n",
    "#         mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        \n",
    "#         loss = 0.0\n",
    "#         for i,task in enumerate(tasks):\n",
    "#             y_pred = mol_prediction[:, i]\n",
    "#             loss += loss_function(y_pred, label[:,i])*ratio_list[i]**2\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#     return np.mean(losses)\n",
    "\n",
    "        \n",
    "# def eval(model, smiles_list):\n",
    "#     model.eval()\n",
    "#     eval_MAE_list = {}\n",
    "#     eval_MSE_list = {}\n",
    "#     y_val_list = {}\n",
    "#     y_pred_list = {}\n",
    "#     for i,task in enumerate(tasks):\n",
    "#         y_pred_list[task] = np.array([])\n",
    "#         y_val_list[task] = np.array([])\n",
    "#         eval_MAE_list[task] = np.array([])\n",
    "#         eval_MSE_list[task] = np.array([])\n",
    "        \n",
    "#     eval_loader = DataLoader(graph_dataset(smiles_list, whole_graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "#                               num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "#     for b, (smiles, atom, bond, bond_index, mol_index, label) in tqdm(enumerate(eval_loader)):\n",
    "#         atom = atom.to(device)\n",
    "#         bond = bond.to(device)\n",
    "#         bond_index = bond_index.to(device)\n",
    "#         mol_index = mol_index.to(device)\n",
    "#         label = label.to(device)\n",
    "        \n",
    "#         mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "#         for i,task in enumerate(tasks):\n",
    "#             y_pred = mol_prediction[:, i]\n",
    "\n",
    "#             MAE = F.l1_loss(y_pred, label[:,i], reduction='none')        \n",
    "#             MSE = F.mse_loss(y_pred, label[:,i], reduction='none')\n",
    "        \n",
    "#             y_pred_list[task] = np.concatenate([y_pred_list[task], y_pred.cpu().detach().numpy()])\n",
    "#             y_val_list[task] = np.concatenate([y_val_list[task], label[:,i].cpu().detach().numpy()])\n",
    "#             eval_MAE_list[task] = np.concatenate([eval_MAE_list[task],MAE.data.squeeze().cpu().numpy()])\n",
    "#             eval_MSE_list[task] = np.concatenate([eval_MSE_list[task],MSE.data.squeeze().cpu().numpy()])\n",
    "# #     r2_score_list = [r2_score(y_val_list[task], y_pred_list[task]) for task in tasks]\n",
    "#     eval_MAE_normalized = np.array([eval_MAE_list[task].mean() for i, task in enumerate(tasks)])\n",
    "#     eval_MAE = np.multiply(eval_MAE_normalized, np.array(std_list))\n",
    "#     eval_RMSE_normalized = np.sqrt(np.array([eval_MSE_list[task].mean() for i, task in enumerate(tasks)]))\n",
    "#     eval_RMSE = np.multiply(eval_RMSE_normalized, np.array(std_list))\n",
    "\n",
    "#     return eval_MAE_normalized, eval_MAE, eval_RMSE_normalized, eval_RMSE #, r2_score_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, smiles_list):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(graph_dataset(smiles_list, whole_graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=True, worker_init_fn=np.random.seed(SEED))\n",
    "    losses = []\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in tqdm(enumerate(train_loader)):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device) * ratio_array_square # set STD/MAD ratio as the weights\n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index) * ratio_array_square\n",
    "        \n",
    "        loss = loss_function(mol_prediction.reshape(-1), label.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "def eval(model, smiles_list):\n",
    "    model.eval()\n",
    "    eval_MAE_list = {}\n",
    "    eval_MSE_list = {}\n",
    "    y_val_list = {}\n",
    "    y_pred_list = {}\n",
    "    for i,task in enumerate(tasks):\n",
    "        y_pred_list[task] = np.array([])\n",
    "        y_val_list[task] = np.array([])\n",
    "        eval_MAE_list[task] = np.array([])\n",
    "        eval_MSE_list[task] = np.array([])\n",
    "        \n",
    "    eval_loader = DataLoader(graph_dataset(smiles_list, whole_graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in tqdm(enumerate(eval_loader)):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        for i,task in enumerate(tasks):\n",
    "            y_pred = mol_prediction[:, i]\n",
    "\n",
    "            MAE = F.l1_loss(y_pred, label[:,i], reduction='none')        \n",
    "            MSE = F.mse_loss(y_pred, label[:,i], reduction='none')\n",
    "        \n",
    "            y_pred_list[task] = np.concatenate([y_pred_list[task], y_pred.cpu().detach().numpy()])\n",
    "            y_val_list[task] = np.concatenate([y_val_list[task], label[:,i].cpu().detach().numpy()])\n",
    "            eval_MAE_list[task] = np.concatenate([eval_MAE_list[task],MAE.data.squeeze().cpu().numpy()])\n",
    "            eval_MSE_list[task] = np.concatenate([eval_MSE_list[task],MSE.data.squeeze().cpu().numpy()])\n",
    "#     r2_score_list = [r2_score(y_val_list[task], y_pred_list[task]) for task in tasks]\n",
    "    eval_MAE_normalized = np.array([eval_MAE_list[task].mean() for i, task in enumerate(tasks)])\n",
    "    eval_MAE = np.multiply(eval_MAE_normalized, np.array(std_list))\n",
    "    eval_RMSE_normalized = np.sqrt(np.array([eval_MSE_list[task].mean() for i, task in enumerate(tasks)]))\n",
    "    eval_RMSE = np.multiply(eval_RMSE_normalized, np.array(std_list))\n",
    "\n",
    "    return eval_MAE_normalized, eval_MAE, eval_RMSE_normalized, eval_RMSE #, r2_score_list\n",
    "\n",
    "\n",
    "\n",
    "# def eval(model, smiles_list):\n",
    "#     model.eval()\n",
    "#     MAE_array = torch.Tensor()\n",
    "#     RMSE_array = torch.Tensor()\n",
    "#     mol_prediction_array = torch.Tensor()\n",
    "#     label_array = torch.Tensor()\n",
    "        \n",
    "#     eval_loader = DataLoader(graph_dataset(smiles_list, whole_graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "#                               num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "#     for b, (smiles, atom, bond, bond_index, mol_index, label) in tqdm(enumerate(eval_loader)):\n",
    "#         atom = atom.to(device)\n",
    "#         bond = bond.to(device)\n",
    "#         bond_index = bond_index.to(device)\n",
    "#         mol_index = mol_index.to(device)\n",
    "#         label = label.to(device)\n",
    "        \n",
    "#         mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "#         MAE = F.l1_loss(mol_prediction.reshape(-1), label.reshape(-1), reduction='none')        \n",
    "#         MSE = F.mse_loss(mol_prediction.reshape(-1), label.reshape(-1), reduction='none')\n",
    "        \n",
    "#         mol_prediction_array = torch.cat([mol_prediction_array.reshape(-1,12), mol_prediction.reshape(-1,12)],0)\n",
    "        \n",
    "#         label_array = torch.cat([label_array.reshape(-1,12),label.reshape(-1,12)])\n",
    "#         MAE_array = torch.cat([MAE.reshape(-1,12),MAE.reshape(-1,12)])\n",
    "#         MSE_array = torch.cat([MSE.reshape(-1,12),MSE.reshape(-1,12)])\n",
    "# #     mol_prediction_array = mol_prediction_array.data.squeeze().cpu().numpy()\n",
    "# #     label_array = label_array.data.squeeze().cpu().numpy()    \n",
    "# #     r2_score_list = [r2_score(mol_prediction_array[:,i], label_array[:,i]) for i, task in enumerate(tasks)]\n",
    "#     MAE_normalized = MAE_array.mean(0).reshape(-1)    \n",
    "#     MAE_raw = MAE_normalized * std_array\n",
    "#     RMSE_normalized = torch.sqrt(MAE_array.mean().reshape(-1))\n",
    "#     RMSE_raw = RMSE_normalized * std_array\n",
    "\n",
    "#     return MAE_normalized, MAE_raw, RMSE_normalized, RMSE_raw #, r2_score_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = Logger()\n",
    "# log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "# f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "# log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "# start = timer()\n",
    "\n",
    "# best_param ={}\n",
    "# best_param[\"train_epoch\"] = 0\n",
    "# best_param[\"valid_epoch\"] = 0\n",
    "# best_param[\"train_MSE\"] = 9e8\n",
    "# best_param[\"valid_MSE\"] = 9e8\n",
    "\n",
    "# fold_index = 3\n",
    "# for epoch in range(800):\n",
    "#     losses = train(smiles_list[train_fold[fold_index]])\n",
    "#     traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "#     valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "\n",
    "#     timing = time_to_str((timer() - start), 'min')  \n",
    "#     log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "\n",
    "#     if train_MSE < best_param[\"train_MSE\"]:\n",
    "#         best_param[\"train_epoch\"] = epoch\n",
    "#         best_param[\"train_MSE\"] = train_MSE\n",
    "#     if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#         best_param[\"valid_epoch\"] = epoch\n",
    "#         best_param[\"valid_MSE\"] = valid_MSE\n",
    "# #         if valid_MSE < 0.35:\n",
    "# #              torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#     if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "#         break\n",
    "# print(best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bug_list1 = ['C[NH2+]C1C[C@H](C(N)=O)O1', 'C[C@@H](O)[C@H]1CCC([NH-])O1', '[NH-]c1nc(N)c(O)[cH+]o1', 'O=CN1[C@H]2[C@H]3[C@@H]4[C@H]3[C@H]2[C@@H]41', 'C[NH2+]C1O[CH+]N=C[CH+]1C', 'C[C@H]1O[C@@H](CO)C1NO', 'CCO[C@H]1[C@@H](C)C1(C)C', 'COCc1c[nH]c(O)n1', 'O=CCCc1ccon1', 'C#CC1=C[C@@H]2[C@@H]3[C@H]1O[C@@H]32', 'CC1=C[C@H](C#N)COC1', 'C1CC12[C@H]1[C@@H]3[C@H]1[C@H]1[C@@H]3[N@H+]12', 'CCC[C@H]1C[C@@](C)(O)C1', 'C[C@H]1C[C@@]12C[C@@H](O)C2=O', 'O[C@H]1CC=C2CCO[C@H]21', 'CC(C)(O)CCCO', 'CC[C@]12C[N@H+]1C[C@@]21CN1', 'C#C[C@H]1C[C@H](COC)C1', '[NH-]c1nc(O)nc(N)[nH+]1', 'C[C@H]1C[C@]2(CN2)[C@@]12CN2', 'C[C@@]1(O)C[C@H]2OC[C@@H]1O2', 'C[C@@]12O[C@H]3[C@@H]4C[N@@H+]([C@@H]41)[C@H]32', 'C1=C[C@]2(CC1)[C@@]13C[C@]12O3', 'C[C@H]1[C@@H](O)C(=O)C1(C)C', 'C[N@H+]1[C@H]2[C@H]3C[C@H]3[C@@H]1[C@H]2O', 'O=C1C[C@H]2COC(=O)N12', 'CC[C@H]1CC2=CCO[C@H]21', 'O=C1CC[C@H]2C[C@@H](C1)O2', 'CCn1cnc(NC)c1', 'C#CC#C[C@H](O)CCC', 'Nc1[nH]c(O)cc1O', 'O=C1COCC=C[C@@H]1O', 'O=C1O[C@H]2CO[C@@H]1OC2', 'OCC[N@H+]1[C@H]2[C@H]3C[C@H]3[C@H]21', 'O=c1[nH]ncnc1F', '[NH-]c1[nH+]cc(N)[nH]c1=O', 'C[C@H]1O[C@]12C=CCC2', '[NH-]c1[nH+]c(O)cnc1O', 'O=C[C@H]1[C@@H]2C=C[C@H]1[C@H]2O', 'C[C@H]1CC12CC1(CC1)C2', 'N#CC1(N)CC1', 'C#C[C@@](C)(C=O)C(N)=O', 'C[C@H]1OC[C@H]2N[C@H]2[C@H]1O', 'Cc1c[nH+]c([NH-])nc1F', 'O=C1C[C@@H]2NC(=O)[C@@H]2C1', 'C[C@H]1C[C@H]([C@@]2(C)CO2)O1', 'c1cnc([C@H]2CN2)cn1', 'CC[C@@H]1[C@H](C)C=C[C@@H]1C', 'C[C@]12C[C@H]1[C@H]1C=CC[C@H]12', 'C[C@]12C[C@H]1C[C@H]2CCO']\n",
    "\n",
    "# bug_list2 = ['NC(=O)[C@H]1O[C@H]1C=O', 'CC(=O)c1n[nH]cc1O', 'Cc1c[nH]c(O)c1C=O', 'OC[C@@H](O)[C@H]1CCCO1', 'CCC1=CCCNC1=O', 'CNc1cnc(O)[nH]1', 'C[C@H]1C[N@H+]2[C@@H]3[C@H](O)[C@H]2[C@@H]31', 'C[N+]1=CNNC([NH-])C1=O', 'CNC(=N)CCC(=O)O', '[NH-]c1nn[nH+]c(N)c1F', 'C[NH2+]C1OCC12CC2', 'CC(C)(O)[C@H]1CCCO1', 'CC1(C)N[C@@]12COC2=O', '[NH-]c1n[cH+]occ1N', 'C#Cc1[nH]ccc1N', 'N#C[C@H]1CCC[C@H]2O[C@@H]12', 'COC[C@@H](C)[C@H]1C[C@H]1C', 'C[C@@H]1[C@@H]2C[C@H]1[C@@]2(C)CO', 'CC(C)C(=O)C[C@@H](C)O', 'Nc1oncc1NC=O', 'C[C@H]1C[C@]2(C)[C@H]1[C@@H]1N[C@@H]12', 'C[C@@H]1C=C[C@]2(CC1)CO2', 'NCC[C@@H](O)CC(=O)O', '[NH-]C1OC[C@@H]2[C@H]1[C@@H]2C=O', 'CC1(C)[C@H]2O[CH+][NH2+][C@]21C', 'CO[C@@H]1C([NH-])OC[C@@H]1O', 'C#C[C@H](C=O)CCCC', 'Cc1ccnc(C)c1', 'C[C@H]1C[N@H+](CC(N)=O)C1', 'O=CO[CH+][NH2+]C[C@H]1CO1', 'C#CC(=O)N[C@H](C)C=O', 'C#C[C@]1(CC#CC)CN1', 'C[C@@H](O)CC(=O)C1CC1', 'CC[N@@H+]1[C@H](C#N)[C@@H]2C[C@@H]21', 'C[C@]12C(=O)O[C@]13C[C@H]2C3', 'CCCCOC(=O)CN', 'CCC#C[C@@]1(C=O)CN1', 'c1nc2ocnc2[nH]1', 'N#C[C@@]1(O)C[C@@H]1N', 'O=C1CNC2(COC2)C1', 'CC[C@H]1C[C@@]2(C)O[C@@]12C', 'CC[C@@]12C[C@@H](O1)[C@H]1O[C@H]12', 'CCc1cc([NH-])nn[nH+]1', 'C#C[C@H](C)[C@@H](C)C#N', 'CCn1cnc(NC)n1', 'C1CC1[NH+]1CC1', 'CC(=O)[C@H]1COC(N)[NH2+]1', 'C#CC1(C#C)C[C@H]2C[C@H]21', 'O=C[C@@]1(O)[C@@H]2O[C@H]3[C@@H]2[C@H]31', 'C1[C@H]2C[C@]3(C[C@H]4O[C@H]43)[C@@H]12']\n",
    "\n",
    "# bug_list3 = ['c1cc2n[nH+]ccc2[nH]1', 'COCC[C@@H]1C=CCO1', 'C#C[C@@H]1C[C@@H]1CCO', 'C[C@@]12CC[C@@H](O)[C@]1(C)N2', 'C[C@@]1(O)CC(=O)[C@@]12CN2', 'Nc1cc(N2CC2)c[nH]1', '[NH-]C1CN=CN1C(N)=O', 'CNC[C@@]1(C#N)CN1', 'Cn1ccc(O)c1C#N', 'C[C@]12C([NH-])O[C@@H]3[C@]14O[C@@]324', 'O=C[C@@H]1[C@@H]2OCC[C@@]21O', 'CC1=C[C@H]2C[C@@]13OC[C@@H]23', 'Cc1n[nH+]c(O)nc1[NH-]', 'CCOC[C@H]1[C@H]2CC[C@H]21', 'CN/C=N/c1nc[nH]n1', 'CC(C)[C@H]1[C@@H](O)[C@]1(C)O', 'C[C@@]12[C@H]3[NH2+]C[C@@H]1[C@@H]2CO3', 'C[C@]12CC[C@H](CC1)O2', 'CCc1c(C)c[nH]c1C', 'C[C@@]12C[C@H]3[C@H]4[C@@H]1[C@@H]2C[N@@H+]34', 'C[C@H]1O[C@]23CO[C@H](C2)[C@@H]13', 'CCCOCCC1CC1', 'O=C1CN=CO[C@H]2C[C@@H]12', 'O=C1CCCOCCO1', 'C#C[C@]12C[C@@H]3C(=O)N1[C@@H]32', 'C[C@@]12C[C@H]3[N@@H+]1[C@@]3(C)[C@@H]2O', 'NC1=CC(=O)OC12CC2', 'C#CC[C@H](CC)[NH+]1CC1', 'C[C@H]1O[C@@]1(C=O)CC=O', 'O[C@H]1C[C@]12[C@H]1[C@@H]3C[C@H]1[C@@H]32', 'O[C@@H]1C=C[C@@H]2C[N@@H+]2CC1', 'C[C@@]12CC[C@H]3[C@H](O)[C@@H]1[C@H]32', 'CC(=O)OCC[C@@H](C)O', 'C#CC[C@]12CC[C@H]1CC2', 'C1C[C@@]23C[C@@H](O1)O[C@@H]2C3', 'c1cc2[nH]nnc2cn1', 'C[C@H]1CC2(CC2)O[C@H]1C', 'O=CN1[C@H]2[C@@H]3[C@H](O)[C@H]2[C@@H]31', 'CC(=O)CC12CC(C1)C2', 'C1=C[C@H]2[C@@H]3CC[C@H]2O[C@H]13', 'OCC[C@]12C[C@H]1[NH2+][CH+]O2', 'C[C@]12C[C@@H]3C[C@H](C1)[N@@H+]32', 'O=C1CN[C@@H]1CCCO', 'CC#C[C@]12C[N@H+](C1)[C@H]2C', 'CC[C@H]1O[C@H]2[C@@H]1C2(C)C', 'C[C@@H]1O[C@@H]2[C@H](O1)C2(C)C', 'C[C@H]1OC([NH-])[C@H]2C[C@H]21', 'N#C[C@@H]1C[C@@H]1NCC=O', 'C[C@@H](C=O)[C@@](C)(O)CO', 'CC[C@@H](C)n1nnnn1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = train(model, bug_list1)\n",
    "# print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_smiles = train_df.smiles.values\n",
    "# valid_df_smiles = valid_df.smiles.values\n",
    "# test_df_smiles = test_df.smiles.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# losses = train(model, train_df_smiles)\n",
    "# print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_MAE_normalized, valid_MAE, valid_RMSE_normalized, valid_RMSE, = eval(model, train_df_smiles)\n",
    "# print(valid_MAE_normalized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "533it [02:10,  4.19it/s]\n",
      "533it [00:53,  9.92it/s]\n",
      "67it [00:06, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:\t0 0.5010901165612419\n",
      "train_MAE:\n",
      "[7.57546136e-01 1.28789696e+00 7.84580145e-03 8.83616099e-03\n",
      " 1.10977971e-02 6.95174582e+01 5.18439707e-03 3.56884385e+00\n",
      " 3.27143282e+00 3.56546212e+00 3.40923654e+00 7.99121740e-01]\n",
      "valid_MAE:\n",
      "[7.49156837e-01 1.30172131e+00 7.83709289e-03 8.83324704e-03\n",
      " 1.10947514e-02 6.98520013e+01 5.20709269e-03 3.61870466e+00\n",
      " 3.30429455e+00 3.61154773e+00 3.45028175e+00 8.06897494e-01]\n",
      "train_MSE_normalized_mean: 0.2624980601415247\n",
      "valid_MSE_normalized_mean: 0.2614280556342246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "237it [00:57,  4.15it/s]"
     ]
    }
   ],
   "source": [
    "best_param ={}\n",
    "best_param[\"train_epoch\"] = 0\n",
    "best_param[\"valid_epoch\"] = 0\n",
    "best_param[\"train_MSE_normalized\"] = 9e8\n",
    "best_param[\"valid_MSE_normalized\"] = 9e8\n",
    "train_df_smiles = train_df.smiles.values\n",
    "valid_df_smiles = valid_df.smiles.values\n",
    "\n",
    "for epoch in range(300):\n",
    "    losses = train(model, train_df_smiles)\n",
    "    train_MAE_normalized, train_MAE, train_MSE_normalized, train_MSE = eval(model, train_df_smiles)\n",
    "    valid_MAE_normalized, valid_MAE, valid_MSE_normalized, valid_MSE, = eval(model, valid_df_smiles)\n",
    "    print(\"EPOCH:\\t\"+str(epoch)+' '+str(losses)+'\\n'\\\n",
    "#         +\"train_MAE_normalized: \"+str(train_MAE_normalized)+'\\n'\\\n",
    "#         +\"valid_MAE_normalized: \"+str(valid_MAE_normalized)+'\\n'\\\n",
    "        +\"train_MAE\"+\":\"+\"\\n\"+str(train_MAE)+'\\n'\\\n",
    "        +\"valid_MAE\"+\":\"+\"\\n\"+str(valid_MAE)+'\\n'\\\n",
    "          \n",
    "        +\"train_MSE_normalized_mean: \"+str(train_MSE_normalized.mean())+'\\n'\\\n",
    "        +\"valid_MSE_normalized_mean: \"+str(valid_MSE_normalized.mean())+'\\n'\\\n",
    "#         +\"train_MSE_normalized: \"+str(train_MSE_normalized)+'\\n'\\\n",
    "#         +\"valid_MSE_normalized: \"+str(valid_MSE_normalized)+'\\n'\\\n",
    "        )\n",
    "    if train_MSE_normalized.mean() < best_param[\"train_MSE_normalized\"]:\n",
    "        best_param[\"train_epoch\"] = epoch\n",
    "        best_param[\"train_MSE_normalized\"] = train_MSE_normalized.mean()\n",
    "    if valid_MSE_normalized.mean() < best_param[\"valid_MSE_normalized\"]:\n",
    "        best_param[\"valid_epoch\"] = epoch\n",
    "        best_param[\"valid_MSE_normalized\"] = valid_MSE_normalized.mean()\n",
    "        if valid_MSE_normalized.mean() < 0.06:\n",
    "             torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#     if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "#         break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mu, alpha, homo, lumo, gap, r2, zpve, u0, u298, h298, g298, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"valid_epoch\"])+'.pt')     \n",
    "\n",
    "test_MAE_normalized, test_MAE, test_MSE_normalized, test_MSE, = eval(best_model, test_df)\n",
    "print(\"best epoch:\"+str(best_param[\"valid_epoch\"])+'\\n'\\\n",
    "    +\"test_MAE\"+\":\"+\"\\n\"+str(test_MAE)+'\\n'\\\n",
    "    +\"test_MSE_normalized_mean: \"+str(test_MSE_normalized.mean())+'\\n'\\\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
