{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "import random\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#then import my own modules\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight\n",
    "from timeit import default_timer as timer\n",
    "from AttentiveFP import Fingerprint, Fingerprint_viz, graph_dict, graph_dataset, null_collate, Graph, Logger, time_to_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_aviable = torch.cuda.is_available()\n",
    "device = torch.device(0)\n",
    "\n",
    "SEED = 8\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  9999\n",
      "number of successfully processed smiles:  9999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'Malaria Bioactivity'\n",
    "tasks = ['Loge EC50']\n",
    "\n",
    "raw_filename = \"../data/malaria-processed.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename, names = [\"Loge EC50\", \"smiles\"])\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)].reset_index()\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 200\n",
    "\n",
    "p_dropout= 0.5\n",
    "fingerprint_dim = 64\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 3\n",
    "K = 3\n",
    "T = 2\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph dicts loaded from ../data/malaria-processed.pkl\n"
     ]
    }
   ],
   "source": [
    "smiles_list = smiles_tasks_df['smiles'].values\n",
    "label_list = smiles_tasks_df[tasks[0]].values\n",
    "graph_dict = graph_dict(smiles_list, label_list, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  200\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "train_fold = []\n",
    "valid_fold = []\n",
    "for k, (train_idx, valid_idx) in enumerate(kfold.split(smiles_list)):\n",
    "    train_fold.append(train_idx)\n",
    "    valid_fold.append(valid_idx)\n",
    "    \n",
    "while (len(train_fold[0]) % batch_size) / batch_size <0.8:\n",
    "    batch_size +=1\n",
    "print(\"batch size: \", batch_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344135\n",
      "sum_importance torch.Size([1])\n",
      "preprocess.0.linear.weight torch.Size([64, 39])\n",
      "preprocess.0.linear.bias torch.Size([64])\n",
      "preprocess.0.bn.weight torch.Size([64])\n",
      "preprocess.0.bn.bias torch.Size([64])\n",
      "propagate.0.encoder.0.linear.weight torch.Size([4096, 10])\n",
      "propagate.0.encoder.0.linear.bias torch.Size([4096])\n",
      "propagate.0.encoder.0.bn.weight torch.Size([4096])\n",
      "propagate.0.encoder.0.bn.bias torch.Size([4096])\n",
      "propagate.0.align.weight torch.Size([1, 128])\n",
      "propagate.0.align.bias torch.Size([1])\n",
      "propagate.0.attend.linear.weight torch.Size([64, 64])\n",
      "propagate.0.attend.linear.bias torch.Size([64])\n",
      "propagate.0.attend.bn.weight torch.Size([64])\n",
      "propagate.0.attend.bn.bias torch.Size([64])\n",
      "propagate.0.gru.weight_ih torch.Size([192, 64])\n",
      "propagate.0.gru.weight_hh torch.Size([192, 64])\n",
      "propagate.0.gru.bias_ih torch.Size([192])\n",
      "propagate.0.gru.bias_hh torch.Size([192])\n",
      "propagate.1.encoder.0.linear.weight torch.Size([4096, 10])\n",
      "propagate.1.encoder.0.linear.bias torch.Size([4096])\n",
      "propagate.1.encoder.0.bn.weight torch.Size([4096])\n",
      "propagate.1.encoder.0.bn.bias torch.Size([4096])\n",
      "propagate.1.align.weight torch.Size([1, 128])\n",
      "propagate.1.align.bias torch.Size([1])\n",
      "propagate.1.attend.linear.weight torch.Size([64, 64])\n",
      "propagate.1.attend.linear.bias torch.Size([64])\n",
      "propagate.1.attend.bn.weight torch.Size([64])\n",
      "propagate.1.attend.bn.bias torch.Size([64])\n",
      "propagate.1.gru.weight_ih torch.Size([192, 64])\n",
      "propagate.1.gru.weight_hh torch.Size([192, 64])\n",
      "propagate.1.gru.bias_ih torch.Size([192])\n",
      "propagate.1.gru.bias_hh torch.Size([192])\n",
      "propagate.2.encoder.0.linear.weight torch.Size([4096, 10])\n",
      "propagate.2.encoder.0.linear.bias torch.Size([4096])\n",
      "propagate.2.encoder.0.bn.weight torch.Size([4096])\n",
      "propagate.2.encoder.0.bn.bias torch.Size([4096])\n",
      "propagate.2.align.weight torch.Size([1, 128])\n",
      "propagate.2.align.bias torch.Size([1])\n",
      "propagate.2.attend.linear.weight torch.Size([64, 64])\n",
      "propagate.2.attend.linear.bias torch.Size([64])\n",
      "propagate.2.attend.bn.weight torch.Size([64])\n",
      "propagate.2.attend.bn.bias torch.Size([64])\n",
      "propagate.2.gru.weight_ih torch.Size([192, 64])\n",
      "propagate.2.gru.weight_hh torch.Size([192, 64])\n",
      "propagate.2.gru.bias_ih torch.Size([192])\n",
      "propagate.2.gru.bias_hh torch.Size([192])\n",
      "superGather.0.align.weight torch.Size([1, 128])\n",
      "superGather.0.align.bias torch.Size([1])\n",
      "superGather.0.attend.linear.weight torch.Size([64, 64])\n",
      "superGather.0.attend.linear.bias torch.Size([64])\n",
      "superGather.0.attend.bn.weight torch.Size([64])\n",
      "superGather.0.attend.bn.bias torch.Size([64])\n",
      "superGather.0.gru.weight_ih torch.Size([192, 64])\n",
      "superGather.0.gru.weight_hh torch.Size([192, 64])\n",
      "superGather.0.gru.bias_ih torch.Size([192])\n",
      "superGather.0.gru.bias_hh torch.Size([192])\n",
      "superGather.1.align.weight torch.Size([1, 128])\n",
      "superGather.1.align.bias torch.Size([1])\n",
      "superGather.1.attend.linear.weight torch.Size([64, 64])\n",
      "superGather.1.attend.linear.bias torch.Size([64])\n",
      "superGather.1.attend.bn.weight torch.Size([64])\n",
      "superGather.1.attend.bn.bias torch.Size([64])\n",
      "superGather.1.gru.weight_ih torch.Size([192, 64])\n",
      "superGather.1.gru.weight_hh torch.Size([192, 64])\n",
      "superGather.1.gru.bias_ih torch.Size([192])\n",
      "superGather.1.gru.bias_hh torch.Size([192])\n",
      "predict.0.linear.weight torch.Size([512, 64])\n",
      "predict.0.linear.bias torch.Size([512])\n",
      "predict.0.bn.weight torch.Size([512])\n",
      "predict.0.bn.bias torch.Size([512])\n",
      "predict.3.weight torch.Size([1, 512])\n",
      "predict.3.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "# optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in  model.parameters()])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(smiles_list):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=True, worker_init_fn=np.random.seed(SEED))\n",
    "    losses = []\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(train_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        \n",
    "        loss = loss_function(mol_prediction, label.view(-1,1))     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "        \n",
    "def eval(smiles_list):\n",
    "    model.eval()\n",
    "    eval_MAE_list = []\n",
    "    eval_MSE_list = []\n",
    "    eval_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(eval_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        MAE = F.l1_loss(mol_prediction, label.view(-1,1), reduction='none')        \n",
    "        MSE = F.mse_loss(mol_prediction, label.view(-1,1), reduction='none')\n",
    "        \n",
    "        eval_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        eval_MSE_list.extend(MSE.data.squeeze().cpu().numpy())\n",
    "    return np.array(eval_MAE_list).mean(), np.array(eval_MSE_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = Logger()\n",
    "# log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "# f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "# log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "# start = timer()\n",
    "\n",
    "# best_param ={}\n",
    "# best_param[\"train_epoch\"] = 0\n",
    "# best_param[\"valid_epoch\"] = 0\n",
    "# best_param[\"train_MSE\"] = 9e8\n",
    "# best_param[\"valid_MSE\"] = 9e8\n",
    "\n",
    "# fold_index = 0\n",
    "# for epoch in range(800):\n",
    "#     losses = train(smiles_list[train_fold[fold_index]])\n",
    "#     traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "#     valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "\n",
    "#     timing = time_to_str((timer() - start), 'min')  \n",
    "#     log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "\n",
    "#     if train_MSE < best_param[\"train_MSE\"]:\n",
    "#         best_param[\"train_epoch\"] = epoch\n",
    "#         best_param[\"train_MSE\"] = train_MSE\n",
    "#     if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#         best_param[\"valid_epoch\"] = epoch\n",
    "#         best_param[\"valid_MSE\"] = valid_MSE\n",
    "# #         if valid_MSE < 0.35:\n",
    "# #              torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#     if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "#         break\n",
    "# print(best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | loss | train MSE |  valid MSE |  time \n",
      "  0   | 1.4395  | 1.5994  | 1.6467  |  0 hr 00 min \n",
      "  1   | 1.3102  | 1.2415  | 1.3316  |  0 hr 00 min \n",
      "  2   | 1.2816  | 1.2513  | 1.3694  |  0 hr 00 min \n",
      "  3   | 1.2369  | 1.1982  | 1.3158  |  0 hr 00 min \n",
      "  4   | 1.2333  | 1.1555  | 1.2997  |  0 hr 00 min \n",
      "  5   | 1.2053  | 1.1510  | 1.2832  |  0 hr 00 min \n",
      "  6   | 1.1990  | 1.1354  | 1.3102  |  0 hr 01 min \n",
      "  7   | 1.1679  | 1.1067  | 1.2512  |  0 hr 01 min \n",
      "  8   | 1.1526  | 1.1979  | 1.3807  |  0 hr 01 min \n",
      "  9   | 1.1461  | 1.1103  | 1.2466  |  0 hr 01 min \n",
      " 10   | 1.1376  | 1.0683  | 1.2227  |  0 hr 01 min \n",
      " 11   | 1.1265  | 1.0881  | 1.2812  |  0 hr 01 min \n",
      " 12   | 1.1140  | 1.0536  | 1.2519  |  0 hr 01 min \n",
      " 13   | 1.1076  | 1.0534  | 1.2102  |  0 hr 02 min \n",
      " 14   | 1.0819  | 1.0177  | 1.1979  |  0 hr 02 min \n",
      " 15   | 1.0668  | 1.0094  | 1.2080  |  0 hr 02 min \n",
      " 16   | 1.0534  | 0.9964  | 1.2184  |  0 hr 02 min \n",
      " 17   | 1.0588  | 0.9911  | 1.2048  |  0 hr 02 min \n",
      " 18   | 1.0520  | 0.9878  | 1.1752  |  0 hr 02 min \n",
      " 19   | 1.0392  | 0.9626  | 1.1805  |  0 hr 02 min \n",
      " 20   | 1.0290  | 0.9743  | 1.2187  |  0 hr 03 min \n",
      " 21   | 1.0254  | 0.9395  | 1.1411  |  0 hr 03 min \n",
      " 22   | 1.0121  | 0.9614  | 1.2022  |  0 hr 03 min \n",
      " 23   | 1.0096  | 0.9201  | 1.1733  |  0 hr 03 min \n",
      " 24   | 0.9824  | 0.9121  | 1.1589  |  0 hr 03 min \n",
      " 25   | 0.9734  | 0.8993  | 1.1350  |  0 hr 03 min \n",
      " 26   | 0.9650  | 0.8871  | 1.1509  |  0 hr 03 min \n",
      " 27   | 0.9490  | 0.8860  | 1.1980  |  0 hr 04 min \n",
      " 28   | 0.9398  | 0.8495  | 1.1618  |  0 hr 04 min \n",
      " 29   | 0.9245  | 0.8785  | 1.1407  |  0 hr 04 min \n",
      " 30   | 0.9208  | 0.8212  | 1.1902  |  0 hr 04 min \n",
      " 31   | 0.9071  | 0.8523  | 1.1913  |  0 hr 04 min \n",
      " 32   | 0.8933  | 0.8083  | 1.1954  |  0 hr 04 min \n",
      " 33   | 0.8795  | 0.8181  | 1.2004  |  0 hr 04 min \n",
      " 34   | 0.8722  | 0.7698  | 1.2200  |  0 hr 05 min \n",
      " 35   | 0.8567  | 0.7571  | 1.1365  |  0 hr 05 min \n",
      " 36   | 0.8367  | 0.7906  | 1.2600  |  0 hr 05 min \n",
      " 37   | 0.8253  | 0.7603  | 1.2034  |  0 hr 05 min \n",
      " 38   | 0.8305  | 0.7471  | 1.1961  |  0 hr 05 min \n",
      " 39   | 0.8142  | 0.7083  | 1.2266  |  0 hr 05 min \n",
      " 40   | 0.8084  | 0.6938  | 1.2509  |  0 hr 05 min \n",
      " 41   | 0.7919  | 0.6686  | 1.2209  |  0 hr 06 min \n",
      " 42   | 0.7731  | 0.6732  | 1.1978  |  0 hr 06 min \n",
      " 43   | 0.7755  | 0.6538  | 1.2235  |  0 hr 06 min \n",
      " 44   | 0.7353  | 0.6399  | 1.2514  |  0 hr 06 min \n",
      " 45   | 0.7383  | 0.6233  | 1.2260  |  0 hr 06 min \n",
      " 46   | 0.7265  | 0.6034  | 1.2689  |  0 hr 06 min \n",
      " 47   | 0.7022  | 0.5911  | 1.2542  |  0 hr 06 min \n",
      " 48   | 0.7037  | 0.5886  | 1.2022  |  0 hr 07 min \n",
      " 49   | 0.6928  | 0.5828  | 1.2752  |  0 hr 07 min \n",
      " 50   | 0.6626  | 0.5772  | 1.3242  |  0 hr 07 min \n",
      " 51   | 0.6599  | 0.5339  | 1.3057  |  0 hr 07 min \n",
      " 52   | 0.6460  | 0.5276  | 1.3072  |  0 hr 07 min \n",
      " 53   | 0.6372  | 0.5349  | 1.3079  |  0 hr 07 min \n",
      " 54   | 0.6303  | 0.5108  | 1.3428  |  0 hr 07 min \n"
     ]
    }
   ],
   "source": [
    "\n",
    "log = Logger()\n",
    "log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "start = timer()\n",
    "\n",
    "log2 = Logger()\n",
    "log2.open(f'{prefix_filename}_best_{start_time}.txt')\n",
    "f2 = '{:^5} | {:^5} | {:^7.4f} | {:^7.4f} \\n'\n",
    "\n",
    "for fold_index in range(5):\n",
    "    \n",
    "    model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "    \n",
    "    best_param ={}\n",
    "    best_param[\"train_epoch\"] = 0\n",
    "    best_param[\"valid_epoch\"] = 0\n",
    "    best_param[\"train_MSE\"] = 9e8\n",
    "    best_param[\"valid_MSE\"] = 9e8\n",
    "    for epoch in range(800):\n",
    "        losses = train(smiles_list[train_fold[fold_index]])\n",
    "        traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "        valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "        \n",
    "        timing = time_to_str((timer() - start), 'min')  \n",
    "        log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "        \n",
    "        if train_MSE < best_param[\"train_MSE\"]:\n",
    "            best_param[\"train_epoch\"] = epoch\n",
    "            best_param[\"train_MSE\"] = train_MSE\n",
    "        if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "            best_param[\"valid_epoch\"] = epoch\n",
    "            best_param[\"valid_MSE\"] = valid_MSE\n",
    "#             if valid_MSE < 0.35:\n",
    "#                  torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "        if (epoch - best_param[\"train_epoch\"] >18) and (epoch - best_param[\"valid_epoch\"] >28):        \n",
    "            break\n",
    "\n",
    "    log2.write('fold | epoch | train_MSE | valid MSE \\n')\n",
    "    log2.write(f2.format(fold_index, best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model\n",
    "# best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"valid_epoch\"])+'.pt')     \n",
    "\n",
    "# best_model_dict = best_model.state_dict()\n",
    "# best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "# (best_model.align[0].weight == model.align[0].weight).all()\n",
    "# test_MAE, test_MSE = eval(model, test_df)\n",
    "# print(\"best epoch:\",best_param[\"test_epoch\"],\"\\n\",\"test MSE:\",test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range(20):\n",
    "#     losses = train(smiles_list[valid_fold[fold_index]])\n",
    "#     print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
