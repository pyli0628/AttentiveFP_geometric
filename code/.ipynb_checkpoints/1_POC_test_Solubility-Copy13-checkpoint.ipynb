{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "import random\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#then import my own modules\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight\n",
    "from timeit import default_timer as timer\n",
    "from AttentiveFP.featurizing import graph_dict\n",
    "from AttentiveFP.AttentiveLayers_new3 import Fingerprint, graph_dataset, null_collate, Graph, Logger, time_to_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_aviable = torch.cuda.is_available()\n",
    "device = torch.device(0)\n",
    "\n",
    "SEED = 3\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1128\n",
      "number of successfully processed smiles:  1128\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'solubility'\n",
    "tasks = ['measured log solubility in mols per litre']\n",
    "\n",
    "raw_filename = \"../data/delaney-processed.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \", len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)].reset_index()\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "epochs = 80\n",
    "batch_size =100\n",
    "\n",
    "p_dropout= 0.2\n",
    "fingerprint_dim = 200\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 2.5\n",
    "K = 2\n",
    "T = 2\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph dicts loaded from ../data/delaney-processed.pkl\n"
     ]
    }
   ],
   "source": [
    "smiles_list = smiles_tasks_df['smiles'].values\n",
    "label_list = smiles_tasks_df[tasks[0]].values\n",
    "graph_dict = graph_dict(smiles_list, label_list, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  101\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "train_fold = []\n",
    "valid_fold = []\n",
    "for k, (train_idx, valid_idx) in enumerate(kfold.split(smiles_list)):\n",
    "    train_fold.append(train_idx)\n",
    "    valid_fold.append(valid_idx)\n",
    "    \n",
    "# avoiding the last batch has too few samples by slightly tune the batch_size\n",
    "while (len(train_fold[0]) % batch_size) / batch_size <0.8:\n",
    "    batch_size +=1\n",
    "print(\"batch size: \", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2281653\n",
      "preprocess.0.linear.weight torch.Size([200, 39])\n",
      "preprocess.0.linear.bias torch.Size([200])\n",
      "preprocess.0.bn.weight torch.Size([200])\n",
      "preprocess.0.bn.bias torch.Size([200])\n",
      "propagate.0.encoder.0.linear.weight torch.Size([40000, 10])\n",
      "propagate.0.encoder.0.linear.bias torch.Size([40000])\n",
      "propagate.0.encoder.0.bn.weight torch.Size([40000])\n",
      "propagate.0.encoder.0.bn.bias torch.Size([40000])\n",
      "propagate.0.align.weight torch.Size([1, 400])\n",
      "propagate.0.align.bias torch.Size([1])\n",
      "propagate.0.attend.linear.weight torch.Size([200, 200])\n",
      "propagate.0.attend.linear.bias torch.Size([200])\n",
      "propagate.0.attend.bn.weight torch.Size([200])\n",
      "propagate.0.attend.bn.bias torch.Size([200])\n",
      "propagate.0.gru.weight_ih torch.Size([600, 200])\n",
      "propagate.0.gru.weight_hh torch.Size([600, 200])\n",
      "propagate.0.gru.bias_ih torch.Size([600])\n",
      "propagate.0.gru.bias_hh torch.Size([600])\n",
      "propagate.1.encoder.0.linear.weight torch.Size([40000, 10])\n",
      "propagate.1.encoder.0.linear.bias torch.Size([40000])\n",
      "propagate.1.encoder.0.bn.weight torch.Size([40000])\n",
      "propagate.1.encoder.0.bn.bias torch.Size([40000])\n",
      "propagate.1.align.weight torch.Size([1, 400])\n",
      "propagate.1.align.bias torch.Size([1])\n",
      "propagate.1.attend.linear.weight torch.Size([200, 200])\n",
      "propagate.1.attend.linear.bias torch.Size([200])\n",
      "propagate.1.attend.bn.weight torch.Size([200])\n",
      "propagate.1.attend.bn.bias torch.Size([200])\n",
      "propagate.1.gru.weight_ih torch.Size([600, 200])\n",
      "propagate.1.gru.weight_hh torch.Size([600, 200])\n",
      "propagate.1.gru.bias_ih torch.Size([600])\n",
      "propagate.1.gru.bias_hh torch.Size([600])\n",
      "superGather.0.align.weight torch.Size([1, 400])\n",
      "superGather.0.align.bias torch.Size([1])\n",
      "superGather.0.attend.linear.weight torch.Size([200, 200])\n",
      "superGather.0.attend.linear.bias torch.Size([200])\n",
      "superGather.0.attend.bn.weight torch.Size([200])\n",
      "superGather.0.attend.bn.bias torch.Size([200])\n",
      "superGather.0.gru.weight_ih torch.Size([600, 200])\n",
      "superGather.0.gru.weight_hh torch.Size([600, 200])\n",
      "superGather.0.gru.bias_ih torch.Size([600])\n",
      "superGather.0.gru.bias_hh torch.Size([600])\n",
      "superGather.1.align.weight torch.Size([1, 400])\n",
      "superGather.1.align.bias torch.Size([1])\n",
      "superGather.1.attend.linear.weight torch.Size([200, 200])\n",
      "superGather.1.attend.linear.bias torch.Size([200])\n",
      "superGather.1.attend.bn.weight torch.Size([200])\n",
      "superGather.1.attend.bn.bias torch.Size([200])\n",
      "superGather.1.gru.weight_ih torch.Size([600, 200])\n",
      "superGather.1.gru.weight_hh torch.Size([600, 200])\n",
      "superGather.1.gru.bias_ih torch.Size([600])\n",
      "superGather.1.gru.bias_hh torch.Size([600])\n",
      "predict.0.linear.weight torch.Size([512, 200])\n",
      "predict.0.linear.bias torch.Size([512])\n",
      "predict.0.bn.weight torch.Size([512])\n",
      "predict.0.bn.bias torch.Size([512])\n",
      "predict.3.weight torch.Size([1, 512])\n",
      "predict.3.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "# optimizer = optim.SGD(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in  model.parameters()])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(smiles_list):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=True, worker_init_fn=np.random.seed(SEED))\n",
    "    losses = []\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(train_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        \n",
    "        loss = loss_function(mol_prediction, label.view(-1,1))     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "        \n",
    "def eval(smiles_list):\n",
    "    model.eval()\n",
    "    eval_MAE_list = []\n",
    "    eval_MSE_list = []\n",
    "    eval_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(eval_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        MAE = F.l1_loss(mol_prediction, label.view(-1,1), reduction='none')        \n",
    "        MSE = F.mse_loss(mol_prediction, label.view(-1,1), reduction='none')\n",
    "        \n",
    "        eval_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        eval_MSE_list.extend(MSE.data.squeeze().cpu().numpy())\n",
    "    return np.array(eval_MAE_list).mean(), np.array(eval_MSE_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = Logger()\n",
    "# log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "# f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "# log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "# start = timer()\n",
    "\n",
    "# best_param ={}\n",
    "# best_param[\"train_epoch\"] = 0\n",
    "# best_param[\"valid_epoch\"] = 0\n",
    "# best_param[\"train_MSE\"] = 9e8\n",
    "# best_param[\"valid_MSE\"] = 9e8\n",
    "\n",
    "# fold_index = 3\n",
    "# for epoch in range(800):\n",
    "#     losses = train(smiles_list[train_fold[fold_index]])\n",
    "#     traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "#     valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "\n",
    "#     timing = time_to_str((timer() - start), 'min')  \n",
    "#     log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "\n",
    "#     if train_MSE < best_param[\"train_MSE\"]:\n",
    "#         best_param[\"train_epoch\"] = epoch\n",
    "#         best_param[\"train_MSE\"] = train_MSE\n",
    "#     if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#         best_param[\"valid_epoch\"] = epoch\n",
    "#         best_param[\"valid_MSE\"] = valid_MSE\n",
    "# #         if valid_MSE < 0.35:\n",
    "# #              torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#     if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "#         break\n",
    "# print(best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | loss | train MSE |  valid MSE |  time \n",
      "  0   | 5.1915  | 25.6658 | 21.3439 |  0 hr 00 min \n",
      "  1   | 2.0234  | 12.2007 | 10.1084 |  0 hr 00 min \n",
      "  2   | 0.9561  | 3.5481  | 2.6428  |  0 hr 00 min \n",
      "  3   | 0.8637  | 2.3242  | 1.7554  |  0 hr 00 min \n",
      "  4   | 0.6192  | 0.8833  | 0.6908  |  0 hr 00 min \n",
      "  5   | 0.7772  | 0.6922  | 0.6738  |  0 hr 00 min \n",
      "  6   | 0.4953  | 0.3700  | 0.3635  |  0 hr 00 min \n",
      "  7   | 0.4688  | 0.5090  | 0.6114  |  0 hr 00 min \n",
      "  8   | 0.4490  | 0.3317  | 0.4090  |  0 hr 00 min \n",
      "  9   | 0.5682  | 0.6674  | 0.6083  |  0 hr 00 min \n",
      " 10   | 0.5277  | 0.3360  | 0.3656  |  0 hr 00 min \n",
      " 11   | 0.5263  | 0.2824  | 0.3415  |  0 hr 00 min \n",
      " 12   | 0.4889  | 0.3553  | 0.4436  |  0 hr 00 min \n",
      " 13   | 0.4328  | 0.3312  | 0.4300  |  0 hr 00 min \n",
      " 14   | 0.4299  | 0.2735  | 0.3405  |  0 hr 00 min \n",
      " 15   | 0.4266  | 0.2824  | 0.3615  |  0 hr 00 min \n",
      " 16   | 0.3762  | 0.2817  | 0.4288  |  0 hr 00 min \n",
      " 17   | 0.4628  | 0.2617  | 0.3470  |  0 hr 00 min \n",
      " 18   | 0.4234  | 0.2254  | 0.3342  |  0 hr 01 min \n",
      " 19   | 0.3372  | 0.2122  | 0.3475  |  0 hr 01 min \n",
      " 20   | 0.3198  | 0.1936  | 0.3581  |  0 hr 01 min \n",
      " 21   | 0.3337  | 0.1839  | 0.3567  |  0 hr 01 min \n",
      " 22   | 0.3789  | 0.2937  | 0.4374  |  0 hr 01 min \n",
      " 23   | 0.2866  | 0.2888  | 0.4296  |  0 hr 01 min \n",
      " 24   | 0.3804  | 0.2978  | 0.5327  |  0 hr 01 min \n",
      " 25   | 0.3919  | 0.2532  | 0.4079  |  0 hr 01 min \n",
      " 26   | 0.3765  | 0.2810  | 0.3776  |  0 hr 01 min \n",
      " 27   | 0.2997  | 0.1577  | 0.3189  |  0 hr 01 min \n",
      " 28   | 0.2819  | 0.1634  | 0.3393  |  0 hr 01 min \n",
      " 29   | 0.2441  | 0.2314  | 0.3878  |  0 hr 01 min \n",
      " 30   | 0.2871  | 0.1710  | 0.3471  |  0 hr 01 min \n",
      " 31   | 0.3229  | 0.2403  | 0.3944  |  0 hr 01 min \n",
      " 32   | 0.3013  | 0.1779  | 0.3692  |  0 hr 01 min \n",
      " 33   | 0.2674  | 0.2843  | 0.4178  |  0 hr 01 min \n",
      " 34   | 0.3523  | 0.1672  | 0.3858  |  0 hr 01 min \n",
      " 35   | 0.3235  | 0.1609  | 0.3132  |  0 hr 02 min \n",
      " 36   | 0.2377  | 0.1871  | 0.3780  |  0 hr 02 min \n",
      " 37   | 0.2494  | 0.1478  | 0.3640  |  0 hr 02 min \n",
      " 38   | 0.2317  | 0.1269  | 0.3630  |  0 hr 02 min \n",
      " 39   | 0.2731  | 0.1219  | 0.3676  |  0 hr 02 min \n",
      " 40   | 0.2165  | 0.1583  | 0.3927  |  0 hr 02 min \n",
      " 41   | 0.2808  | 0.1433  | 0.3690  |  0 hr 02 min \n",
      " 42   | 0.2509  | 0.1770  | 0.4042  |  0 hr 02 min \n",
      " 43   | 0.2543  | 0.1318  | 0.3446  |  0 hr 02 min \n",
      " 44   | 0.2520  | 0.1193  | 0.3425  |  0 hr 02 min \n",
      " 45   | 0.1965  | 0.1161  | 0.3557  |  0 hr 02 min \n",
      " 46   | 0.2415  | 0.1158  | 0.3274  |  0 hr 02 min \n",
      " 47   | 0.1912  | 0.1598  | 0.3798  |  0 hr 02 min \n",
      " 48   | 0.2199  | 0.1249  | 0.3587  |  0 hr 02 min \n",
      " 49   | 0.2350  | 0.1508  | 0.3792  |  0 hr 02 min \n",
      " 50   | 0.2345  | 0.1156  | 0.3918  |  0 hr 02 min \n",
      " 51   | 0.2436  | 0.1079  | 0.3547  |  0 hr 02 min \n",
      " 52   | 0.2326  | 0.1249  | 0.3906  |  0 hr 02 min \n",
      " 53   | 0.2959  | 0.1171  | 0.3684  |  0 hr 03 min \n",
      " 54   | 0.1883  | 0.1008  | 0.3608  |  0 hr 03 min \n",
      " 55   | 0.2755  | 0.1442  | 0.4194  |  0 hr 03 min \n",
      " 56   | 0.1936  | 0.1243  | 0.4042  |  0 hr 03 min \n",
      " 57   | 0.1843  | 0.0832  | 0.3890  |  0 hr 03 min \n",
      " 58   | 0.1980  | 0.1324  | 0.4368  |  0 hr 03 min \n",
      " 59   | 0.2118  | 0.1016  | 0.3630  |  0 hr 03 min \n",
      " 60   | 0.2782  | 0.1528  | 0.3822  |  0 hr 03 min \n",
      " 61   | 0.2309  | 0.0996  | 0.3651  |  0 hr 03 min \n",
      " 62   | 0.2050  | 0.1247  | 0.3976  |  0 hr 03 min \n",
      " 63   | 0.2967  | 0.1181  | 0.3207  |  0 hr 03 min \n",
      " 64   | 0.1943  | 0.0918  | 0.3599  |  0 hr 03 min \n",
      " 65   | 0.1918  | 0.1021  | 0.3581  |  0 hr 03 min \n",
      " 66   | 0.1811  | 0.1169  | 0.4161  |  0 hr 03 min \n",
      " 67   | 0.2357  | 0.1045  | 0.3712  |  0 hr 03 min \n",
      " 68   | 0.2150  | 0.1296  | 0.4447  |  0 hr 03 min \n",
      "fold | epoch | train_MSE | valid MSE \n",
      "  0   |  35   | 0.0832  | 0.3132  \n",
      "  0   | 5.1689  | 40.2427 | 45.8328 |  0 hr 03 min \n",
      "  1   | 1.4613  | 78.2746 | 87.6371 |  0 hr 03 min \n",
      "  2   | 1.0674  | 9.7268  | 11.3391 |  0 hr 04 min \n",
      "  3   | 0.6476  | 3.4929  | 4.1014  |  0 hr 04 min \n",
      "  4   | 0.5942  | 1.0762  | 1.3942  |  0 hr 04 min \n",
      "  5   | 0.6149  | 0.4099  | 0.6613  |  0 hr 04 min \n",
      "  6   | 0.5552  | 0.3787  | 0.6164  |  0 hr 04 min \n",
      "  7   | 0.4781  | 0.3302  | 0.5977  |  0 hr 04 min \n",
      "  8   | 0.4959  | 0.3801  | 0.6263  |  0 hr 04 min \n",
      "  9   | 0.4486  | 0.2984  | 0.5466  |  0 hr 04 min \n",
      " 10   | 0.4903  | 0.3237  | 0.5603  |  0 hr 04 min \n",
      " 11   | 0.4012  | 0.3023  | 0.5847  |  0 hr 04 min \n",
      " 12   | 0.4096  | 0.3695  | 0.6137  |  0 hr 04 min \n",
      " 13   | 0.4363  | 0.2268  | 0.4881  |  0 hr 04 min \n",
      " 14   | 0.3281  | 0.2401  | 0.5084  |  0 hr 04 min \n",
      " 15   | 0.3512  | 0.2316  | 0.4770  |  0 hr 04 min \n",
      " 16   | 0.3360  | 0.2853  | 0.5645  |  0 hr 04 min \n",
      " 17   | 0.3429  | 0.2567  | 0.5413  |  0 hr 04 min \n",
      " 18   | 0.3768  | 0.2360  | 0.5073  |  0 hr 04 min \n",
      " 19   | 0.3359  | 0.2116  | 0.4177  |  0 hr 04 min \n",
      " 20   | 0.3553  | 0.2171  | 0.5009  |  0 hr 05 min \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3c843326f5f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mtraine_MAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mvalid_MAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfold_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4a8b8c0f49f0>\u001b[0m in \u001b[0;36meval\u001b[0;34m(smiles_list)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mmol_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbond_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mMAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AttentiveFP/code/AttentiveFP/AttentiveLayers_new3.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AttentiveFP/code/AttentiveFP/AttentiveLayers_new3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, atom, bond, bond_index, mol_index)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0msuperatom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuperGather\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuperatom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmol_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuperatom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AttentiveFP/code/AttentiveFP/AttentiveLayers_new3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, superatom, atom, mol_index)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0msuperatom_expand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuperatom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmol_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mfeature_align\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuperatom_expand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matom\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0malign_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_align\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "log = Logger()\n",
    "log.open(f'log/{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "start = timer()\n",
    "\n",
    "log2 = Logger()\n",
    "log2.open(f'{prefix_filename}_best_{start_time}.txt')\n",
    "f2 = '{:^5} | {:^5} | {:^7.4f} | {:^7.4f} \\n'\n",
    "\n",
    "for fold_index in range(5):\n",
    "    \n",
    "    model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "    \n",
    "    best_param ={}\n",
    "    best_param[\"train_epoch\"] = 0\n",
    "    best_param[\"valid_epoch\"] = 0\n",
    "    best_param[\"train_MSE\"] = 9e8\n",
    "    best_param[\"valid_MSE\"] = 9e8\n",
    "    for epoch in range(800):\n",
    "        losses = train(smiles_list[train_fold[fold_index]])\n",
    "        traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "        valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "        \n",
    "        timing = time_to_str((timer() - start), 'min')  \n",
    "        log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "        \n",
    "        if train_MSE < best_param[\"train_MSE\"]:\n",
    "            best_param[\"train_epoch\"] = epoch\n",
    "            best_param[\"train_MSE\"] = train_MSE\n",
    "        if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "            best_param[\"valid_epoch\"] = epoch\n",
    "            best_param[\"valid_MSE\"] = valid_MSE\n",
    "#             if valid_MSE < 0.35:\n",
    "#                  torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "        if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "            break\n",
    "\n",
    "    log2.write('fold | epoch | train_MSE | valid MSE \\n')\n",
    "    log2.write(f2.format(fold_index, best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model\n",
    "# best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"valid_epoch\"])+'.pt')     \n",
    "\n",
    "# best_model_dict = best_model.state_dict()\n",
    "# best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "# (best_model.align[0].weight == model.align[0].weight).all()\n",
    "# test_MAE, test_MSE = eval(model, test_df)\n",
    "# print(\"best epoch:\",best_param[\"test_epoch\"],\"\\n\",\"test MSE:\",test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range(20):\n",
    "#     losses = train(smiles_list[valid_fold[fold_index]])\n",
    "#     print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
