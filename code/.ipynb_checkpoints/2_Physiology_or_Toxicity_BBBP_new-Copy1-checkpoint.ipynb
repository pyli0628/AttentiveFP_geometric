{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "import random\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#then import my own modules\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight\n",
    "from timeit import default_timer as timer\n",
    "from AttentiveFP.featurizing import graph_dict\n",
    "from AttentiveFP.AttentiveLayers_new3 import Fingerprint, graph_dataset, null_collate, Graph, Logger, time_to_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_aviable = torch.cuda.is_available()\n",
    "device = torch.device(0)\n",
    "\n",
    "SEED = 8\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  2050\n",
      "failed to process smiles:  O=N([O-])C1=C(CN=C1NCCSCc2ncccc2)Cc3ccccc3\n",
      "failed to process smiles:  c1(nc(NC(N)=[NH2])sc1)CSCCNC(=[NH]C#N)NC\n",
      "failed to process smiles:  Cc1nc(sc1)\\[NH]=C(\\N)N\n",
      "failed to process smiles:  s1cc(CSCCN\\C(NC)=[NH]\\C#N)nc1\\[NH]=C(\\N)N\n",
      "failed to process smiles:  c1c(c(ncc1)CSCCN\\C(=[NH]\\C#N)NCC)Br\n",
      "failed to process smiles:  n1c(csc1\\[NH]=C(\\N)N)c1ccccc1\n",
      "failed to process smiles:  n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)N\n",
      "failed to process smiles:  n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)NC(C)=O\n",
      "failed to process smiles:  n1c(csc1\\[NH]=C(\\N)N)c1cccc(c1)N\\C(NC)=[NH]\\C#N\n",
      "failed to process smiles:  s1cc(nc1\\[NH]=C(\\N)N)C\n",
      "failed to process smiles:  c1(cc(N\\C(=[NH]\\c2cccc(c2)CC)C)ccc1)CC\n",
      "number of successfully processed smiles:  2039\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAC/CAYAAAB+KF5fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAASqElEQVR4nO3df0xV9/3H8dcFFBUQmLuD78pa7S6XlDpwWRGZRDJ+GNO0I7qtnftjdXWhW8rm1uLsxmRdYjqjTNpCCLWb6bam3ZolwzAXLC0d3cQSg9No1N4LLgux8ZZW8SJ0t8C93z8a7rwC914+crmAz0diKp/zvqefc468+NzPOfeDxefz+QQAmJaYaHcAAOYjwhMADBCeAGCA8AQAA4QnABggPAHAAOEJAAbiot2BmXD16pC83qkfV12xIlEffnh9FnuEyXAd5gauQ3hiYixKTU2YcvuCCE+v1xc0PMdrEH1ch7mB63DreNsOAAYITwAwQHgCgAHCEwAMLIgbRvPdqFfyjIwGrYlfFKc4ftQBcwbhOQd4RkZ14rwraE3ePWmKi+dyAXMFYxkAMEB4AoABwhMADBCeAGCA8AQAA4QnABggPAHAAOEJAAYITwAwQHgCgAHCEwAMEJ4AYIDwBAADhCcAGCA8AcAA4QkABghPADBAeAKAAcITAAwQngBggPAEAAOEJwAYIDwBwADhCQAGCE8AMEB4AoABo/Csr69XVlaWysvLJ2w7duyYHnroIeXk5KigoEA1NTVyu90T6oaGhrRnzx4VFhYqJydHW7Zs0ZtvvmnSHQCYddMOT6fTqRdffFGf/vSnJ2zr6upSRUWF0tPT1dTUpF27dqm9vV0VFRXyer0BtZWVlWppadGOHTv0wgsvyGazqbKyUh0dHeZHAwCzJG46xV6vV9XV1frGN74hh8MxYUS5f/9+ZWZm6tlnn1VMzCe5bLVa9eijj6q1tVX333+/JKmjo0OdnZ1qaGhQWVmZJGndunXq6+vT3r17VVRUNBPHBgARM62R50svvaTLly/rxz/+8YRtLpdLZ86cUXl5uT84JWn9+vVKS0vT0aNH/W1tbW1KSkpSSUmJv81isWjz5s26ePGienp6TI4FAGZN2OHZ19en559/XjU1NUpMTJyw3eFwSJIyMzMnbLPb7XI6nf6vnU6nbDZbQMhKUlZWVsC+AGCuCuttu8/n089//nMVFhaqtLR00pqBgQFJUnJy8oRtycnJOnfuXEDtypUrJ627cV/hWrFiYpjfzGpNmtY+Z5PvyrCSEpcErVm2LF7WTy2bpR5Fzly+DrcTrsOtCys8X3vtNZ09e1Z/+9vfQtZaLJaw2qeqC7VtMh9+eF1er2/K7VZrkvr7B6e1z5ky6pU8I6NBa7w+afD6f4PWDA971D82NpNdm3XRvA74H65DeGJiLEEHZiHD88qVK9q/f78ee+wxLV261H+TaHR0VF6vV263W/Hx8UpJSZE0+ajx2rVrASPSlJSUKeukyUev85VnZFQnzruC1uTarbPUGwAzJeScp8vl0uDgoH79618rLy/P/+fkyZNyOBzKy8tTfX29f67zxrnNcQ6HI2Au1Gazqbe3d8LjS+NznXa7/ZYOCgAiLeTI884779Tvf//7Ce3PPPOMhoeHtWfPHn32s59Venq6Vq9erZaWFj3yyCP+m0HHjx+Xy+XSxo0b/a8tKyvTn//8Z7W3twfMoTY3N2vVqlWy2WwzcWwAEDEhwzMhIUH5+fkT2pcvXy5JAduqqqq0fft2PfHEE3r44YflcrlUW1ur3Nxcbdq0yV9XVFSk/Px8VVdXa2BgQBkZGWpublZ3d7caGxtn4rgAIKKm9ZB8KAUFBWpqalJ9fb0qKiqUkJCg0tJS7dy5U7Gxsf46i8WixsZGHThwQHV1dXK73bLZbGpoaFBxcfFMdgkAIsLi8/mmvk09T8zlu+1DnvBuGJ129AetybsnTQnxM/qzbtZxl3du4DqEJ9TddlZVAgADhCcAGCA8AcAA4QkABghPADBAeAKAAcITAAwQngBggPAEAAOEJwAYmN+f97uNWGIsGvIEX1RZkuIXxSmOH4lAxBGe84RnZCzk59+lTz4DHzfPPwMPzAeMUQDAAOEJAAYITwAwQHgCgAHCEwAMEJ4AYIDwBAADhCcAGCA8AcAA4QkABghPADBAeAKAAcITAAwQngBggPAEAAOEJwAYIDwBwADhCQAGCE8AMEB4AoABwhMADBCeAGCA8AQAA/yC7wXGEmPRkGc0aE38ojjF8WMTuCWE5wLjGRnTaUd/0Jq8e9IUF8+lB24F4w8AMEB4AoABwhMADBCeAGCA8AQAA4QnABgI+bzK8ePHdfjwYf3rX//S5cuXlZycrJycHP3gBz9QVlZWQO2xY8f03HPP6cKFC0pISFBZWZmqqqq0fPnygLqhoSHV1dWptbVVbrdbNptNjz/+uEpKSmb26AAgQkKOPF999VW999572rZtm1588UU99dRTeu+99/T1r39dp06d8td1dXWpoqJC6enpampq0q5du9Te3q6Kigp5vd6AfVZWVqqlpUU7duzQCy+8IJvNpsrKSnV0dMz8EQJABIQcef7iF7/QihUrAtoKCwtVUlKi3/72t6qvr5ck7d+/X5mZmXr22WcVE/NJJlutVj366KNqbW3V/fffL0nq6OhQZ2enGhoaVFZWJklat26d+vr6tHfvXhUVFc3oAQJAJIQced4cnJK0fPly3XXXXbp8+bIkyeVy6cyZMyovL/cHpyStX79eaWlpOnr0qL+tra1NSUlJAW/RLRaLNm/erIsXL6qnp+eWDggAZoPRDaMrV67I6XQqMzNTkuRwOCTJ//WN7Ha7nE6n/2un0ymbzRYQspL886fj+wKAuWzaH3D2+XzavXu3vF6vtm/fLkkaGBiQJCUnJ0+oT05O1rlz5/xfDwwMaOXKlZPW3biv6VixIjFkjdWaNO39zgTflWElJS4JWrNoUdyM1IRbt2xZvKyfWhZyX5EQreuAQFyHWzft8Ny3b5/eeOMN/epXv9LnP//5gG0Wi2XS19zcPlVdqG1T+fDD6/J6fVNut1qT1N8/OO39hjLqlTwjwVcw8vqkwev/DVozMjI6IzXh1g0Pe9Q/NhZyXzMtUtcB08N1CE9MjCXowGxa4VlXV6dDhw6purpaW7Zs8benpKRImnzUeO3atYARaUpKypR10uSj17nKMzKqE+ddQWty7dZZ6g2A2RT2nOdzzz2npqYm7dy5U9/+9rcDto3Pdd44tznO4XAEzIXabDb19vZOeHxpfK7TbreH33sAiJKwwrOhoUGNjY3asWOHvvvd707Ynp6ertWrV6ulpSUgFI8fPy6Xy6WNGzf628rKyuR2u9Xe3h6wj+bmZq1atUo2m830WABg1oR8237o0CHV19frK1/5ir785S8HPBi/ePFiZWdnS5Kqqqq0fft2PfHEE3r44YflcrlUW1ur3Nxcbdq0yf+aoqIi5efnq7q6WgMDA8rIyFBzc7O6u7vV2NgYgUMEgJkXMjzfeust/3/H/z7ujjvu8I8gCwoK1NTUpPr6elVUVCghIUGlpaXauXOnYmNj/a+xWCxqbGzUgQMHVFdX5/94ZkNDg4qLi2fy2AAgYkKG5x/+8Iewd7ZhwwZt2LAhZF1iYqJqampUU1MT9r4BYC5hVSUAMEB4AoABwhMADBCeAGCA8AQAA4QnABggPAHAAOEJAAYITwAwQHgCgAHCEwAMEJ4AYIDwBAADhCcAGCA8AcAA4QkABghPADBAeAKAAcITAAyE/B1Gt6tRr+QZGQ1a4/XNUmcAzDmE5xQ8I6M6cd4VtCbXbp2l3gCYawjP25AlxqIhT/BRdfyiOMUxqQNMifC8DXlGxnTa0R+0Ju+eNMXF888DmApjCwAwQHgCgAHCEwAMEJ4AYIDwBAADhCcAGCA8AcAA4QkABghPADBAeAKAAcITAAwQngBggPAEAAOEJwAYuC3XHGOVeAC36rYMT1aJB3CreNsOAAYITwAwQHgCgIHbcs4TofFL4oDgohaeQ0NDqqurU2trq9xut2w2mx5//HGVlJREq0u4Ab8kDgguav/yKysrde7cOVVVVSkjI0N/+ctfVFlZqaamJhUVFUWrW5iGcEani+LiNDL6SY3vyrCGJ6kPZwQbzuNljIQxm6ISnh0dHers7FRDQ4PKysokSevWrVNfX5/27t1LeM4T4YxOc+1Wf01S4hINXv/vhJq196bLMxL8wVqvT+q+EPzxMkbCmE1R+ZfW1tampKSkgLfoFotFmzdv1u7du9XT0yObzRaNriEKwg1hYC6JSng6nU7ZbDbFxAS+x8rKypIkORyOaYVnTIxlWjVxsTFatmRR0Pr5WDMX+3RjzdL4OI2NTqyfsf/Xolh5Rr1BaxbHxSp2Ft/aj3mlj0fHgtbMdp+k8L5nbnehzlFUwnNgYEArV66c0J6cnOzfPh2pqQkha1asSAz4OuP/kkO+5u6M1HlXMxf7NNvHj9Bu/n7A9EVtet1imTrVg20DgLkgKuGZkpIy6ejy2rVrkv43AgWAuSoq4Wmz2dTb2yuvN3B+yuFwSJLsdns0ugUAYYtKeJaVlcntdqu9vT2gvbm5WatWreJOO4A5Lyo3jIqKipSfn6/q6moNDAwoIyNDzc3N6u7uVmNjYzS6BADTYvH5fFFZ9vf69es6cOCAjh49GvDxzNLS0mh0BwCmJWrhCQDzGZ8EBgADhCcAGFiw4Tk0NKQ9e/aosLBQOTk52rJli958881od2vB6urqUlZW1qR/ent7A2qPHTumhx56SDk5OSooKFBNTY3cbneUej5/Xb58WXv27NHWrVv1xS9+UVlZWerq6pq0tqWlRV/96lf1hS98QRs2bFBtba08Hs+Eug8++EC7du1Sfn6+1qxZo29961s6efJkpA9lXlqwS9Cw5F10VFVVKS8vL6AtIyPD//euri5VVFSopKREP/rRj/T++++rtrZWDodDr7zyyoT1DjC1//znPzpy5Iiys7O1bt26CY/+jTt8+LB+8pOfaOvWrfrZz36m3t5e1dbW6tKlS6qrq/PXeTwebdu2TcPDw9q9e7dSUlL0u9/9Ttu2bdMf//hHZWdnz9ahzQ++Bejvf/+7z263+15//XV/m9fr9X3zm9/0bdq0KYo9W7jeeecdn91u97W1tQWt+9rXvuYrLy/3jY2N+dv++c9/+ux2u+/IkSOR7uaCcuM5bGtr89ntdt8777wTUDM6Oupbv36973vf+15A+5/+9Cef3W73nTp1yt/28ssv++x2u+/s2bP+No/H4ysuLvZt3749Qkcxfy3IH/PBlry7ePGienp6oti725fL5dKZM2dUXl4eMMJcv3690tLSdPTo0Sj2bv4JZ5R+6tQp9ff3a/PmzQHtDz74oBYtWhRwzt944w3Z7Xbde++9/rbFixfrgQceUGdnp65fvz5znV8AFmR4hrPkHSKjpqZG2dnZ+tKXvqTHHntMZ8+e9W8bP++ZmZkTXme32+V0Ometn7eL8XN68zlfunSpPve5zwWcc6fTOelHo7OysjQ2NqaLFy9GtrPzzIKc85zpJe8QWlJSkh555BGtXbtWKSkp6u3t1cGDB7V161a9/PLLys3N9Z/3yRZ+SU5O1rlz52a72wteqHN+4/fCwMDAlHWSdPXq1Qj1cn5akOEpseTdbMvOzg64oXDfffepuLhYDzzwgOrq6vTSSy/5t011/rkukRPuOef7JnwL8m07S97NDVarVYWFhTp9+rSkT66LNPnI/9q1a1yXCJjOOQ/1fTO+L3xiQYYnS97NHTdeg/F5t8nmNh0Ox6Rzobg14yuU3XzOP/roI/X19QWcc5vNNun9gHfffVexsbG6++67I9vZeWZBhidL3s0N/f396uzs1Jo1ayRJ6enpWr16tVpaWgJC9fjx43K5XNq4cWO0urpgrVmzRlarVYcPHw5o/+tf/6qRkZGAc15WViaHw6Hz58/72z7++GMdOXJEBQUFSkzkV3fcKPbpp59+OtqdmGl33XWXTpw4oddee02pqalyu91qaGjQW2+9pWeeeUarVq2KdhcXnCeffFLnz5/X4OCgPvjgA/3jH//QT3/6Uw0ODmr//v1KS0uTJN155506dOiQenp6lJycrO7ubv3yl79UZmamnnrqKR6Sn6bW1lb19PTo9OnTOnnypDIyMnTlyhVdunRJK1euVExMjFJTU3Xw4EFdvXpVS5Ys0dtvv619+/apuLhY3/nOd/z7ysrK0uuvv66WlhZZrVa9//772rt3r959913V1tbqM5/5TBSPdO5ZsKsqseTd7Dp48KCOHDmiS5cu6aOPPlJKSorWrl2r73//+xOmSd5++23V19frwoULSkhIUGlpqXbu3Mmcp4Hxx+9udscddwS88zp8+LB+85vf6N///rdSU1P14IMP6oc//KGWLFkS8Lr+/n7t27dPHR0d8ng8ys7O1pNPPqn77rsvoscxHy3Y8ASASOI9EgAYIDwBwADhCQAGCE8AMEB4AoABwhMADBCeAGCA8AQAA4QnABj4f23Myd7jPmxxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'BBBP'\n",
    "tasks = ['BBBP']\n",
    "raw_filename = \"../data/BBBP.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(\"failed to process smiles: \", smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)]\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 200\n",
    "\n",
    "p_dropout= 0.2\n",
    "fingerprint_dim = 32\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 2.5\n",
    "K = 2\n",
    "T = 2\n",
    "per_task_output_units_num = 2 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph dicts saved as ../data/BBBP.pkl\n"
     ]
    }
   ],
   "source": [
    "smiles_list = smiles_tasks_df['smiles'].values\n",
    "label_list = smiles_tasks_df[tasks[0]].values\n",
    "graph_dict = graph_dict(smiles_list, label_list, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "class ScaffoldGenerator(object):\n",
    "    \"\"\"\n",
    "    Generate molecular scaffolds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    include_chirality : : bool, optional (default False)\n",
    "      Include chirality in scaffolds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, include_chirality=False):\n",
    "        self.include_chirality = include_chirality\n",
    "\n",
    "    def get_scaffold(self, mol):\n",
    "        \"\"\"\n",
    "        Get Murcko scaffolds for molecules.\n",
    "\n",
    "        Murcko scaffolds are described in DOI: 10.1021/jm9602928. They are\n",
    "        essentially that part of the molecule consisting of rings and the\n",
    "        linker atoms between them.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mols : array_like\n",
    "            Molecules.\n",
    "        \"\"\"\n",
    "        return MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=self.include_chirality)\n",
    "\n",
    "\n",
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    engine = ScaffoldGenerator(include_chirality=include_chirality)\n",
    "    scaffold = engine.get_scaffold(mol)\n",
    "    return scaffold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for i,task in enumerate(tasks):    \n",
    "    negative_df = smiles_tasks_df[smiles_tasks_df[task] == 0][[\"smiles\",task]]\n",
    "    positive_df = smiles_tasks_df[smiles_tasks_df[task] == 1][[\"smiles\",task]]\n",
    "    weights.append([(positive_df.shape[0]+negative_df.shape[0])/negative_df.shape[0],\\\n",
    "                    (positive_df.shape[0]+negative_df.shape[0])/positive_df.shape[0]])\n",
    "    \n",
    "scaffold_list = []\n",
    "all_scaffolds_dict = {}\n",
    "\n",
    "for index, smiles in enumerate(smiles_tasks_df['smiles']):\n",
    "    scaffold = generate_scaffold(smiles)\n",
    "    scaffold_list.append(scaffold)\n",
    "    if scaffold not in all_scaffolds_dict:\n",
    "        all_scaffolds_dict[scaffold] = [index]\n",
    "    else:\n",
    "        all_scaffolds_dict[scaffold].append(index)\n",
    "smiles_tasks_df['scaffold'] = scaffold_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaffold</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>c1ccccc1</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>O=C1C=CC2C(=C1)CCC1C3CCCC3CCC21</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>c1ccc2c(c1)Nc1ccccc1S2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>O=C1CN=C(c2ccccc2)c2ccccc2N1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>O=C1C=C2CCC3C4CCCC4CCC3C2CC1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>c1ccc(Cc2ccccc2)cc1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>O=C1CC(=O)NC(=O)N1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>O=C(Cc1ccccc1)N1CCNCC1CN1CCCC1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>c1ccncc1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            scaffold  count\n",
       "995                         c1ccccc1    137\n",
       "0                                        99\n",
       "379  O=C1C=CC2C(=C1)CCC1C3CCCC3CCC21     76\n",
       "949           c1ccc2c(c1)Nc1ccccc1S2     26\n",
       "498     O=C1CN=C(c2ccccc2)c2ccccc2N1     26\n",
       "357     O=C1C=C2CCC3C4CCCC4CCC3C2CC1     24\n",
       "837              c1ccc(Cc2ccccc2)cc1     21\n",
       "400               O=C1CC(=O)NC(=O)N1     18\n",
       "201   O=C(Cc1ccccc1)N1CCNCC1CN1CCCC1     17\n",
       "998                         c1ccncc1     16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_tasks_df.groupby(['scaffold'])['scaffold'].count() \\\n",
    "                     .reset_index(name='count') \\\n",
    "                     .sort_values(['count'], ascending=False) \\\n",
    "                     .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "20 190 45 [544, 1020, 1315, 1242, 1358, 1462, 1465, 1940, 1941, 1952, 1646, 1793, 143, 1693, 625, 1123, 1797, 244, 429, 548, 575, 652, 1296, 1731, 1887, 1956, 29, 821, 931, 1216, 814, 102, 146, 842, 882, 893, 179, 324, 596, 598, 945, 1018, 1531, 1915, 1140, 900, 1996, 319, 301, 1346, 592, 992, 1214, 1804, 1326, 641, 626, 859, 129, 1218, 1017, 1562, 1623, 975, 812, 111, 317, 2008, 390, 38, 307, 360, 131, 113, 1616, 1889, 286, 1525, 411, 1362, 1483, 482, 553, 568, 646, 702, 1413, 1783, 1920, 1353, 1585, 416, 236, 1212, 169, 2005, 25, 48, 58, 114, 275, 325, 357, 560, 561, 611, 612, 642, 1401, 1402, 1568, 1860, 437, 651, 1199, 1724, 1384, 367, 1926, 1660, 1955, 344, 693, 1125, 1280, 373, 1088, 1089, 1726, 1884, 72, 82, 334, 338, 13, 313, 1550, 1261, 1262, 1204, 535, 1352, 1587, 1998, 475, 197, 1363, 1542, 1264, 1971, 1172, 1409, 1787, 897, 899, 901, 1798, 220, 550, 630, 1097, 1106, 1249, 466, 1086, 1603, 868, 912, 225, 237, 678, 953, 1042, 1637, 1639, 1937, 813, 1385, 1670, 1903, 1930, 172, 1630, 1912, 1987, 1518, 1982, 860, 665, 1102]\n",
      "10 187 47 [1251, 40, 959, 960, 1058, 1263, 16, 547, 364, 1016, 1779, 1074, 1815, 591, 1423, 1452, 1945, 1773, 270, 45, 55, 166, 185, 186, 240, 443, 483, 484, 523, 629, 686, 687, 698, 1129, 1225, 1663, 1669, 1678, 1758, 1788, 1863, 1867, 1893, 1964, 2015, 1141, 1071, 1786, 740, 112, 1219, 1591, 486, 543, 1595, 815, 63, 942, 1919, 224, 1313, 800, 865, 947, 1068, 607, 671, 684, 1820, 1460, 524, 1495, 1007, 295, 1482, 672, 1817, 1885, 2019, 1434, 819, 870, 871, 215, 1582, 1948, 1652, 447, 1343, 979, 1332, 845, 855, 1973, 1271, 1611, 1917, 4, 144, 234, 804, 438, 627, 833, 216, 346, 1936, 1169, 824, 1657, 1916, 713, 1442, 1190, 1001, 972, 982, 1187, 1554, 1556, 1644, 1966, 1974, 1991, 1029, 95, 336, 1422, 1279, 731, 742, 831, 876, 925, 1094, 259, 261, 305, 371, 423, 413, 415, 435, 924, 445, 946, 47, 766, 767, 39, 42, 52, 57, 563, 1083, 664, 1802, 1319, 1430, 106, 1165, 122, 248, 1408, 2030, 873, 1232, 1924, 844, 1310, 1805, 181, 1662, 1658, 157, 981, 1124, 1429, 1716, 279, 328, 1765, 341, 787, 966, 930, 989]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/erikxiong/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def scaffold_randomized_spliting(scaffolds_dict, sample_size, random_seed = 0): \n",
    "    count = 0\n",
    "    minor_count = 0\n",
    "    minor_class = np.argmax(weights[0]) # weights are inverse of the ratio\n",
    "    minor_ratio= 1/weights[0][minor_class]\n",
    "    optimal_count = 0.1*len(smiles_tasks_df)\n",
    "    while (count < optimal_count*0.9 or  count > optimal_count*1.1) \\\n",
    "        or (minor_count < minor_ratio*optimal_count*0.9 \\\n",
    "            or  minor_count > minor_ratio*optimal_count*1.1):\n",
    "        random_seed +=1\n",
    "        random.seed(random_seed)\n",
    "        scaffold = random.sample(list(scaffolds_dict.keys()), sample_size)\n",
    "        count = sum([len(scaffolds_dict[scaffold]) for scaffold in scaffold])\n",
    "        index = [index for scaffold in scaffold for index in scaffolds_dict[scaffold]]\n",
    "        minor_count = len(smiles_tasks_df.iloc[index, :][smiles_tasks_df[tasks[0]] == minor_class])\n",
    "#     print(random)\n",
    "    print(random_seed, count, minor_count, index)\n",
    "    return scaffold, index\n",
    "\n",
    "samples_size = int(len(all_scaffolds_dict.keys())*0.1)\n",
    "print(samples_size)\n",
    "test_scaffold, test_index = scaffold_randomized_spliting(all_scaffolds_dict, samples_size, random_seed=8)\n",
    "training_scaffolds_dict = {x: all_scaffolds_dict[x] for x in all_scaffolds_dict.keys() if x not in test_scaffold}\n",
    "valid_scaffold, valid_index = scaffold_randomized_spliting(training_scaffolds_dict, samples_size, random_seed=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = smiles_tasks_df.iloc[test_index,:] # test set\n",
    "valid_df = smiles_tasks_df.iloc[valid_index,:] # valid set\n",
    "train_df = smiles_tasks_df.drop(test_df.index).drop(valid_df.index) # train set\n",
    "test_smiles = test_df.smiles.values\n",
    "valid_smiles = valid_df.smiles.values\n",
    "train_smiles = train_df.smiles.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76998\n",
      "preprocess.0.linear.weight torch.Size([32, 39])\n",
      "preprocess.0.linear.bias torch.Size([32])\n",
      "preprocess.0.bn.weight torch.Size([32])\n",
      "preprocess.0.bn.bias torch.Size([32])\n",
      "propagate.0.encoder.0.linear.weight torch.Size([1024, 10])\n",
      "propagate.0.encoder.0.linear.bias torch.Size([1024])\n",
      "propagate.0.encoder.0.bn.weight torch.Size([1024])\n",
      "propagate.0.encoder.0.bn.bias torch.Size([1024])\n",
      "propagate.0.align.weight torch.Size([1, 64])\n",
      "propagate.0.align.bias torch.Size([1])\n",
      "propagate.0.attend.linear.weight torch.Size([32, 32])\n",
      "propagate.0.attend.linear.bias torch.Size([32])\n",
      "propagate.0.attend.bn.weight torch.Size([32])\n",
      "propagate.0.attend.bn.bias torch.Size([32])\n",
      "propagate.0.gru.weight_ih torch.Size([96, 32])\n",
      "propagate.0.gru.weight_hh torch.Size([96, 32])\n",
      "propagate.0.gru.bias_ih torch.Size([96])\n",
      "propagate.0.gru.bias_hh torch.Size([96])\n",
      "propagate.1.encoder.0.linear.weight torch.Size([1024, 10])\n",
      "propagate.1.encoder.0.linear.bias torch.Size([1024])\n",
      "propagate.1.encoder.0.bn.weight torch.Size([1024])\n",
      "propagate.1.encoder.0.bn.bias torch.Size([1024])\n",
      "propagate.1.align.weight torch.Size([1, 64])\n",
      "propagate.1.align.bias torch.Size([1])\n",
      "propagate.1.attend.linear.weight torch.Size([32, 32])\n",
      "propagate.1.attend.linear.bias torch.Size([32])\n",
      "propagate.1.attend.bn.weight torch.Size([32])\n",
      "propagate.1.attend.bn.bias torch.Size([32])\n",
      "propagate.1.gru.weight_ih torch.Size([96, 32])\n",
      "propagate.1.gru.weight_hh torch.Size([96, 32])\n",
      "propagate.1.gru.bias_ih torch.Size([96])\n",
      "propagate.1.gru.bias_hh torch.Size([96])\n",
      "superGather.0.align.weight torch.Size([1, 64])\n",
      "superGather.0.align.bias torch.Size([1])\n",
      "superGather.0.attend.linear.weight torch.Size([32, 32])\n",
      "superGather.0.attend.linear.bias torch.Size([32])\n",
      "superGather.0.attend.bn.weight torch.Size([32])\n",
      "superGather.0.attend.bn.bias torch.Size([32])\n",
      "superGather.0.gru.weight_ih torch.Size([96, 32])\n",
      "superGather.0.gru.weight_hh torch.Size([96, 32])\n",
      "superGather.0.gru.bias_ih torch.Size([96])\n",
      "superGather.0.gru.bias_hh torch.Size([96])\n",
      "superGather.1.align.weight torch.Size([1, 64])\n",
      "superGather.1.align.bias torch.Size([1])\n",
      "superGather.1.attend.linear.weight torch.Size([32, 32])\n",
      "superGather.1.attend.linear.bias torch.Size([32])\n",
      "superGather.1.attend.bn.weight torch.Size([32])\n",
      "superGather.1.attend.bn.bias torch.Size([32])\n",
      "superGather.1.gru.weight_ih torch.Size([96, 32])\n",
      "superGather.1.gru.weight_hh torch.Size([96, 32])\n",
      "superGather.1.gru.bias_ih torch.Size([96])\n",
      "superGather.1.gru.bias_hh torch.Size([96])\n",
      "predict.0.linear.weight torch.Size([512, 32])\n",
      "predict.0.linear.bias torch.Size([512])\n",
      "predict.0.bn.weight torch.Size([512])\n",
      "predict.0.bn.bias torch.Size([512])\n",
      "predict.3.weight torch.Size([2, 512])\n",
      "predict.3.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "loss_function = [nn.CrossEntropyLoss(torch.Tensor(weight),reduction='mean') for weight in weights]\n",
    "\n",
    "model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "# optimizer = optim.SGD(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in  model.parameters()])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, smiles_list):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=True, worker_init_fn=np.random.seed(SEED),drop_last=True)\n",
    "    losses = []\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(train_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "#         label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        \n",
    "        loss = 0.0\n",
    "        for i,task in enumerate(tasks):\n",
    "            y_pred = mol_prediction[:, i * per_task_output_units_num:(i + 1) *\n",
    "                                    per_task_output_units_num]\n",
    "            \n",
    "            validInds = np.where((label==0) | (label==1))[0]\n",
    "#             validInds = np.where(label != -1)[0]\n",
    "            if len(validInds) == 0:\n",
    "                continue\n",
    "            label_adjust = np.array([label[v] for v in validInds]).astype(float)\n",
    "            validInds = torch.cuda.LongTensor(validInds).squeeze()\n",
    "            y_pred_adjust = torch.index_select(y_pred, 0, validInds)\n",
    "\n",
    "            loss += loss_function[i](\n",
    "                y_pred_adjust,\n",
    "                torch.cuda.LongTensor(label_adjust))\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "        \n",
    "def eval(model, smiles_list):\n",
    "    model.eval()\n",
    "    label_list = {}\n",
    "    y_pred_list = {}\n",
    "    losses_list = []\n",
    "    eval_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(eval_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "#         label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        for i,task in enumerate(tasks):\n",
    "            y_pred = mol_prediction[:, i * per_task_output_units_num:(i + 1) *\n",
    "                                    per_task_output_units_num]\n",
    "\n",
    "            validInds = np.where((label==0) | (label==1))[0]\n",
    "#             validInds = np.where((label=='0') | (label=='1'))[0]\n",
    "            if len(validInds) == 0:\n",
    "                continue\n",
    "            label_adjust = np.array([label[v] for v in validInds]).astype(float)\n",
    "            validInds = torch.cuda.LongTensor(validInds).squeeze()\n",
    "            y_pred_adjust = torch.index_select(y_pred, 0, validInds)\n",
    "            loss = loss_function[i](\n",
    "                y_pred_adjust,\n",
    "                torch.cuda.LongTensor(label_adjust))\n",
    "            \n",
    "            y_pred_adjust = F.softmax(y_pred_adjust,dim=-1).data.cpu().numpy()[:,1]\n",
    "            losses_list.append(loss.cpu().detach().numpy())\n",
    "            try:\n",
    "                label_list[i].extend(label_adjust)\n",
    "                y_pred_list[i].extend(y_pred_adjust)\n",
    "            except:\n",
    "                label_list[i] = []\n",
    "                y_pred_list[i] = []\n",
    "                label_list[i].extend(label_adjust)\n",
    "                y_pred_list[i].extend(y_pred_adjust)\n",
    "                \n",
    "    eval_roc = [roc_auc_score(label_list[i], y_pred_list[i]) for i in range(len(tasks))]\n",
    "#     eval_prc = [auc(precision_recall_curve(label_list[i], y_pred_list[i])[1],precision_recall_curve(label_list[i], y_pred_list[i])[0]) for i in range(len(tasks))]\n",
    "#     eval_precision = [precision_score(label_list[i],\n",
    "#                                      (np.array(y_pred_list[i]) > 0.5).astype(int)) for i in range(len(tasks))]\n",
    "#     eval_recall = [recall_score(label_list[i],\n",
    "#                                (np.array(y_pred_list[i]) > 0.5).astype(int)) for i in range(len(tasks))]\n",
    "    eval_loss = np.array(losses_list).mean()\n",
    "    \n",
    "    return eval_roc, eval_loss #eval_prc, eval_precision, eval_recall, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch |  losses  | train loss |  valid loss | train roc |  valid roc |  time \n",
      "  0   | 0.5365  | 0.5928  | 0.8019  | 0.8409  | 0.9096  |  0 hr 00 min \n",
      "  1   | 0.4355  | 0.5132  | 0.4109  | 0.8973  | 0.9085  |  0 hr 00 min \n",
      "  2   | 0.4096  | 0.4130  | 0.3958  | 0.9304  | 0.9097  |  0 hr 00 min \n",
      "  3   | 0.3885  | 0.3090  | 0.4240  | 0.9402  | 0.9033  |  0 hr 00 min \n",
      "  4   | 0.3617  | 0.2778  | 0.4268  | 0.9509  | 0.8973  |  0 hr 00 min \n",
      "  5   | 0.3398  | 0.2776  | 0.5259  | 0.9518  | 0.8617  |  0 hr 00 min \n",
      "  6   | 0.3511  | 0.3869  | 0.3834  | 0.9336  | 0.9122  |  0 hr 00 min \n",
      "  7   | 0.3403  | 0.2906  | 0.4133  | 0.9489  | 0.9119  |  0 hr 00 min \n",
      "  8   | 0.3527  | 0.2720  | 0.5203  | 0.9561  | 0.8647  |  0 hr 00 min \n",
      "  9   | 0.3119  | 0.2526  | 0.4964  | 0.9596  | 0.8878  |  0 hr 00 min \n",
      " 10   | 0.2809  | 0.2441  | 0.3789  | 0.9673  | 0.9281  |  0 hr 01 min \n",
      " 11   | 0.3149  | 0.2708  | 0.4545  | 0.9528  | 0.9158  |  0 hr 01 min \n",
      " 12   | 0.2981  | 0.2572  | 0.3686  | 0.9712  | 0.9201  |  0 hr 01 min \n",
      " 13   | 0.2785  | 0.2173  | 0.4627  | 0.9720  | 0.8957  |  0 hr 01 min \n",
      " 14   | 0.2741  | 0.3059  | 0.3430  | 0.9677  | 0.9426  |  0 hr 01 min \n",
      " 15   | 0.2594  | 0.2279  | 0.4572  | 0.9780  | 0.8954  |  0 hr 01 min \n",
      " 16   | 0.2508  | 0.1915  | 0.4756  | 0.9766  | 0.9138  |  0 hr 01 min \n",
      " 17   | 0.2294  | 0.1696  | 0.4542  | 0.9831  | 0.8985  |  0 hr 01 min \n",
      " 18   | 0.2309  | 0.1744  | 0.4050  | 0.9830  | 0.9257  |  0 hr 01 min \n",
      " 19   | 0.2306  | 0.2495  | 0.4408  | 0.9808  | 0.9140  |  0 hr 01 min \n",
      " 20   | 0.2319  | 0.1784  | 0.5107  | 0.9803  | 0.8924  |  0 hr 01 min \n",
      " 21   | 0.2086  | 0.1794  | 0.5320  | 0.9822  | 0.9174  |  0 hr 02 min \n",
      " 22   | 0.2152  | 0.1878  | 0.4176  | 0.9816  | 0.9196  |  0 hr 02 min \n",
      " 23   | 0.2192  | 0.1611  | 0.4522  | 0.9893  | 0.9023  |  0 hr 02 min \n",
      " 24   | 0.1956  | 0.1375  | 0.5465  | 0.9879  | 0.9138  |  0 hr 02 min \n",
      " 25   | 0.2051  | 0.1814  | 0.3891  | 0.9890  | 0.9318  |  0 hr 02 min \n",
      " 26   | 0.1705  | 0.1277  | 0.5515  | 0.9898  | 0.9068  |  0 hr 02 min \n",
      " 27   | 0.1843  | 0.1428  | 0.4060  | 0.9902  | 0.9296  |  0 hr 02 min \n",
      " 28   | 0.1800  | 0.1300  | 0.5474  | 0.9901  | 0.9098  |  0 hr 02 min \n",
      " 29   | 0.1825  | 0.1254  | 0.5521  | 0.9935  | 0.8992  |  0 hr 02 min \n",
      " 30   | 0.1604  | 0.1549  | 0.3528  | 0.9908  | 0.9426  |  0 hr 02 min \n",
      " 31   | 0.1763  | 0.1153  | 0.5444  | 0.9903  | 0.9243  |  0 hr 02 min \n",
      " 32   | 0.1495  | 0.1055  | 0.4819  | 0.9949  | 0.9257  |  0 hr 03 min \n",
      " 33   | 0.1766  | 0.1399  | 0.4484  | 0.9881  | 0.9245  |  0 hr 03 min \n",
      "14 14 0.9425531914893617\n"
     ]
    }
   ],
   "source": [
    "log = Logger()\n",
    "log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "f = '{:^5} | {:^6.4f} | {:^10.4f} | {:^10.4f} | {:^9.4f} | {:^9.4f} | {:^10} |\\n'\n",
    "log.write('epoch | losses | train loss | valid loss | train roc |  valid roc |   time  |\\n')\n",
    "start = timer()\n",
    "\n",
    "best_param ={}\n",
    "best_param[\"roc_epoch\"] = 0\n",
    "best_param[\"loss_epoch\"] = 0\n",
    "best_param[\"valid_roc\"] = 0\n",
    "best_param[\"valid_loss\"] = 9e8\n",
    "\n",
    "for epoch in range(epochs):    \n",
    "    losses = train(model, train_smiles)\n",
    "    train_roc, train_loss = eval(model, train_smiles)\n",
    "    valid_roc, valid_loss = eval(model, valid_smiles)\n",
    "    train_roc_mean = np.array(train_roc).mean()\n",
    "    valid_roc_mean = np.array(valid_roc).mean()\n",
    "    \n",
    "    timing = time_to_str((timer() - start), 'min')  \n",
    "    log.write(f.format(epoch, losses, train_loss, valid_loss, train_roc_mean, valid_roc_mean, timing))\n",
    "\n",
    "    if valid_roc_mean > best_param[\"valid_roc\"]:\n",
    "        best_param[\"roc_epoch\"] = epoch\n",
    "        best_param[\"valid_roc\"] = valid_roc_mean\n",
    "        if valid_roc_mean > 0.80:\n",
    "             torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')             \n",
    "    if valid_loss < best_param[\"valid_loss\"]:\n",
    "        best_param[\"loss_epoch\"] = epoch\n",
    "        best_param[\"valid_loss\"] = valid_loss\n",
    "\n",
    "    if (epoch - best_param[\"roc_epoch\"] >8) and (epoch - best_param[\"loss_epoch\"] >18):        \n",
    "        break\n",
    "        \n",
    "print(best_param[\"roc_epoch\"],best_param[\"loss_epoch\"],best_param[\"valid_roc\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch:14\n",
      "test_roc:[0.8500383141762452]\n",
      "test_roc_mean: 0.8500383141762452\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"roc_epoch\"])+'.pt')     \n",
    "\n",
    "# best_model_dict = best_model.state_dict()\n",
    "# best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "# (best_model.align[0].weight == model.align[0].weight).all()\n",
    "\n",
    "test_roc, test_losses = eval(best_model, test_smiles)\n",
    "\n",
    "print(\"best epoch:\"+str(best_param[\"roc_epoch\"])\n",
    "      +\"\\n\"+\"test_roc:\"+str(test_roc)\n",
    "      +\"\\n\"+\"test_roc_mean:\",str(np.array(test_roc).mean())\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
