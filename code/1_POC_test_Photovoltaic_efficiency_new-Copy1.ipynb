{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "import random\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#then import my own modules\n",
    "# from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight\n",
    "from timeit import default_timer as timer\n",
    "from AttentiveFP import Fingerprint, Fingerprint_viz, graph_dict, graph_dataset, null_collate, Graph, Logger, time_to_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_aviable = torch.cuda.is_available()\n",
    "device = torch.device(0)\n",
    "\n",
    "SEED = 8\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  29978\n",
      "number of successfully processed smiles:  29978\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'Photovoltaic efficiency'\n",
    "tasks = ['PCE']\n",
    "\n",
    "raw_filename = \"../data/cep-processed.csv\"\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)].reset_index()\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 200\n",
    "\n",
    "p_dropout= 0.2\n",
    "fingerprint_dim = 50\n",
    "\n",
    "weight_decay = 5 # also known as l2_regularization_lambda\n",
    "learning_rate = 3\n",
    "K = 3\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph dicts loaded from ../data/cep-processed.pkl\n"
     ]
    }
   ],
   "source": [
    "smiles_list = smiles_tasks_df['smiles'].values\n",
    "label_list = smiles_tasks_df[tasks[0]].values\n",
    "graph_dict = graph_dict(smiles_list, label_list, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:  200\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "train_fold = []\n",
    "valid_fold = []\n",
    "for k, (train_idx, valid_idx) in enumerate(kfold.split(smiles_list)):\n",
    "    train_fold.append(train_idx)\n",
    "    valid_fold.append(valid_idx)\n",
    "    \n",
    "while (len(train_fold[0]) % batch_size) / batch_size <0.8:\n",
    "    batch_size +=1\n",
    "print(\"batch size: \", batch_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199454\n",
      "sum_importance torch.Size([1])\n",
      "preprocess.0.linear.weight torch.Size([50, 39])\n",
      "preprocess.0.linear.bias torch.Size([50])\n",
      "preprocess.0.bn.weight torch.Size([50])\n",
      "preprocess.0.bn.bias torch.Size([50])\n",
      "propagate.0.encoder.0.linear.weight torch.Size([2500, 10])\n",
      "propagate.0.encoder.0.linear.bias torch.Size([2500])\n",
      "propagate.0.encoder.0.bn.weight torch.Size([2500])\n",
      "propagate.0.encoder.0.bn.bias torch.Size([2500])\n",
      "propagate.0.align.weight torch.Size([1, 100])\n",
      "propagate.0.align.bias torch.Size([1])\n",
      "propagate.0.attend.linear.weight torch.Size([50, 50])\n",
      "propagate.0.attend.linear.bias torch.Size([50])\n",
      "propagate.0.attend.bn.weight torch.Size([50])\n",
      "propagate.0.attend.bn.bias torch.Size([50])\n",
      "propagate.0.gru.weight_ih torch.Size([150, 50])\n",
      "propagate.0.gru.weight_hh torch.Size([150, 50])\n",
      "propagate.0.gru.bias_ih torch.Size([150])\n",
      "propagate.0.gru.bias_hh torch.Size([150])\n",
      "propagate.1.encoder.0.linear.weight torch.Size([2500, 10])\n",
      "propagate.1.encoder.0.linear.bias torch.Size([2500])\n",
      "propagate.1.encoder.0.bn.weight torch.Size([2500])\n",
      "propagate.1.encoder.0.bn.bias torch.Size([2500])\n",
      "propagate.1.align.weight torch.Size([1, 100])\n",
      "propagate.1.align.bias torch.Size([1])\n",
      "propagate.1.attend.linear.weight torch.Size([50, 50])\n",
      "propagate.1.attend.linear.bias torch.Size([50])\n",
      "propagate.1.attend.bn.weight torch.Size([50])\n",
      "propagate.1.attend.bn.bias torch.Size([50])\n",
      "propagate.1.gru.weight_ih torch.Size([150, 50])\n",
      "propagate.1.gru.weight_hh torch.Size([150, 50])\n",
      "propagate.1.gru.bias_ih torch.Size([150])\n",
      "propagate.1.gru.bias_hh torch.Size([150])\n",
      "propagate.2.encoder.0.linear.weight torch.Size([2500, 10])\n",
      "propagate.2.encoder.0.linear.bias torch.Size([2500])\n",
      "propagate.2.encoder.0.bn.weight torch.Size([2500])\n",
      "propagate.2.encoder.0.bn.bias torch.Size([2500])\n",
      "propagate.2.align.weight torch.Size([1, 100])\n",
      "propagate.2.align.bias torch.Size([1])\n",
      "propagate.2.attend.linear.weight torch.Size([50, 50])\n",
      "propagate.2.attend.linear.bias torch.Size([50])\n",
      "propagate.2.attend.bn.weight torch.Size([50])\n",
      "propagate.2.attend.bn.bias torch.Size([50])\n",
      "propagate.2.gru.weight_ih torch.Size([150, 50])\n",
      "propagate.2.gru.weight_hh torch.Size([150, 50])\n",
      "propagate.2.gru.bias_ih torch.Size([150])\n",
      "propagate.2.gru.bias_hh torch.Size([150])\n",
      "superGather.0.align.weight torch.Size([1, 100])\n",
      "superGather.0.align.bias torch.Size([1])\n",
      "superGather.0.attend.linear.weight torch.Size([50, 50])\n",
      "superGather.0.attend.linear.bias torch.Size([50])\n",
      "superGather.0.attend.bn.weight torch.Size([50])\n",
      "superGather.0.attend.bn.bias torch.Size([50])\n",
      "superGather.0.gru.weight_ih torch.Size([150, 50])\n",
      "superGather.0.gru.weight_hh torch.Size([150, 50])\n",
      "superGather.0.gru.bias_ih torch.Size([150])\n",
      "superGather.0.gru.bias_hh torch.Size([150])\n",
      "predict.0.linear.weight torch.Size([512, 50])\n",
      "predict.0.linear.bias torch.Size([512])\n",
      "predict.0.bn.weight torch.Size([512])\n",
      "predict.0.bn.bias torch.Size([512])\n",
      "predict.3.weight torch.Size([1, 512])\n",
      "predict.3.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "# optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "optimizer = optim.SGD(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "\n",
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in  model.parameters()])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(smiles_list):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=True, worker_init_fn=np.random.seed(SEED))\n",
    "    losses = []\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(train_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        \n",
    "        loss = loss_function(mol_prediction, label.view(-1,1))     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "        \n",
    "def eval(smiles_list):\n",
    "    model.eval()\n",
    "    eval_MAE_list = []\n",
    "    eval_MSE_list = []\n",
    "    eval_loader = DataLoader(graph_dataset(smiles_list, graph_dict), batch_size, collate_fn=null_collate, \\\n",
    "                              num_workers=8, pin_memory=True, shuffle=False, worker_init_fn=np.random.seed(SEED))\n",
    "    for b, (smiles, atom, bond, bond_index, mol_index, label) in enumerate(eval_loader):\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        bond_index = bond_index.to(device)\n",
    "        mol_index = mol_index.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        mol_prediction = model(atom, bond, bond_index, mol_index)\n",
    "        MAE = F.l1_loss(mol_prediction, label.view(-1,1), reduction='none')        \n",
    "        MSE = F.mse_loss(mol_prediction, label.view(-1,1), reduction='none')\n",
    "        \n",
    "        eval_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        eval_MSE_list.extend(MSE.data.squeeze().cpu().numpy())\n",
    "    return np.array(eval_MAE_list).mean(), np.array(eval_MSE_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log = Logger()\n",
    "# log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "# f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "# log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "# start = timer()\n",
    "\n",
    "# best_param ={}\n",
    "# best_param[\"train_epoch\"] = 0\n",
    "# best_param[\"valid_epoch\"] = 0\n",
    "# best_param[\"train_MSE\"] = 9e8\n",
    "# best_param[\"valid_MSE\"] = 9e8\n",
    "\n",
    "# fold_index = 0\n",
    "# for epoch in range(800):\n",
    "#     losses = train(smiles_list[train_fold[fold_index]])\n",
    "#     traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "#     valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "\n",
    "#     timing = time_to_str((timer() - start), 'min')  \n",
    "#     log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "\n",
    "#     if train_MSE < best_param[\"train_MSE\"]:\n",
    "#         best_param[\"train_epoch\"] = epoch\n",
    "#         best_param[\"train_MSE\"] = train_MSE\n",
    "#     if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#         best_param[\"valid_epoch\"] = epoch\n",
    "#         best_param[\"valid_MSE\"] = valid_MSE\n",
    "# #         if valid_MSE < 0.35:\n",
    "# #              torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#     if (epoch - best_param[\"train_epoch\"] >10) and (epoch - best_param[\"valid_epoch\"] >18):        \n",
    "#         break\n",
    "# print(best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# log = Logger()\n",
    "# log.open(f'{prefix_filename}_{start_time}.txt')\n",
    "\n",
    "# f = '{:^5} | {:^7.4f} | {:^7.4f} | {:^7.4f} | {:^7} \\n'\n",
    "# log.write('epoch | loss | train MSE |  valid MSE |  time \\n')\n",
    "# start = timer()\n",
    "\n",
    "# log2 = Logger()\n",
    "# log2.open(f'{prefix_filename}_best_{start_time}.txt')\n",
    "# f2 = '{:^5} | {:^5} | {:^7.4f} | {:^7.4f} \\n'\n",
    "\n",
    "# for fold_index in range(5):\n",
    "    \n",
    "#     model = Fingerprint(output_units_num, fingerprint_dim, K=K, T=T, p_dropout=p_dropout)\n",
    "#     model.to(device)\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "    \n",
    "#     best_param ={}\n",
    "#     best_param[\"train_epoch\"] = 0\n",
    "#     best_param[\"valid_epoch\"] = 0\n",
    "#     best_param[\"train_MSE\"] = 9e8\n",
    "#     best_param[\"valid_MSE\"] = 9e8\n",
    "#     for epoch in range(800):\n",
    "#         losses = train(smiles_list[train_fold[fold_index]])\n",
    "#         traine_MAE, train_MSE = eval(smiles_list[train_fold[fold_index]])\n",
    "#         valid_MAE, valid_MSE = eval(smiles_list[valid_fold[fold_index]])\n",
    "        \n",
    "#         timing = time_to_str((timer() - start), 'min')  \n",
    "#         log.write(f.format(epoch, losses, train_MSE, valid_MSE, timing))\n",
    "        \n",
    "#         if train_MSE < best_param[\"train_MSE\"]:\n",
    "#             best_param[\"train_epoch\"] = epoch\n",
    "#             best_param[\"train_MSE\"] = train_MSE\n",
    "#         if valid_MSE < best_param[\"valid_MSE\"]:\n",
    "#             best_param[\"valid_epoch\"] = epoch\n",
    "#             best_param[\"valid_MSE\"] = valid_MSE\n",
    "# #             if valid_MSE < 0.35:\n",
    "# #                  torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')\n",
    "#         if (epoch - best_param[\"train_epoch\"] >18) and (epoch - best_param[\"valid_epoch\"] >28):        \n",
    "#             break\n",
    "\n",
    "#     log2.write('fold | epoch | train_MSE | valid MSE \\n')\n",
    "#     log2.write(f2.format(fold_index, best_param[\"valid_epoch\"],best_param[\"train_MSE\"],best_param[\"valid_MSE\"]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model\n",
    "# best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"valid_epoch\"])+'.pt')     \n",
    "\n",
    "# best_model_dict = best_model.state_dict()\n",
    "# best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "# (best_model.align[0].weight == model.align[0].weight).all()\n",
    "# test_MAE, test_MSE = eval(model, test_df)\n",
    "# print(\"best epoch:\",best_param[\"test_epoch\"],\"\\n\",\"test MSE:\",test_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for e in range(20):\n",
    "#     losses = train(smiles_list[valid_fold[fold_index]])\n",
    "#     print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
