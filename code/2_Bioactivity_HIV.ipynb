{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "torch.manual_seed(8) # for reproduce\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP import Fingerprint, Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "# from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from IPython.display import SVG, display\n",
    "import seaborn as sns; sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  41127\n",
      "number of successfully processed smiles:  41127\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'HIV'\n",
    "tasks = ['HIV_active']\n",
    "raw_filename = \"../data/HIV.csv\"\n",
    "feature_filename = raw_filename.replace('.csv','.pickle')\n",
    "filename = raw_filename.replace('.csv','')\n",
    "prefix_filename = raw_filename.split('/')[-1].replace('.csv','')\n",
    "smiles_tasks_df = pd.read_csv(raw_filename)\n",
    "smilesList = smiles_tasks_df.smiles.values\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        remained_smiles.append(smiles)\n",
    "    except:\n",
    "        print(\"not successfully processed smiles: \", smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "smiles_tasks_df = smiles_tasks_df[smiles_tasks_df[\"smiles\"].isin(remained_smiles)]\n",
    "smiles_tasks_df['cano_smiles'] =canonical_smiles_list\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# print(len([i for i in atom_num_dist if i<51]),len([i for i in atom_num_dist if i>50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 8\n",
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "start = time.time()\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 800\n",
    "p_dropout = 0.1\n",
    "fingerprint_dim = 150\n",
    "\n",
    "radius = 4\n",
    "T = 2\n",
    "weight_decay = 3.9 # also known as l2_regularization_lambda\n",
    "learning_rate = 3\n",
    "per_task_output_units_num = 2 # for classification model with 2 classes\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1CN[Co-4]23(N1)(NCCN2)NCCN3\n",
      "O=C1O[Cu-5]2(O)(O)(OC1=O)OC(=O)C(=O)O2\n",
      "CCc1cc[n+]([Mn](SC#N)(SC#N)([n+]2ccc(CC)cc2)([n+]2ccc(CC)cc2)[n+]2ccc(CC)cc2)cc1\n",
      "O=C1O[Al]23(OC1=O)(OC(=O)C(=O)O2)OC(=O)C(=O)O3\n",
      "O=C1C[N+]23CC[N+]45CC(=O)O[Ni-4]24(O1)(OC(=O)C3)OC(=O)C5\n"
     ]
    }
   ],
   "source": [
    "smilesList = [smiles for smiles in canonical_smiles_list if len(Chem.MolFromSmiles(smiles).GetAtoms())<101]\n",
    "\n",
    "if os.path.isfile(feature_filename):\n",
    "    feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "else:\n",
    "    feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "# feature_dicts = get_smiles_dicts(smilesList)\n",
    "\n",
    "remained_df = smiles_tasks_df[smiles_tasks_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = smiles_tasks_df.drop(remained_df.index)\n",
    "uncovered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "class ScaffoldGenerator(object):\n",
    "    \"\"\"\n",
    "    Generate molecular scaffolds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    include_chirality : : bool, optional (default False)\n",
    "      Include chirality in scaffolds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, include_chirality=False):\n",
    "        self.include_chirality = include_chirality\n",
    "\n",
    "    def get_scaffold(self, mol):\n",
    "        \"\"\"\n",
    "        Get Murcko scaffolds for molecules.\n",
    "\n",
    "        Murcko scaffolds are described in DOI: 10.1021/jm9602928. They are\n",
    "        essentially that part of the molecule consisting of rings and the\n",
    "        linker atoms between them.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mols : array_like\n",
    "            Molecules.\n",
    "        \"\"\"\n",
    "        return MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=self.include_chirality)\n",
    "\n",
    "\n",
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    engine = ScaffoldGenerator(include_chirality=include_chirality)\n",
    "    scaffold = engine.get_scaffold(mol)\n",
    "    return scaffold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for i,task in enumerate(tasks):    \n",
    "    negative_df = remained_df[remained_df[task] == 0][[\"smiles\",task]]\n",
    "    positive_df = remained_df[remained_df[task] == 1][[\"smiles\",task]]\n",
    "    weights.append([(positive_df.shape[0]+negative_df.shape[0])/negative_df.shape[0],\\\n",
    "                    (positive_df.shape[0]+negative_df.shape[0])/positive_df.shape[0]])\n",
    "    \n",
    "scaffold_list = []\n",
    "all_scaffolds_dict = {}\n",
    "\n",
    "for index, smiles in enumerate(remained_df['cano_smiles']):\n",
    "    scaffold = generate_scaffold(smiles)\n",
    "    scaffold_list.append(scaffold)\n",
    "    if scaffold not in all_scaffolds_dict:\n",
    "        all_scaffolds_dict[scaffold] = [index]\n",
    "    else:\n",
    "        all_scaffolds_dict[scaffold].append(index)\n",
    "remained_df['scaffold'] = scaffold_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remained_df.groupby(['scaffold'])['scaffold'].count() \\\n",
    "                     .reset_index(name='count') \\\n",
    "                     .sort_values(['count'], ascending=False) \\\n",
    "                     .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_randomized_spliting(scaffolds_dict, random_seed = 0): \n",
    "    count = 0\n",
    "    negative_count = 0\n",
    "    while (count < 0.095*len(remained_df) or  count > 0.105*len(remained_df)) \\\n",
    "        or (negative_count < 45 or  negative_count > 55):\n",
    "        random_seed +=1\n",
    "        random.seed(random_seed)\n",
    "        scaffold = random.sample(list(scaffolds_dict.keys()), 100)\n",
    "        count = sum([len(scaffolds_dict[scaffold]) for scaffold in scaffold])\n",
    "        index = [index for scaffold in scaffold for index in scaffolds_dict[scaffold]]\n",
    "        negative_count = len(remained_df.iloc[index, :][remained_df['BBBP'] == 0])\n",
    "#     print(random)\n",
    "    print(random_seed, count, negative_count, index)\n",
    "    return scaffold, index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaffold, test_index = scaffold_randomized_spliting(all_scaffolds_dict, random_seed=88)\n",
    "training_scaffolds_dict = {x: all_scaffolds_dict[x] for x in all_scaffolds_dict.keys() if x not in test_scaffold}\n",
    "valid_scaffold, valid_index = scaffold_randomized_spliting(training_scaffolds_dict, random_seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = remained_df.iloc[test_index,:] # test set\n",
    "valid_df = remained_df.iloc[valid_index,:] # valid set\n",
    "train_df = remained_df.drop(test_df.index).drop(valid_df.index) # train set\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([canonical_smiles_list[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "\n",
    "loss_function = [nn.CrossEntropyLoss(torch.Tensor(weight),reduction='mean') for weight in weights]\n",
    "model = Fingerprint(radius, T, num_atom_features,num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "model.cuda()\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer, loss_function):\n",
    "    model.train()\n",
    "    np.random.seed(epoch)\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    for counter, train_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[train_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        atoms_prediction, mol_prediction = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask))\n",
    "#         print(torch.Tensor(x_atom).size(),torch.Tensor(x_bonds).size(),torch.cuda.LongTensor(x_atom_index).size(),torch.cuda.LongTensor(x_bond_index).size(),torch.Tensor(x_mask).size())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "        for i,task in enumerate(tasks):\n",
    "            y_pred = mol_prediction[:, i * per_task_output_units_num:(i + 1) *\n",
    "                                    per_task_output_units_num]\n",
    "            y_val = batch_df[task].values\n",
    "\n",
    "            validInds = np.where((y_val==0) | (y_val==1))[0]\n",
    "#             validInds = np.where(y_val != -1)[0]\n",
    "            if len(validInds) == 0:\n",
    "                continue\n",
    "            y_val_adjust = np.array([y_val[v] for v in validInds]).astype(float)\n",
    "            validInds = torch.cuda.LongTensor(validInds).squeeze()\n",
    "            y_pred_adjust = torch.index_select(y_pred, 0, validInds)\n",
    "\n",
    "            loss += loss_function[i](\n",
    "                y_pred_adjust,\n",
    "                torch.cuda.LongTensor(y_val_adjust))\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "def eval(model, dataset):\n",
    "    model.eval()\n",
    "    y_val_list = {}\n",
    "    y_pred_list = {}\n",
    "    losses_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    for counter, eval_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[eval_batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        atoms_prediction, mol_prediction = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask))\n",
    "        atom_pred = atoms_prediction.data[:,:,1].unsqueeze(2).cpu().numpy()\n",
    "        for i,task in enumerate(tasks):\n",
    "            y_pred = mol_prediction[:, i * per_task_output_units_num:(i + 1) *\n",
    "                                    per_task_output_units_num]\n",
    "            y_val = batch_df[task].values\n",
    "\n",
    "            validInds = np.where((y_val==0) | (y_val==1))[0]\n",
    "#             validInds = np.where((y_val=='0') | (y_val=='1'))[0]\n",
    "#             print(validInds)\n",
    "            if len(validInds) == 0:\n",
    "                continue\n",
    "            y_val_adjust = np.array([y_val[v] for v in validInds]).astype(float)\n",
    "            validInds = torch.cuda.LongTensor(validInds).squeeze()\n",
    "            y_pred_adjust = torch.index_select(y_pred, 0, validInds)\n",
    "#             print(validInds)\n",
    "            loss = loss_function[i](\n",
    "                y_pred_adjust,\n",
    "                torch.cuda.LongTensor(y_val_adjust))\n",
    "#             print(y_pred_adjust)\n",
    "            y_pred_adjust = F.softmax(y_pred_adjust,dim=-1).data.cpu().numpy()[:,1]\n",
    "            losses_list.append(loss.cpu().detach().numpy())\n",
    "            try:\n",
    "                y_val_list[i].extend(y_val_adjust)\n",
    "                y_pred_list[i].extend(y_pred_adjust)\n",
    "            except:\n",
    "                y_val_list[i] = []\n",
    "                y_pred_list[i] = []\n",
    "                y_val_list[i].extend(y_val_adjust)\n",
    "                y_pred_list[i].extend(y_pred_adjust)\n",
    "                \n",
    "    eval_roc = [roc_auc_score(y_val_list[i], y_pred_list[i]) for i in range(len(tasks))]\n",
    "#     eval_prc = [auc(precision_recall_curve(y_val_list[i], y_pred_list[i])[1],precision_recall_curve(y_val_list[i], y_pred_list[i])[0]) for i in range(len(tasks))]\n",
    "#     eval_precision = [precision_score(y_val_list[i],\n",
    "#                                      (np.array(y_pred_list[i]) > 0.5).astype(int)) for i in range(len(tasks))]\n",
    "#     eval_recall = [recall_score(y_val_list[i],\n",
    "#                                (np.array(y_pred_list[i]) > 0.5).astype(int)) for i in range(len(tasks))]\n",
    "    eval_loss = np.array(losses_list).mean()\n",
    "    \n",
    "    return eval_roc, eval_loss #eval_prc, eval_precision, eval_recall, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_param ={}\n",
    "best_param[\"roc_epoch\"] = 0\n",
    "best_param[\"loss_epoch\"] = 0\n",
    "best_param[\"valid_roc\"] = 0\n",
    "best_param[\"valid_loss\"] = 9e8\n",
    "\n",
    "for epoch in range(epochs):    \n",
    "    train_roc, train_loss = eval(model, train_df)\n",
    "    valid_roc, valid_loss = eval(model, valid_df)\n",
    "    train_roc_mean = np.array(train_roc).mean()\n",
    "    valid_roc_mean = np.array(valid_roc).mean()\n",
    "    \n",
    "#     tensorboard.add_scalars('ROC',{'train_roc':train_roc_mean,'valid_roc':valid_roc_mean},epoch)\n",
    "#     tensorboard.add_scalars('Losses',{'train_losses':train_loss,'valid_losses':valid_loss},epoch)\n",
    "\n",
    "    if valid_roc_mean > best_param[\"valid_roc\"]:\n",
    "        best_param[\"roc_epoch\"] = epoch\n",
    "        best_param[\"valid_roc\"] = valid_roc_mean\n",
    "        if valid_roc_mean > 0.80:\n",
    "             torch.save(model, 'saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(epoch)+'.pt')             \n",
    "    if valid_loss < best_param[\"valid_loss\"]:\n",
    "        best_param[\"loss_epoch\"] = epoch\n",
    "        best_param[\"valid_loss\"] = valid_loss\n",
    "\n",
    "    print(\"EPOCH:\\t\"+str(epoch)+'\\n'\\\n",
    "        +\"train_roc\"+\":\"+str(train_roc)+'\\n'\\\n",
    "        +\"valid_roc\"+\":\"+str(valid_roc)+'\\n'\\\n",
    "#         +\"train_roc_mean\"+\":\"+str(train_roc_mean)+'\\n'\\\n",
    "#         +\"valid_roc_mean\"+\":\"+str(valid_roc_mean)+'\\n'\\\n",
    "        )\n",
    "    if (epoch - best_param[\"roc_epoch\"] >16) and (epoch - best_param[\"loss_epoch\"] >18):        \n",
    "        break\n",
    "        \n",
    "    train(model, train_df, optimizer, loss_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "best_model = torch.load('saved_models/model_'+prefix_filename+'_'+start_time+'_'+str(best_param[\"roc_epoch\"])+'.pt')     \n",
    "\n",
    "# best_model_dict = best_model.state_dict()\n",
    "# best_model_wts = copy.deepcopy(best_model_dict)\n",
    "\n",
    "# model.load_state_dict(best_model_wts)\n",
    "# (best_model.align[0].weight == model.align[0].weight).all()\n",
    "test_roc, test_losses = eval(best_model, test_df)\n",
    "\n",
    "print(\"best epoch:\"+str(best_param[\"roc_epoch\"])\n",
    "      +\"\\n\"+\"test_roc:\"+str(test_roc)\n",
    "      +\"\\n\"+\"test_roc_mean:\",str(np.array(test_roc).mean())\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
